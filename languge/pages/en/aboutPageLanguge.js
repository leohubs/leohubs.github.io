export default {
  bannerTitle:"About Us",
  title:"Tencent Media Lab",
  text:"Tencent Media Lab (TML) dedicates to Cutting-Edge research on audio and video technologies, including real-time video communication, advaced audio and video codec algorithms and standardization, computer vision, image processing, and multimedia quality evaluation. TML is recognized as an elite pioneer in the multimedia industry, providing total solutions with leading technologies across various multimedia applications.",
  members:"",
  sideNavBarTitle:["Tencent Media Lab Introduction","Paper","Join us"],

  //关于我们-实验室介绍
  team: "Team",
  expertList:[
    {
      id: 1,
      imgClass: "icon-about_image_expert_0",
      name:"Dr. Shan Liu",
      text:"Shan Liu is a Distinguished Scientist and General Manager at Tencent where she heads the Tencent Media Lab. Prior to joining Tencent she was the Chief Scientist and Head of America Media Lab at Futurewei Technologies. She was formerly Director of Multimedia Technology Division at MediaTek USA. She was also formerly with MERL, Sony and IBM. Dr. Liu is the inventor of more than 200 US and global patent applications and the author of more than 60 journal and conference articles. She actively contributes to international standards such as ITU-T H.265 | ISO/IEC HEVC, MPEG-DASH, MPEG-I, etc. and served as co-Editor of H.265/HEVC v4 and MPEG-I VVC. She was in technical and organizing committees, or an invited speaker, at various international conferences including IEEE ICIP, VCIP, ICNC, ICME, MIPR and ACM Multimedia. She served in Industrial Relationship Committee of IEEE Signal Processing Society 2014-2015. She was the VP of Industrial Relations and Development of Asia-Pacific Signal and Information Processing Association (APSIPA) 2016-2017 and was named the Industrial Distinguished Leader of APSIPA in 2018. Dr. Liu obtained her B.Eng. degree in Electronics Engineering from Tsinghua University, Beijing, China and M.S. and Ph.D. degrees in Electrical Engineering from University of Southern California, Los Angeles, USA." 
    },
    {
      id: 2,
      imgClass: "icon-about_image_expert_1",
      name:"Dr. Xiang Li",
      text:"Xiang Li received the B.Sc. and M.Sc. degrees in electronic engineering from Tsinghua University, Beijing, China, and the Dr.-Ing degree in Electrical, Electronic and Communication Engineering from University of Erlangen-Nuremberg, Germany. He is currently a senior principal researcher and the head of video coding standards in Tencent’s Media Lab. Before joining Tencent, he was with Qualcomm and Siemens. Dr. Li has been working in the field of video compression for years and is an active contributor to international video coding standards. He served as chair and co-chair in a number of Ad Hoc groups, core experiments, including the co-chair of JEM reference software, VVC reference software, and co-editor of MPEG-5 EVC. Dr. Li is a senior member of IEEE. He has published over 40 journal and conference papers, 300+ standard contributions, and hold 120+ US granted and pending patents." 
    },
    {
      id: 3,
      imgClass: "icon-about_image_expert_2",
      name: "Dr. Xin Zhao",
      text: "Dr. Xin Zhao received the B.S. degree in electronic engineering from Tsinghua University, Beijing, China, in 2006, and the Ph.D. degree in computer applications from Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China.In 2017, he joined Tencent, Palo Alto, CA, USA, where he is currently a Principal Researcher, focusing on the research and development of VVC standard, and leading a team developing new video coding standard. From 2012 to 2017, he was a Staff Engineer with Qualcomm, San Diego, CA, USA. Since 2009, he has been actively contributing to the development of international video coding standards and their extensions, such as HEVC, 3D extensions to H.264/AVC and HEVC standards, and VVC with Joint Video Exploration Team (JVET).Dr. Zhao has over 12 years’ experience on video coding and processing algorithms. He has contributed over 200 proposals to multiple video coding standards, including around 30 adopted proposals. Dr. Zhao has been serving as the chairs of core experiments and multiple Break-out-Groups and Ad-hoc Group established in JVET. He is the inventor of over 260 filed patent applications with around 20 granted patents, and author of over 30 papers published in top-tier academic journals and conferences. His current research interests include image and video coding, video processing."
    },
    {
      id: 4,
      imgClass: "icon-about_image_expert_3",
      name:"Dr. Stephan Wenger",
      text:"Dr. Stephan Wenger is Sr. Director of Intellectual Property and Standards in Tencent America LLC.  Until early 2018, he was VP IP & Standards at Vidyo Inc., a leading supplier of unified collaboration solutions, and before that, he was active in Nokia's Intellectual Property Rights department and research center.  He has also helped start companies in the field of multimedia coding and served on the Board of Directors of UB Video Inc. and eBrisk Video until their successful acquisitions in 2006 and 2016, respectively.  He is also very active in the standardization for new Multimedia technologies, especially in the IETF, ITU-T, and MPEG.   Dr. Wenger has been inventor of some 40 patents with many more pending. He received the diploma and Dr.-Ing degrees in Computer Science from Technische Universität Berlin, Germany, in 1989 and 1995 respectively." 
    },
    {
      id: 5,
      imgClass: "icon-about_image_expert_4",
      name:"Dr. Iraj Sodagar",
      text:"Iraj Sodagar joined Tencent America’s Media Lab in November 2018 as a Principal Researcher. Before Tencent, he was with Microsoft for 10 years as a Principal Multimedia System Architect. At Microsoft, he worked with various research and product groups in the development and standardization of multimedia technologies. He was responsible for Windows’ multimedia delivery strategy and alignment of products and standardization, as well as coordination of all streaming standardization cross Microsoft. Throughout the last 25 years, Dr. Sodagar has participated, led and managed various R&D projects, advanced architecture design and product development including image and video coding, media indexing and analysis, media storage, transport and delivery, and media transcoding on cloud.  He built and managed technical teams for R&D, software development and productization, and standardization of developed technology. He had various leadership positions and engagements in ITU-T, JPEG, MPEG, AMQP, W3C, DVB, 3GPP, W3C, CTA WAVE, DLNA and AOMedia organizations. He also started Wireless Media Forum, the first consortia on mobile video delivery in early 2000s. More recently, he has been the chair of MPEG’s DASH subgroup from its very start. He is also the founder, and the President and Chairman of Board of DASH-Industry Forum (DASH-IF). He was also the co-chair of MPEG’s CMAF subgroup from the start until publication of its 1st edition.He received the Ph.D. degree in electrical engineering from Georgia Institute of Technology, Atlanta, in 1994." 
    },
    {
      id: 6,
      imgClass: "icon-about_image_expert_5",
      name:"Dr. Xiaozhong Xu",
      text:"Dr.Xiaozhong Xu joined Tencent Media Lab in 2017.He received his B.S. and Ph.D. degrees both from Department of Electronic Egnineering, Tsinghua University. He started working on video coding research and standardization since 2004. He has participated in multiple national and international video coding standard activities, including H.264/AVC, H.265/HEVC, H.266/VVC, MPEG-5, ITU-T IPTV and AVS. For those standardization activities, he have submitted more than 100 technical contributions and had more than 20 of them been adopted by the related standard committees. Dr. Xu has been actively promoting the standardization work by serving as a co-chair for screen content coding sub-groups in H.265/HEVC, H.266/VVC and AVS standards, and as a co-editor for MPEG-5 specification." 
    },
    {
      id: 7,
      imgClass: "icon-about_image_expert_6",
      name:"Dr. Bin Zhu",
      text:"Bin graduated from Iowa State University with Ph.D in Electrical & Computer Engineering. After graduation, Bin worked in several semiconductor start ups and later joined Intel and Apple to lead the team to develop core video products and technologies.  Bin joined Tencent in Dec, 2018 and lead the team to develop next generation(VVC/H266) video codec products." 
  }
],




  //关于我们-加入我们
  contactTitle: "Please contact  Tencent Media Lab",
  contactEmail: "medialab@tencent.com",
  positionList: [
    {
      id: 1,
      positionName: "Video Kernel Developing Principal Engineer",
      address: "Working Place ：Shenzhen",
      duty: {
        title: "Responsibilities:",
        content: [
          "- Optimize H264、H265 coding algorithm.",
          "- Follow up frontier coding standard establishment in industry, such as H266, AV1,etc."
        ]
      },
      jobRequirements: {
        title: "Requirements:",
        content: [
          "- Expert knowledge in video coding basic algorithms, such as H264, HEVC, etc.",
          "- Experience in assembly optimization and GPU acceleration development is preferred.",
          "- Expert in open-source framework such as ffmpeg，x264 and x265 with the ability of mobile hardware acceleration development.",
          "- Solid knowledge in math and data structure, good written and oral communication skills."
        ]
      }
    },
    {
      id: 2,
      positionName: "Video Processing Principal Engineer",
      address: "Working Place ：Shenzhen",
      duty: {
        title: "Responsibilities:",
        content: [
          "- Fundamental video processing algorithms, video enhancement and assessment, cross platform solution, etc.",
          "- Video processing algorithm development, including video denoiser, artifact reduction, color and edge enhancement, frame rate up conversion, etc.",
          "- Familiar with temporal technologies and solutions for video processing, in both fields of deep learning and signal processing.",
          "- Video processing algorithm optimization across various platforms, e.g. x86, ARM.",
          "- Good knowledge on quality assessment standards and methods for video enhancement."
        ]
      },
      jobRequirements: {
        title: "Requirements:",
        content: [
          "- Expert knowledge in video,  image, and signal processing principles, algorithms, and techniques.",
          "- Expert knowledge in quality assessment standards and methods for video enhancement.",
          "- Expert knowledge in Convolutional Neural Network (CNN) techniques.",
          "- Familiarity with video compression standards and codec would be a plus.",
          "- Familiarity with iOS and Android software development would be a plus.",
          "- Excellent software design, problem solving, and debugging skills.",
          "- Solid programming skills and C/C++, Python coding abilities.",
          "- Good written and oral communication skills."
        ]
      }
    },
    {
      id: 3,
      positionName: "Video Network Principal Engineer",
      address: "Working Place ：Shenzhen",
      duty: {
        title: "Responsibilities:",
        content: [
          "- Network algorithm research and developement on auido/video for global real-time communication.",
        ]
      },
      jobRequirements: {
        title: "Requirements:",
        content: [
          "- C/C++ coding abilities, familiar with TCP/IP protocol stacks, experience in high loading APP development.",
          "- Expert in audio/video coding and open-source framework such as FFmpeg, VLC, WebRTC, X264, etc. Expert in online audio/video core technology, such as JITTER BUFFER and FEC, along with bandwidth prediction and code rate self-adaption technology. Experience in online audio/video communication projects is preferred.",
          "- Expert in network deployment and operation. Experience in oversea acceleration, CDN maintainance and edge computing is preferred.",
          "- Expert in network metrics. Experience in research on FCC Speed Test principles, protocol and reliazaion is preferred."
        ]
      }
    },
    {
      id: 4,
      positionName: "Audio Algorithm Researcher",
      address: "Working Place ：Shenzhen",
      duty: {
        title: "Responsibilities:",
        content: [
          "- Fondamental audio algorithm, echo cancellation, noise suppression，voice conversion and microphone array technology,etc.",
        ]
      },
      jobRequirements: {
        title: "Requirements:",
        content: [
          "- Master or doctor degree in computier engineering, sensory science, computer science, physics, maths or related fields.",
          "- Solid programming skills and C/C++, Python coding abilities. Capable of adaptating and optmizing machine learning algorithm among different mobile platforms.",
          "- Experience in audio signal processing and content analysis.",
          "- Experience in pattern recognition, machine learning, linguistics and signal processing.",
          "- Experience in academic competitions like Kaggle or frontier labs in audio/video signal process/machine learning/data mining. Top on significant dataset such as ImageNet.",
          "- Good written and oral communication skills."
        ]
      }
    },
    {
      id: 5,
      positionName: "Audio Kernel Developing Principal Engineer",
      address: "Working Place ：Shenzhen",
      duty: {
        title: "Responsibilities:",
        content: [
          "-Audio commucation core coding for QQ and Tencent Cloud, before-and-after algorithm, multi-platform voice engine, before algorithm realted to voice recognition, such as mobile corss-platform voice acquisition and broadcasting.",
          "-Before-and-after algorithm for echo cancellation, noise suppression, silence detection, auto gain, etch. ",
          "-Network transmission adaption, FEC strategy, buffering control, error concealment.",
          "-Core coding optimization in adaptation to different platforms."
        ]
      },
      jobRequirements: {
        title: "Requirements:",
        content: [
          "- Master degree in computer and comminication with working experience over 2 years.",
          "- Expert knowledge in digital signal processing, audio coding standard and audio core algorithm.",
          "- Experience in before-and-after algorithm for echo cancellation, noise suppression, silence detection, auto gain and  frequency response equalizer.",
          "- Expert in audio network transmission and Qos control.",
          "- Expert in C/C++ and software development.",
          "- Experience in mobile(iOS/Andorid) development and optimization is preferred.",
          "- Good written and oral communication skills."
        ]
      }
    }
  ],

   //关于我们-相关论文
   navList: ["All","Speech and Audio","Transmission Optimization","Image quality assessment","Video Coding","Computer Vision","3D point cloud"],
   Total: 'Total ',
   item: '  ',
   thesisList:[
    [
      [22,"all"],
      [
        [
          {
            id:'all-3-5',
            title: "Multiple Reference Line Coding for Most Probable Modes in Intra Prediction",
            author: "Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand",
            time: "2019",
            content: "This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.",
            src: "https://ieeexplore.ieee.org/document/8712640"
          },
          {
            id:'all-3-4',
            title: "Wide Angular Intra Prediction for Versatile Video Coding",
            author: "Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape",
            time: "2019",
            content: "This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).",
            src: "https://ieeexplore.ieee.org/document/8712681"
          },
          {
            id:'all-3-1',
            title: "Blind image quality assessment based on joint log-contrast statistics",
            author: "Yabin Zhang et at.",
            time: "2019",
            content: "During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.",
            src: "https://www.sciencedirect.com/science/article/pii/S0925231218313432"
          },
          {
            id:'all-4-1',
            title: "Fast Adaptive Multiple Transform for Versatile Video Coding",
            author: "Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu",
            time: "2019",
            content: "The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.",
            src: "https://ieeexplore.ieee.org/document/8712650"
          },
          {
            id:'all-4-2',
            title: "Recent advances in video coding beyond the HEVC standard",
            author: "Xiaozhong Xu, Shan Liu",
            time: "2019",
            content: "The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.",
            src: "https://doi.org/10.1017/ATSIP.2019.11"
          },
          
        ],
        [
          {
            id:'all-4-3',
            title: "Current Picture Referencing in Versatile Video Coding",
            author: "Xiaozhong Xu, Xiang Li, Shan Liu",
            time: "2019",
            content: "Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.",
            src: "https://ieeexplore.ieee.org/document/8695359"
          },
          {
            id:'all-5-1',
            title: "High Performance Gesture Recognition via Effective and Efficient Temporal Modeling",
            author: "Yang Yi, Feng Li, et al.",
            time: "2019",
            content: "State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization",
            src: "https://www.ijcai.org/proceedings/2019/0141.pdf"
          },
          {
            id:'all-5-2',
            title: "A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.",
            author: "Yabin Zhang, et al",
            time: "2019",
            content: "3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.",
            src: "https://ieeexplore.ieee.org/document/8704253"
          },
          {
            id:'all-1-1',
            title: "A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network",
            author: "sarahqwang",
            time: "2018",
            content: "The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.",
            src: "https://ieeexplore.ieee.org/abstract/document/8706582"
          },
          {
            id:'all-2-1',
            title: "Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy",
            author: "zeqilai",
            time: "2018",
            content: "While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in today’s network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps’ performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.",
            src: "https://dl.acm.org/citation.cfm?id=3318101"
          }
        ],
        [
          {
            id:'all-3-2',
            title: "Intra Block Copy for Next Generation Video Coding",
            author: "Xiaozhong Xu, Xiang Li, Shan Liu",
            time: "2018",
            content: "Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.",
            src: "https://ieeexplore.ieee.org/document/8551528"
          },
          {
            id:'all-3-3',
            title: "Coupled Primary and Secondary Transform for Next Generation Video Coding",
            author: "Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu",
            time: "2018",
            content: "The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.",
            src: "https://ieeexplore.ieee.org/document/8698635"
          },
          {
            id:'all-1-5',
            title: "Gaussian density guided deep neural network for single-channel speech enhancement",
            author: "yannanwang",
            time: "2017",
            content: "Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.",
            src: "https://ieeexplore.ieee.org/abstract/document/8168116"
          },
          {
            id:'all-1-3',
            title: "A maximum likelihood approach to deep neural network based speech dereverberation",
            author: "yannanwang",
            time: "2017",
            content: "Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.",
            src: "https://ieeexplore.ieee.org/abstract/document/8282019"
          },
          {
            id:'all-1-2',
            title: "A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation",
            author: "yannanwang",
            time: "2017",
            content: "In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.",
            src: "https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF"
          }
        ],
        [
          {
            id:'all-1-4',
            title: "A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks",
            author: "yannanwang",
            time: "2017",
            content: "We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemale–female, male–male, and female–male mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.",
            src: "https://ieeexplore.ieee.org/abstract/document/7917323"
          },
          {
            id:'all-2-2',
            title: "Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy",
            author: "zeqilai",
            time: "2017",
            content: "While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.",
            src: "https://ieeexplore.ieee.org/document/8117568"
          },
          {
            id:'all-2-3',
            title: "Furion: Engineering high-quality immersive virtual reality on today's mobile devices",
            author: "zeqilai",
            time: "2017",
            content: "In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).",
            src: "https://dl.acm.org/citation.cfm?id=3117815"
          },
          {
            id:'all-4-4',
            title: "TPG Image Compression Technology",
            author: "ShitaoWang、PiaoDing、XiaozhengHuang、HanjunLiu、BinjiLuo、XinxingChen、YoubaoWu、RonggangWang",
            time: "",
            content: "TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression",
            src: "http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238"
          },
          {
            id:'all-4-5',
            title: "Saliency detection with two-level fully convolutional networks",
            author: "Yang Yi, et al.",
            time: "2017",
            content: "This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.",
            src: "https://ieeexplore.ieee.org/document/8019309/"
          },
        ],
        [
          {
            id:'all-2-4',
            title: "Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing",
            author: "zeqilai",
            time: "2017",
            content: "Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.",
            src: "https://ieeexplore.ieee.org/document/7898362"
          },
          {
            id:'all-2-5',
            title: "Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing",
            author: "zeqilai",
            time: "2017",
            content: "We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of user’s input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.",
            src: "https://dl.acm.org/citation.cfm?id=3234201"
          }
        ]
      ]
    ],
    [
      [5,"audio"],
      [
        [
          {
            id:'audio-1-1',
            title: "A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network",
            author: "sarahqwang",
            time: "2018",
            content: "The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.",
            src: "https://ieeexplore.ieee.org/abstract/document/8706582"
          },
          {
            id:'audio-1-2',
            title: "A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation",
            author: "yannanwang",
            time: "2017",
            content: "In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.",
            src: "https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF"
          },
          {
            id:'audio-1-3',
            title: "A maximum likelihood approach to deep neural network based speech dereverberation",
            author: "yannanwang",
            time: "2017",
            content: "Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.",
            src: "https://ieeexplore.ieee.org/abstract/document/8282019"
          },
          {
            id:'audio-1-4',
            title: "A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks",
            author: "yannanwang",
            time: "2017",
            content: "We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemale–female, male–male, and female–male mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.",
            src: "https://ieeexplore.ieee.org/abstract/document/7917323"
          },
          {
            id:'audio-1-5',
            title: "Gaussian density guided deep neural network for single-channel speech enhancement",
            author: "yannanwang",
            time: "2017",
            content: "Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.",
            src: "https://ieeexplore.ieee.org/abstract/document/8168116"
          }
        ]
      ]
    ],
    [
      [5,"network"],
      [
        [
          {
            id:'network-1-1',
            title: "Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy",
            author: "zeqilai",
            time: "2018",
            content: "While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in today’s network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps’ performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.",
            src: "https://dl.acm.org/citation.cfm?id=3318101"
          },
          {
            id:'network-1-2',
            title: "Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy",
            author: "zeqilai",
            time: "2017",
            content: "While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.",
            src: "https://ieeexplore.ieee.org/document/8117568"
          },
          {
            id:'network-1-3',
            title: "Furion: Engineering high-quality immersive virtual reality on today's mobile devices",
            author: "zeqilai",
            time: "2017",
            content: "In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).",
            src: "https://dl.acm.org/citation.cfm?id=3117815"
          },
          {
            id:'network-1-4',
            title: "Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing",
            author: "zeqilai",
            time: "2017",
            content: "Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.",
            src: "https://ieeexplore.ieee.org/document/7898362"
          },
          {
            id:'network-1-5',
            title: "Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing",
            author: "zeqilai",
            time: "2017",
            content: "We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of user’s input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.",
            src: "https://dl.acm.org/citation.cfm?id=3234201"
          },
        ]
      ]
    ],
    [
      [1,"quality"],
      [
        [
          {
            id:'quality-1-1',
            title: "Blind image quality assessment based on joint log-contrast statistics",
            author: "Yabin Zhang et at.",
            time: "2019",
            content: "During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.",
            src: "https://www.sciencedirect.com/science/article/pii/S0925231218313432"
          }
        ]
      ]
    ],
    [
      [8,"video"],
      [
        [
          {
            id:'video-1-3',
            title: "Wide Angular Intra Prediction for Versatile Video Coding",
            author: "Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape",
            time: "2019",
            content: "This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).",
            src: "https://ieeexplore.ieee.org/document/8712681"
          },
          {
            id:'video-1-4',
            title: "Multiple Reference Line Coding for Most Probable Modes in Intra Prediction",
            author: "Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand",
            time: "2019",
            content: "This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.",
            src: "https://ieeexplore.ieee.org/document/8712640"
          },
          {
            id:'video-1-5',
            title: "Fast Adaptive Multiple Transform for Versatile Video Coding",
            author: "Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu",
            time: "2019",
            content: "The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.",
            src: "https://ieeexplore.ieee.org/document/8712650"
          },
          {
            id:'video-2-1',
            title: "Recent advances in video coding beyond the HEVC standard",
            author: "Xiaozhong Xu, Shan Liu",
            time: "2019",
            content: "The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.",
            src: "https://doi.org/10.1017/ATSIP.2019.11"
          },
          {
            id:'video-2-2',
            title: "Current Picture Referencing in Versatile Video Coding",
            author: "Xiaozhong Xu, Xiang Li, Shan Liu",
            time: "2019",
            content: "Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.",
            src: "https://ieeexplore.ieee.org/document/8695359"
          }
        ],
        [
          {
            id:'video-1-2',
            title: "Coupled Primary and Secondary Transform for Next Generation Video Coding",
            author: "Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu",
            time: "2018",
            content: "The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.",
            src: "https://ieeexplore.ieee.org/document/8698635"
          },
          {
            id:'video-1-1',
            title: "Intra Block Copy for Next Generation Video Coding",
            author: "Xiaozhong Xu, Xiang Li, Shan Liu",
            time: "2018",
            content: "Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.",
            src: "https://ieeexplore.ieee.org/document/8551528"
          },
          {
            id:'video-2-3',
            title: "TPG Image Compression Technology",
            author: "ShitaoWang、PiaoDing、XiaozhengHuang、HanjunLiu、BinjiLuo、XinxingChen、YoubaoWu、RonggangWang",
            time: "2017",
            content: "TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression",
            src: "http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238"
          }
        ]
      ]
    ],
    [
      [2,"computer"],
      [
        [
          {
            id:'computer-1-2',
            title: "High Performance Gesture Recognition via Effective and Efficient Temporal Modeling",
            author: "Yang Yi, Feng Li, et al.",
            time: "2019",
            content: "State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization",
            src: "https://www.ijcai.org/proceedings/2019/0141.pdf"
          },
          {
            id:'computer-1-1',
            title: "Saliency detection with two-level fully convolutional networks",
            author: "Yang Yi, et al.",
            time: "2017",
            content: "This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.",
            src: "https://ieeexplore.ieee.org/document/8019309/"
          }
        ]
      ]
    ],
    [
      [1,"dianYun"],
      [
        [
          {
            id:'dianYun-1-1',
            title: "A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.",
            author: "Yabin Zhang, et al",
            time: "2019",
            content: "3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.",
            src: "https://ieeexplore.ieee.org/document/8704253"
          } 
        ]
      ]
    ]
  ]
}










