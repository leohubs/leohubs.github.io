export default {
    bannerTitle:"关于我们",
    title:"腾讯多媒体实验室",
    text:"腾讯旗下顶尖的音视频通信和处理研发团队，专注于实时音视频通信、音视频编解码前沿算法研究、音视频国际标准、计算机视觉图像处理、端到端音视频质量评测。在实时音视频通信和处理技术、音视频国际标准等领域积累了完整的解决方案和领先的技术水平。",
    members:"",
    sideNavBarTitle:["实验室介绍","相关论文","加入我们"],
        
    //关于我们-实验室介绍
    team: "专家成员",
    expertList:[
        {
          id: 1,
          imgClass: "icon-about_image_expert_0",
          name:"刘杉 博士",
          text:"本科毕业于清华大学电子工程系，硕士和博士毕业于美国南加州大学电机工程系，现任腾讯集团杰出科学家和多媒体实验室负责人。加入腾讯之前，刘杉博士曾任职多家全球500强和国际知名企业并担任高级技术和管理职务。自2017年加入腾讯，组建标准团队并代表腾讯在多媒体系统，数据压缩和网络传输协议等行业标准领域取得突破性成果。同时，带领团队研发和交付多项多媒体核心技术，全面上线腾讯会议，投屏，微视，企鹅电竞等产品并持续服务QQ空间和腾讯视频等业务，进而积极投入开源云平台和生态建设。刘杉博士深耕多媒体数据压缩、传输协议和系统、无线网络、IoT等行业标准领域，曾多次担任国际标准专家小组主席和联席主席，并多次在国际顶级学术会议担任领域主席或做邀请报告。是超过60篇学术期刊和会议论文的作者和超过200个美国和全球专利申请的发明人。是已定稿国际标准H.265/HEVC v4和正在研发的下一代国际视频编解码标准VVC的联合主编。于2014-2015年担任IEEE Signal Processing Society工业关系委员会委员；2016-2017担任Asia-Pacific Signal and Information Processing Association (APSIPA) 副总裁主管工业关系和发展。于2019年获得APSIPA工业领袖称号。" 
        },
        {
          id: 2,
          imgClass: "icon-about_image_expert_1",
          name:"李翔 博士",
          text:" 本科硕士毕业于清华大学电子工程系，博士毕业于德国纽伦堡-爱尔兰根大学电气电子工程与通信系，现任腾讯多媒体实验室视频标准负责人，资深专家研究员。加入腾讯之前，他曾供职于美国高通公司，德国西门子公司。李翔博士深耕视频压缩领域多年，是国际视频压缩标准的积极贡献者，并担任包括JEM参考软件联席主席、VVC参考软件联席主席、MPEG-5 EVC标准联合编辑等多个专家小组主席/联席主席职位。李翔博士是IEEE高级会员，在相关领域顶级期刊和学术会议发表论文40多篇，提交标准提案300余个，持有美国授权专利及公开专利申请120余件。" 
        },
        {
          id: 3,
          imgClass: "icon-about_image_expert_2",
          name: "赵欣 博士",
          text: "2006年本科毕业于清华大学电子工程系，之后进入中科院计算所继续深造，并于2012年获得工科博士学位。2017年他加入腾讯，任职专家研究员，专注于下一代VVC视频编码标准的研究和制定工作，并带领团队研发新视频标准。在此之前，于2012年至2017年间，他就职位于美国加利福尼亚州圣地亚哥市的高通公司，任职主任工程师。自2009年以来，他积极参与国际视频标准及其扩展标准的制定工作，包括HEVC，3D-AVC，3D-HEVC以及由联合视频专家组（JVET）发起的下一代视频压缩标准VVC。赵欣在视频编码和处理技术领域深耕超过12年，在该领域有着丰富的从业经验。在新一代国际视频编码标准组织JVET中，他担任了多项重要角色，其中包括主持核心技术实验及担任多项BoG和AhG的主席。他有超过260项专利申请，其中包括20多项已授权专利，已发表30多篇著名国际期刊及会议论文。他目前的研究兴趣包括图像及视频压缩，视频处理。"
        },
        {
          id: 4,
          imgClass: "icon-about_image_expert_3",
          name:"Stephan Wenger 博士",
          text:"Stephan Wenger博士于2018年加入腾讯美国，现任知识产权与标准高级总监。加入腾讯之前，他曾任统一协作解决方案提供商Vidyo公司知识产权与标准副总裁。他曾就职于诺基亚知识产权部和研究中心。他曾服务于媒体压缩领域的初创公司，担任UB Video和eBrisk Video公司董事直至两公司于2006和2016年被收购。他同时活跃于多媒体技术标准化的多个组织，如IETF、ITU-T和MPEG。Wenger博士持有40多个已授权专利以及更多的专利申请。他分别于1989年和1995年在柏林工业大学取得计算机科学硕士和博士学位。" 
        },
        {
          id: 5,
          imgClass: "icon-about_image_expert_4",
          name:"Iraj Sodagar 博士",
          text:"Iraj Sodagar 于2018年11月加入腾讯多媒体实验室，现任专家研究员。加入腾讯之前，他在微软担任多媒体系统主任架构师。在微软期间，他进行了多个多媒体技术的研究和产品项目，负责Windows多媒体交付策略，标准和产品的对接，以及协调微软内部流媒体技术标准化工作。在过去的25年中，Sodagar博士参与、领导和管理了多个研发项目、先进的架构设计以及产品开发，包括图片与视频编码，媒体索引、分析、存储、传输、交付已及云端转码。他建立并管理技术团队进行研究，软件开发与产品化，以及成熟技术的标准化。他在多个标准组织中担任领导职位，如in ITU-T, JPEG, MPEG, AMQP, W3C, DVB, 3GPP, W3C, CTA WAVE, DLNA 以及AOMedia。他在21世纪初开创了第一个关于移动视频交付的工业论坛——无线媒体论坛。自MPEG  DASH开始以来，他一直担任主席职务。他建立了DASH工业论坛，并任主席和董事长。此外，他担任MPEG CMAF联席主席至CMAF第一版本发布。Sodagar博士于1994年在亚特兰大的佐治亚理工学院电气工程系取得博士学位。" 
        },
        {
          id: 6,
          imgClass: "icon-about_image_expert_5",
          name:"许晓中 博士",
          text:"许晓中博士于2017年加入腾讯多媒体实验室。他本科和博士均毕业于清华大学电子工程系。从2004年起，开始从事视频编码领域的研究及标准化的工作。他参与进行的标准制定工作包括H.264/AVC, H.265/HEVC, H.266/VVC, MPEG-5, ITU-T IPTV, AVS等一系列国际国内视频标准。在这些标准化组织中，他提交了超过100个技术提案，并有多于20项技术提案被各个标准化组织所采纳。许博士积极推动行业标准的发展，目前担任HEVC SCC, VVC 及AVS等标准组织的屏幕内容编码专题小组的联合主席，MPEG-5标准文本联合编辑等职务。" 
        },
        {
          id: 7,
          imgClass: "icon-about_image_expert_6",
          name:"朱斌 博士",
          text:"毕业于爱荷华州立大学取得电子和计算机工程博士学位。毕业后先后就职于硅谷的几家知名半导体初创公司和Intel，Apple带领技术团队负责视频核心技术开发，期间也有2年独立创业经历。2018年12月加入公司以来，带领团队负责下一代视频编解码国际标准VVC/H.266编解码器 的实现，优化和开源协同。" 
      }
    ],


    //关于我们-加入我们
    contactTitle: "若对以下任意职位有意向，请联系",
    contactEmail: "medialab@tencent.com",
    positionList: [
      {
        id: 1,
        positionName: "视频内核开发高级工程师",
        address: "工作地点：深圳",
        duty: {
          title: "工作职责:",
          content: [
            "- 负责H264、H265编码算法深度调优；",
            "- 跟进业界前沿编码标准制定进展，如H266/AV1等。"
          ]
        },
        jobRequirements: {
          title: "岗位要求:",
          content: [
            "- 熟悉视频编解码底层算法，如H264/HEVC等；",
            "- 有汇编优化，GPU加速开发经验者优先；",
            "- 精通ffmpeg，x264，x265等开源框架，有移动硬件加速开发能力；",
            "- 扎实的数学、数据结构基础，优秀的英文阅读能力，沟通交流能力强，工作积极主动，良好的团队协作能力"
          ]
        }
      },
      {
        id: 2,
        positionName: "视频处理高级工程师",
        address: "工作地点：深圳",
        duty: {
          title: "工作职责:",
          content: [
            "- 视频处理算法研究开发, 包括：降噪, 压缩失真还原, 色彩及边缘增强, 视频倍帧等；",
            "- 熟悉视频处理时域建模技术, 包含深度学习以及时域信号处理框架；",
            "- 针对不同x86及ARM平台优化各类视频处理演算法；",
            "- 了解各类视频增强技术的量化评估方法及标准；"
          ]
        },
        jobRequirements: {
          title: "岗位要求:",
          content: [
            "- 精通视频处理，图像处理，数字信号处理，以及视频增强评估方法；",
            "- 有视频编解码算法相关经验者优先；",
            "- 在CNN深度学习领域有丰富经验；",
            "- 有较强的学术比赛经验或者在重要数据集的Leaderboard上排名靠前，比如ImageNet等学术数据集或者Kaggle等一些国内外商业比赛者优先，或来自国内外计算机视觉/计算机图形学/机器学习等领域内知名实验室；",
            "- 精通 C++，Python 程序开发及软件开发流程，熟悉计算机演算法及资料结构；",
            "- 有手机平台（iOS、Android）开发及性能优化经验者优先；",
            "- 沟通交流能力强，良好的团队合作精神，工作积极主动；"
          ]
        }
      },
      {
        id: 3,
        positionName: "视频网络高级工程师",
        address: "工作地点：深圳",
        duty: {
          title: "工作职责:",
          content: [
            "- 负责音视频全球实时通话网络部分算法研究与开发。",
          ]
        },
        jobRequirements: {
          title: "岗位要求:",
          content: [
            "- C/C++开发能力，熟悉TCP/IP协议栈，有高负载网络应用APP的开发经验；",
            "- 熟悉音视频编解码技术，了解音视频开源项目如FFmpeg，VLC、WebRTC、X264等，熟悉网络音视频通话的核心技术，如JITTER BUFFER，FEC，带宽预测和码率自适应技术，有网络音视频通话项目经验者优先；",
            "- 熟悉网络部署、运营，有海外加速、CDN运维、边缘计算者优先;",
            "- 熟悉网络度量体系，对网络测速的原理、协议和系统实现有深入研究者优先。"
          ]
        }
      },
      {
        id: 4,
        positionName: "音频算法研究员",
        address: "工作地点：深圳",
        duty: {
          title: "工作职责:",
          content: [
            "- 负责音频领域算法的研究和开发，涉及的问题包括但不限于：语音增强领域的回声抵消&噪声抑制、说话人转换、麦克风阵列技术等。",
          ]
        },
        jobRequirements: {
          title: "岗位要求:",
          content: [
            "- 具有计算机工程、感知科学、计算语言学、物理学、数学或者相关领域的硕士或者博士学位；",
            "- 对机器学习、深度学习有较深理解和应用，能独立开展研发工作；",
            "- 熟练的C/C++编程技巧，掌握一门脚本语言，如Python。具备将机器学习算法在不同的移动平台上进行移植适配，性能优化的能力；",
            "- 对音频信号处理和内容分析有较好的研究和开发经验；",
            "- 在模式识别、机器学习、语言学或者信号处理领域有丰富经验；",
            "- 有较强的学术比赛经验或者在重要数据集的Leaderboard上排名靠前，比如ImageNet等学术数据集或者Kaggle等一些国内外商业比赛者优先，或来自国内外音频信号处理实验室/机器学习/数据挖掘等领域内知名实验室；",
            "- 优秀的书面和口头沟通能力，具有团队精神。"
          ]
        }
      },
      {
        id: 5,
        positionName: "音频内核高级开发工程师",
        address: "工作地点：深圳",
        duty: {
          title: "工作职责:",
          content: [
            "- QQ与腾讯云音频通话核心编解码算法、前后处理算法研究，跨平台音频引擎研发，语音识别相关的前处理算法研究开发，包括：手机多平台声音采集播放；前后处理如回声抵消、噪声抑制、静音检测、自动增益等；网络传输适配包括FEC策略、缓冲控制、错误掩盖等；编解码器核心算法针对不同平台优化。",
          ]
        },
        jobRequirements: {
          title: "岗位要求:",
          content: [
            "- 计算机、通信等相关专业硕士以上学历，2年及以上工作经验；",
            "- 精通数字信号处理，熟悉相关音频编解码标准以及音频编解码核心算法；",
            "- 有回声抵消、噪声抑制、自动增益控制、频响均衡等其他语音前后处理及增强算法相关经验；",
            "- 有基于机器学习的语音增强相关经验者优先；",
            "- 了解音频网络传输及Qos控制；",
            "- 精通c/c++程序开发及软件开发流程；",
            "- 有手机平台（iOS、Android）开发及性能优化经验者优先；",
            "- 沟通交流能力强，良好的团队合作精神，工作积极主动。"
          ]
        }
      }
    ],


    //关于我们-相关论文
    navList: ["全部","语音音频","网络传输优化","图像质量评估","视频编解码","计算机视觉","3D点云"],
    Total: '共 ',
    item: ' 条',
    thesisList:[
      [
        [22,"all"],
        [
          [
            {
              id:'all-3-5',
              title: "Multiple Reference Line Coding for Most Probable Modes in Intra Prediction",
              author: "Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand",
              time: "2019",
              content: "This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.",
              src: "https://ieeexplore.ieee.org/document/8712640"
            },
            {
              id:'all-3-4',
              title: "Wide Angular Intra Prediction for Versatile Video Coding",
              author: "Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape",
              time: "2019",
              content: "This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).",
              src: "https://ieeexplore.ieee.org/document/8712681"
            },
            {
              id:'all-3-1',
              title: "Blind image quality assessment based on joint log-contrast statistics",
              author: "Yabin Zhang et at.",
              time: "2019",
              content: "During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.",
              src: "https://www.sciencedirect.com/science/article/pii/S0925231218313432"
            },
            {
              id:'all-4-1',
              title: "Fast Adaptive Multiple Transform for Versatile Video Coding",
              author: "Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu",
              time: "2019",
              content: "The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.",
              src: "https://ieeexplore.ieee.org/document/8712650"
            },
            {
              id:'all-4-2',
              title: "Recent advances in video coding beyond the HEVC standard",
              author: "Xiaozhong Xu, Shan Liu",
              time: "2019",
              content: "The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.",
              src: "https://doi.org/10.1017/ATSIP.2019.11"
            },
            
          ],
          [
            {
              id:'all-4-3',
              title: "Current Picture Referencing in Versatile Video Coding",
              author: "Xiaozhong Xu, Xiang Li, Shan Liu",
              time: "2019",
              content: "Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.",
              src: "https://ieeexplore.ieee.org/document/8695359"
            },
            {
              id:'all-5-1',
              title: "High Performance Gesture Recognition via Effective and Efficient Temporal Modeling",
              author: "Yang Yi, Feng Li, et al.",
              time: "2019",
              content: "State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization",
              src: "https://www.ijcai.org/proceedings/2019/0141.pdf"
            },
            {
              id:'all-5-2',
              title: "A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.",
              author: "Yabin Zhang, et al",
              time: "2019",
              content: "3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.",
              src: "https://ieeexplore.ieee.org/document/8704253"
            },
            {
              id:'all-1-1',
              title: "A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network",
              author: "sarahqwang",
              time: "2018",
              content: "The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.",
              src: "https://ieeexplore.ieee.org/abstract/document/8706582"
            },
            {
              id:'all-2-1',
              title: "Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy",
              author: "zeqilai",
              time: "2018",
              content: "While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in today’s network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps’ performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.",
              src: "https://dl.acm.org/citation.cfm?id=3318101"
            }
          ],
          [
            {
              id:'all-3-2',
              title: "Intra Block Copy for Next Generation Video Coding",
              author: "Xiaozhong Xu, Xiang Li, Shan Liu",
              time: "2018",
              content: "Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.",
              src: "https://ieeexplore.ieee.org/document/8551528"
            },
            {
              id:'all-3-3',
              title: "Coupled Primary and Secondary Transform for Next Generation Video Coding",
              author: "Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu",
              time: "2018",
              content: "The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.",
              src: "https://ieeexplore.ieee.org/document/8698635"
            },
            {
              id:'all-1-5',
              title: "Gaussian density guided deep neural network for single-channel speech enhancement",
              author: "yannanwang",
              time: "2017",
              content: "Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.",
              src: "https://ieeexplore.ieee.org/abstract/document/8168116"
            },
            {
              id:'all-1-3',
              title: "A maximum likelihood approach to deep neural network based speech dereverberation",
              author: "yannanwang",
              time: "2017",
              content: "Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.",
              src: "https://ieeexplore.ieee.org/abstract/document/8282019"
            },
            {
              id:'all-1-2',
              title: "A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation",
              author: "yannanwang",
              time: "2017",
              content: "In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.",
              src: "https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF"
            }
          ],
          [
            {
              id:'all-1-4',
              title: "A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks",
              author: "yannanwang",
              time: "2017",
              content: "We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemale–female, male–male, and female–male mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.",
              src: "https://ieeexplore.ieee.org/abstract/document/7917323"
            },
            {
              id:'all-2-2',
              title: "Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy",
              author: "zeqilai",
              time: "2017",
              content: "While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.",
              src: "https://ieeexplore.ieee.org/document/8117568"
            },
            {
              id:'all-2-3',
              title: "Furion: Engineering high-quality immersive virtual reality on today's mobile devices",
              author: "zeqilai",
              time: "2017",
              content: "In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).",
              src: "https://dl.acm.org/citation.cfm?id=3117815"
            },
            {
              id:'all-4-4',
              title: "TPG Image Compression Technology",
              author: "ShitaoWang、PiaoDing、XiaozhengHuang、HanjunLiu、BinjiLuo、XinxingChen、YoubaoWu、RonggangWang",
              time: "",
              content: "TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression",
              src: "http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238"
            },
            {
              id:'all-4-5',
              title: "Saliency detection with two-level fully convolutional networks",
              author: "Yang Yi, et al.",
              time: "2017",
              content: "This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.",
              src: "https://ieeexplore.ieee.org/document/8019309/"
            },
          ],
          [
            {
              id:'all-2-4',
              title: "Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing",
              author: "zeqilai",
              time: "2017",
              content: "Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.",
              src: "https://ieeexplore.ieee.org/document/7898362"
            },
            {
              id:'all-2-5',
              title: "Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing",
              author: "zeqilai",
              time: "2017",
              content: "We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of user’s input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.",
              src: "https://dl.acm.org/citation.cfm?id=3234201"
            }
          ]
        ]
      ],
      [
        [5,"audio"],
        [
          [
            {
              id:'audio-1-1',
              title: "A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network",
              author: "sarahqwang",
              time: "2018",
              content: "The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.",
              src: "https://ieeexplore.ieee.org/abstract/document/8706582"
            },
            {
              id:'audio-1-2',
              title: "A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation",
              author: "yannanwang",
              time: "2017",
              content: "In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.",
              src: "https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF"
            },
            {
              id:'audio-1-3',
              title: "A maximum likelihood approach to deep neural network based speech dereverberation",
              author: "yannanwang",
              time: "2017",
              content: "Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.",
              src: "https://ieeexplore.ieee.org/abstract/document/8282019"
            },
            {
              id:'audio-1-4',
              title: "A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks",
              author: "yannanwang",
              time: "2017",
              content: "We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemale–female, male–male, and female–male mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.",
              src: "https://ieeexplore.ieee.org/abstract/document/7917323"
            },
            {
              id:'audio-1-5',
              title: "Gaussian density guided deep neural network for single-channel speech enhancement",
              author: "yannanwang",
              time: "2017",
              content: "Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.",
              src: "https://ieeexplore.ieee.org/abstract/document/8168116"
            }
          ]
        ]
      ],
      [
        [5,"network"],
        [
          [
            {
              id:'network-1-1',
              title: "Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy",
              author: "zeqilai",
              time: "2018",
              content: "While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in today’s network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps’ performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.",
              src: "https://dl.acm.org/citation.cfm?id=3318101"
            },
            {
              id:'network-1-2',
              title: "Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy",
              author: "zeqilai",
              time: "2017",
              content: "While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.",
              src: "https://ieeexplore.ieee.org/document/8117568"
            },
            {
              id:'network-1-3',
              title: "Furion: Engineering high-quality immersive virtual reality on today's mobile devices",
              author: "zeqilai",
              time: "2017",
              content: "In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).",
              src: "https://dl.acm.org/citation.cfm?id=3117815"
            },
            {
              id:'network-1-4',
              title: "Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing",
              author: "zeqilai",
              time: "2017",
              content: "Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.",
              src: "https://ieeexplore.ieee.org/document/7898362"
            },
            {
              id:'network-1-5',
              title: "Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing",
              author: "zeqilai",
              time: "2017",
              content: "We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of user’s input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.",
              src: "https://dl.acm.org/citation.cfm?id=3234201"
            },
          ]
        ]
      ],
      [
        [1,"quality"],
        [
          [
            {
              id:'quality-1-1',
              title: "Blind image quality assessment based on joint log-contrast statistics",
              author: "Yabin Zhang et at.",
              time: "2019",
              content: "During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.",
              src: "https://www.sciencedirect.com/science/article/pii/S0925231218313432"
            }
          ]
        ]
      ],
      [
        [8,"video"],
        [
          [
            {
              id:'video-1-3',
              title: "Wide Angular Intra Prediction for Versatile Video Coding",
              author: "Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape",
              time: "2019",
              content: "This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).",
              src: "https://ieeexplore.ieee.org/document/8712681"
            },
            {
              id:'video-1-4',
              title: "Multiple Reference Line Coding for Most Probable Modes in Intra Prediction",
              author: "Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand",
              time: "2019",
              content: "This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.",
              src: "https://ieeexplore.ieee.org/document/8712640"
            },
            {
              id:'video-1-5',
              title: "Fast Adaptive Multiple Transform for Versatile Video Coding",
              author: "Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu",
              time: "2019",
              content: "The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.",
              src: "https://ieeexplore.ieee.org/document/8712650"
            },
            {
              id:'video-2-1',
              title: "Recent advances in video coding beyond the HEVC standard",
              author: "Xiaozhong Xu, Shan Liu",
              time: "2019",
              content: "The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.",
              src: "https://doi.org/10.1017/ATSIP.2019.11"
            },
            {
              id:'video-2-2',
              title: "Current Picture Referencing in Versatile Video Coding",
              author: "Xiaozhong Xu, Xiang Li, Shan Liu",
              time: "2019",
              content: "Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.",
              src: "https://ieeexplore.ieee.org/document/8695359"
            }
          ],
          [
            {
              id:'video-1-2',
              title: "Coupled Primary and Secondary Transform for Next Generation Video Coding",
              author: "Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu",
              time: "2018",
              content: "The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.",
              src: "https://ieeexplore.ieee.org/document/8698635"
            },
            {
              id:'video-1-1',
              title: "Intra Block Copy for Next Generation Video Coding",
              author: "Xiaozhong Xu, Xiang Li, Shan Liu",
              time: "2018",
              content: "Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.",
              src: "https://ieeexplore.ieee.org/document/8551528"
            },
            {
              id:'video-2-3',
              title: "TPG Image Compression Technology",
              author: "ShitaoWang、PiaoDing、XiaozhengHuang、HanjunLiu、BinjiLuo、XinxingChen、YoubaoWu、RonggangWang",
              time: "2017",
              content: "TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression",
              src: "http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238"
            }
          ]
        ]
      ],
      [
        [2,"computer"],
        [
          [
            {
              id:'computer-1-2',
              title: "High Performance Gesture Recognition via Effective and Efficient Temporal Modeling",
              author: "Yang Yi, Feng Li, et al.",
              time: "2019",
              content: "State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization",
              src: "https://www.ijcai.org/proceedings/2019/0141.pdf"
            },
            {
              id:'computer-1-1',
              title: "Saliency detection with two-level fully convolutional networks",
              author: "Yang Yi, et al.",
              time: "2017",
              content: "This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.",
              src: "https://ieeexplore.ieee.org/document/8019309/"
            }
          ]
        ]
      ],
      [
        [1,"dianYun"],
        [
          [
            {
              id:'dianYun-1-1',
              title: "A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.",
              author: "Yabin Zhang, et al",
              time: "2019",
              content: "3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.",
              src: "https://ieeexplore.ieee.org/document/8704253"
            } 
          ]
        ]
      ]
    ]
  }


