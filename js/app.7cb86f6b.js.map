{"version":3,"sources":["webpack:///webpack/bootstrap","webpack:///./src/App.vue?4241","webpack:///./src/App.vue?ef7a","webpack:///./src/App.vue?e250","webpack:///./src/util/util.js","webpack:///./src/App.vue?1160","webpack:///./src/App.vue?bff9","webpack:///./src/router.js","webpack:///./src/store/mutation-type.js","webpack:///./src/store/mutation.js","webpack:///./src/store/index.js","webpack:///./src/components/Layout.vue?e745","webpack:///src/components/Layout.vue","webpack:///./src/components/Layout.vue?448b","webpack:///./src/components/Layout.vue?aaa1","webpack:///./public/languge/pages/zh/homePageLanguge.js","webpack:///./public/languge/pages/zh/researchPageLanguge.js","webpack:///./public/languge/pages/zh/projectPageLanguge.js","webpack:///./public/languge/pages/zh/solutionPageLanguge.js","webpack:///./public/languge/pages/zh/aboutPageLanguge.js","webpack:///./public/languge/zh.js","webpack:///./public/languge/pages/en/homePageLanguge.js","webpack:///./public/languge/pages/en/researchPageLanguge.js","webpack:///./public/languge/pages/en/projectPageLanguge.js","webpack:///./public/languge/pages/en/solutionPageLanguge.js","webpack:///./public/languge/pages/en/aboutPageLanguge.js","webpack:///./public/languge/en.js","webpack:///./src/main.js","webpack:///./src/components/Layout.vue?0cd8"],"names":["webpackJsonpCallback","data","moduleId","chunkId","chunkIds","moreModules","executeModules","i","resolves","length","installedChunks","push","Object","prototype","hasOwnProperty","call","modules","parentJsonpFunction","shift","deferredModules","apply","checkDeferredModules","result","deferredModule","fulfilled","j","depId","splice","__webpack_require__","s","installedModules","installedCssChunks","jsonpScriptSrc","p","exports","module","l","e","promises","cssChunks","Promise","resolve","reject","href","fullhref","existingLinkTags","document","getElementsByTagName","tag","dataHref","getAttribute","rel","existingStyleTags","linkTag","createElement","type","onload","onerror","event","request","target","src","err","Error","code","parentNode","removeChild","head","appendChild","then","installedChunkData","promise","onScriptComplete","script","charset","timeout","nc","setAttribute","clearTimeout","chunk","errorType","realSrc","error","undefined","setTimeout","all","m","c","d","name","getter","o","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","oe","console","jsonpArray","window","oldJsonpFunction","slice","render","_vm","this","_h","$createElement","_c","_self","attrs","staticRenderFns","handleClickEvent","id","eventName","params","clickId","options","BeaconAction","onEvent","log","component","Vue","use","Router","routes","path","redirect","children","SET_NAVITEM_STATUS","SET_NAV_MASK","SET_BANNER_ISANDROID","SET_BANNER_ISIOS","SET_SCROLLINDECX","state","navItemstatus","items1","items2","items3","items4","isMask","isAndroid","isiOS","scrollIndex","mutations","STATE","index","item","STATAE","mutationState","ref","staticClass","style","background","outBg","height","innerHeight","width","iWidth","innerBg","_t","props","mounted","out","getElementById","inner","onresize","title","more","research","researchAndMade","text","touch","newMedia","videoUnderstanding","VideoCodingAndDecoding","videoHandle","audioHandle","connectionQuality","transfer","project","title1","title2","title3","title4","subTitle1","text1","subTitle2","text2","subTitle3","text3","subTitle4","text4","lookAll","solution","solutionTitle","join","join1","joining","navList","showList","childen","link","bgc","explain","tab1","list","subTitle","subContanier","sencTitle","sencText","tab2","tab3","business","businessList","projectName1","projectName2","explain1","list1","explain2","list2","projectName","projectText","textTPG","textTSE","textLiYing","pingtai","nav","back","next","tpg","partner","itemTitle1","itemText1","itemTitle2","itemText2","itemTitle3","itemText3Item1","itemText3Item2","itemTitle4","itemText4Item1","itemText4Item2","itemTitle5","itemText5","one","two","four","three","partners","tse","tesText","itemText1one","itemText1two","itemText2one","itemText2two","liYing","vidio","before","after","vidioAndaudioTest","pingtaiText","bannerTitle","members","sideNavBarTitle","team","expertList","imgClass","contactTitle","contactEmail","positionList","positionName","address","duty","content","jobRequirements","Total","thesisList","author","time","home","about","homeLanguge","researchLanguge","projectLanguge","solutionLanguge","aboutLanguge","routerPush","location","catch","Vuex","config","productionTip","Layout","router","beforeEach","to","from","toRouter","forEach","VueI18n","i18n","locale","messages","zh","en","afterEach","scrollTo","store","Store","h","App","$mount"],"mappings":"aACE,SAASA,EAAqBC,GAQ7B,IAPA,IAMIC,EAAUC,EANVC,EAAWH,EAAK,GAChBI,EAAcJ,EAAK,GACnBK,EAAiBL,EAAK,GAIHM,EAAI,EAAGC,EAAW,GACpCD,EAAIH,EAASK,OAAQF,IACzBJ,EAAUC,EAASG,GAChBG,EAAgBP,IAClBK,EAASG,KAAKD,EAAgBP,GAAS,IAExCO,EAAgBP,GAAW,EAE5B,IAAID,KAAYG,EACZO,OAAOC,UAAUC,eAAeC,KAAKV,EAAaH,KACpDc,EAAQd,GAAYG,EAAYH,IAG/Be,GAAqBA,EAAoBhB,GAE5C,MAAMO,EAASC,OACdD,EAASU,OAATV,GAOD,OAHAW,EAAgBR,KAAKS,MAAMD,EAAiBb,GAAkB,IAGvDe,IAER,SAASA,IAER,IADA,IAAIC,EACIf,EAAI,EAAGA,EAAIY,EAAgBV,OAAQF,IAAK,CAG/C,IAFA,IAAIgB,EAAiBJ,EAAgBZ,GACjCiB,GAAY,EACRC,EAAI,EAAGA,EAAIF,EAAed,OAAQgB,IAAK,CAC9C,IAAIC,EAAQH,EAAeE,GACG,IAA3Bf,EAAgBgB,KAAcF,GAAY,GAE3CA,IACFL,EAAgBQ,OAAOpB,IAAK,GAC5Be,EAASM,EAAoBA,EAAoBC,EAAIN,EAAe,KAGtE,OAAOD,EAIR,IAAIQ,EAAmB,GAGnBC,EAAqB,CACxB,IAAO,GAMJrB,EAAkB,CACrB,IAAO,GAGJS,EAAkB,GAGtB,SAASa,EAAe7B,GACvB,OAAOyB,EAAoBK,EAAI,OAAS,GAAG9B,IAAUA,GAAW,IAAM,CAAC,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,YAAYA,GAAW,MAI5rB,SAASyB,EAAoB1B,GAG5B,GAAG4B,EAAiB5B,GACnB,OAAO4B,EAAiB5B,GAAUgC,QAGnC,IAAIC,EAASL,EAAiB5B,GAAY,CACzCK,EAAGL,EACHkC,GAAG,EACHF,QAAS,IAUV,OANAlB,EAAQd,GAAUa,KAAKoB,EAAOD,QAASC,EAAQA,EAAOD,QAASN,GAG/DO,EAAOC,GAAI,EAGJD,EAAOD,QAKfN,EAAoBS,EAAI,SAAuBlC,GAC9C,IAAImC,EAAW,GAIXC,EAAY,CAAC,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,GAC9aR,EAAmB5B,GAAUmC,EAAS3B,KAAKoB,EAAmB5B,IACzB,IAAhC4B,EAAmB5B,IAAkBoC,EAAUpC,IACtDmC,EAAS3B,KAAKoB,EAAmB5B,GAAW,IAAIqC,QAAQ,SAASC,EAASC,GAIzE,IAHA,IAAIC,EAAO,QAAU,GAAGxC,IAAUA,GAAW,IAAM,CAAC,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,YAAYA,GAAW,OACpqByC,EAAWhB,EAAoBK,EAAIU,EACnCE,EAAmBC,SAASC,qBAAqB,QAC7CxC,EAAI,EAAGA,EAAIsC,EAAiBpC,OAAQF,IAAK,CAChD,IAAIyC,EAAMH,EAAiBtC,GACvB0C,EAAWD,EAAIE,aAAa,cAAgBF,EAAIE,aAAa,QACjE,GAAe,eAAZF,EAAIG,MAAyBF,IAAaN,GAAQM,IAAaL,GAAW,OAAOH,IAErF,IAAIW,EAAoBN,SAASC,qBAAqB,SACtD,IAAQxC,EAAI,EAAGA,EAAI6C,EAAkB3C,OAAQF,IAAK,CAC7CyC,EAAMI,EAAkB7C,GACxB0C,EAAWD,EAAIE,aAAa,aAChC,GAAGD,IAAaN,GAAQM,IAAaL,EAAU,OAAOH,IAEvD,IAAIY,EAAUP,SAASQ,cAAc,QACrCD,EAAQF,IAAM,aACdE,EAAQE,KAAO,WACfF,EAAQG,OAASf,EACjBY,EAAQI,QAAU,SAASC,GAC1B,IAAIC,EAAUD,GAASA,EAAME,QAAUF,EAAME,OAAOC,KAAOjB,EACvDkB,EAAM,IAAIC,MAAM,qBAAuB5D,EAAU,cAAgBwD,EAAU,KAC/EG,EAAIE,KAAO,wBACXF,EAAIH,QAAUA,SACP5B,EAAmB5B,GAC1BkD,EAAQY,WAAWC,YAAYb,GAC/BX,EAAOoB,IAERT,EAAQV,KAAOC,EAEf,IAAIuB,EAAOrB,SAASC,qBAAqB,QAAQ,GACjDoB,EAAKC,YAAYf,KACfgB,KAAK,WACPtC,EAAmB5B,GAAW,KAMhC,IAAImE,EAAqB5D,EAAgBP,GACzC,GAA0B,IAAvBmE,EAGF,GAAGA,EACFhC,EAAS3B,KAAK2D,EAAmB,QAC3B,CAEN,IAAIC,EAAU,IAAI/B,QAAQ,SAASC,EAASC,GAC3C4B,EAAqB5D,EAAgBP,GAAW,CAACsC,EAASC,KAE3DJ,EAAS3B,KAAK2D,EAAmB,GAAKC,GAGtC,IACIC,EADAC,EAAS3B,SAASQ,cAAc,UAGpCmB,EAAOC,QAAU,QACjBD,EAAOE,QAAU,IACb/C,EAAoBgD,IACvBH,EAAOI,aAAa,QAASjD,EAAoBgD,IAElDH,EAAOZ,IAAM7B,EAAe7B,GAE5BqE,EAAmB,SAAUd,GAE5Be,EAAOhB,QAAUgB,EAAOjB,OAAS,KACjCsB,aAAaH,GACb,IAAII,EAAQrE,EAAgBP,GAC5B,GAAa,IAAV4E,EAAa,CACf,GAAGA,EAAO,CACT,IAAIC,EAAYtB,IAAyB,SAAfA,EAAMH,KAAkB,UAAYG,EAAMH,MAChE0B,EAAUvB,GAASA,EAAME,QAAUF,EAAME,OAAOC,IAChDqB,EAAQ,IAAInB,MAAM,iBAAmB5D,EAAU,cAAgB6E,EAAY,KAAOC,EAAU,KAChGC,EAAM3B,KAAOyB,EACbE,EAAMvB,QAAUsB,EAChBF,EAAM,GAAGG,GAEVxE,EAAgBP,QAAWgF,IAG7B,IAAIR,EAAUS,WAAW,WACxBZ,EAAiB,CAAEjB,KAAM,UAAWK,OAAQa,KAC1C,MACHA,EAAOhB,QAAUgB,EAAOjB,OAASgB,EACjC1B,SAASqB,KAAKC,YAAYK,GAG5B,OAAOjC,QAAQ6C,IAAI/C,IAIpBV,EAAoB0D,EAAItE,EAGxBY,EAAoB2D,EAAIzD,EAGxBF,EAAoB4D,EAAI,SAAStD,EAASuD,EAAMC,GAC3C9D,EAAoB+D,EAAEzD,EAASuD,IAClC7E,OAAOgF,eAAe1D,EAASuD,EAAM,CAAEI,YAAY,EAAMC,IAAKJ,KAKhE9D,EAAoBmE,EAAI,SAAS7D,GACX,qBAAX8D,QAA0BA,OAAOC,aAC1CrF,OAAOgF,eAAe1D,EAAS8D,OAAOC,YAAa,CAAEC,MAAO,WAE7DtF,OAAOgF,eAAe1D,EAAS,aAAc,CAAEgE,OAAO,KAQvDtE,EAAoBuE,EAAI,SAASD,EAAOE,GAEvC,GADU,EAAPA,IAAUF,EAAQtE,EAAoBsE,IAC/B,EAAPE,EAAU,OAAOF,EACpB,GAAW,EAAPE,GAA8B,kBAAVF,GAAsBA,GAASA,EAAMG,WAAY,OAAOH,EAChF,IAAII,EAAK1F,OAAO2F,OAAO,MAGvB,GAFA3E,EAAoBmE,EAAEO,GACtB1F,OAAOgF,eAAeU,EAAI,UAAW,CAAET,YAAY,EAAMK,MAAOA,IACtD,EAAPE,GAA4B,iBAATF,EAAmB,IAAI,IAAIM,KAAON,EAAOtE,EAAoB4D,EAAEc,EAAIE,EAAK,SAASA,GAAO,OAAON,EAAMM,IAAQC,KAAK,KAAMD,IAC9I,OAAOF,GAIR1E,EAAoB8E,EAAI,SAASvE,GAChC,IAAIuD,EAASvD,GAAUA,EAAOkE,WAC7B,WAAwB,OAAOlE,EAAO,YACtC,WAA8B,OAAOA,GAEtC,OADAP,EAAoB4D,EAAEE,EAAQ,IAAKA,GAC5BA,GAIR9D,EAAoB+D,EAAI,SAASgB,EAAQC,GAAY,OAAOhG,OAAOC,UAAUC,eAAeC,KAAK4F,EAAQC,IAGzGhF,EAAoBK,EAAI,GAGxBL,EAAoBiF,GAAK,SAAS/C,GAA2B,MAApBgD,QAAQ5B,MAAMpB,GAAYA,GAEnE,IAAIiD,EAAaC,OAAO,gBAAkBA,OAAO,iBAAmB,GAChEC,EAAmBF,EAAWpG,KAAK8F,KAAKM,GAC5CA,EAAWpG,KAAOX,EAClB+G,EAAaA,EAAWG,QACxB,IAAI,IAAI3G,EAAI,EAAGA,EAAIwG,EAAWtG,OAAQF,IAAKP,EAAqB+G,EAAWxG,IAC3E,IAAIU,EAAsBgG,EAI1B9F,EAAgBR,KAAK,CAAC,EAAE,kBAEjBU,K,6ECtQT,yBAAqb,EAAG,G,oCCAxb,IAAI8F,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,MAAM,CAAC,GAAK,QAAQ,CAACF,EAAG,gBAAgB,IAC9IG,EAAkB,GCDtB,iE,oCCAA,oCAAMC,EAAmB,SAACC,EAAIC,EAAWC,GACrC,IAAMC,EAAUH,GAAM,GAChBI,EAAUF,GAAU,GAC1BG,aAAaC,QAAQH,EAASF,EAAWG,GACzCnB,QAAQsB,IAAR,cAAmBJ,EAAnB,wBAA0CF,M,2DCJ9C,yBAA8T,eAAG,G,oCCAjU,oDAQIO,EAAY,eACd,aACA,OACA,QACA,EACA,KACA,KACA,MAIa,aAAAA,E,4iDChBfC,OAAIC,IAAIC,QAGO,I,EAAA,MAAIA,OAAO,CACxBC,OAAQ,CACN,CACEC,KAAM,IACNC,SAAU,CACRlD,KAAM,UAGV,CACEiD,KAAM,SACNjD,KAAM,QACN4C,UAAW,kBAAM,uFAEnB,CACEK,KAAM,YACNL,UAAW,kBAAM,sFACjBO,SAAU,CACR,CACEF,KAAM,gBACNjD,KAAM,gBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,WACNjD,KAAM,WACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,YACNjD,KAAM,YACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,qBACNjD,KAAM,qBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,kBACNjD,KAAM,kBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,aACNjD,KAAM,aACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,kBACNjD,KAAM,kBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,oBACNjD,KAAM,oBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,aACNjD,KAAM,aACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,GACNC,SAAU,mBAIhB,CACED,KAAM,WACNjD,KAAM,UACN4C,UAAW,kBAAM,uFAEnB,CACEK,KAAK,cACLjD,KAAK,aACL4C,UAAW,kBAAK,uFAElB,CACEK,KAAK,cACLjD,KAAK,aACL4C,UAAW,kBAAK,uFAElB,CACEK,KAAK,iBACLjD,KAAK,gBACL4C,UAAW,kBAAK,uFAElB,CACEK,KAAK,wBACLjD,KAAK,uBACL4C,UAAW,kBAAK,uFAElB,CACEK,KAAM,WACNL,UAAW,kBAAM,sFACjBO,SAAS,CACP,CACEF,KAAM,aACNjD,KAAM,aACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,SACNjD,KAAM,SACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,SACNjD,KAAM,SACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,GACNC,SAAU,gBAIhB,CACED,KAAM,YACNjD,KAAM,WACN4C,UAAW,kBAAM,0FC/HR,GACXQ,mBAAoB,qBACpBC,aAAc,eACdC,qBAAsB,uBACtBC,iBAAkB,mBAClBC,iBAAkB,oB,wHCHtB,IAAMC,EAAQ,CACVC,cAAe,CACXC,QAAQ,EACRC,QAAQ,EACRC,QAAQ,EACRC,QAAQ,GAEZC,QAAQ,EACRC,WAAW,EACXC,OAAO,EACPC,YAAa,GAGXC,GAAS,SACVrG,EAAKsF,mBADK,SACegB,EAAO5J,GAC7B4J,EAAMV,cAAN,eAA4BlJ,EAAK6J,QAAW7J,EAAK8J,OAF1C,IAIVxG,EAAKuF,aAJK,SAISkB,EAAQ/J,GACxB+J,EAAOR,OAASvJ,IALT,IAOVsD,EAAKwF,qBAPK,SAOiBc,EAAO5J,GAC/B4J,EAAMJ,UAAYxJ,IARX,IAUVsD,EAAKyF,iBAVK,SAUaa,EAAO5J,GAC3B4J,EAAMH,MAAQzJ,IAXP,IAaVsD,EAAK0F,iBAbK,SAaaY,EAAO5J,GAC3B4J,EAAMF,YAAc1J,IAdb,GAiBA,GACXiJ,QACAU,a,4qBChC8BK,GAA1Bf,E,EAAAA,MAAOU,E,EAAAA,UAEA,GACXA,YACAV,SCNA,EAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACA,EAAG,MAAM,CAAC0C,IAAI,MAAMC,YAAY,MAAMC,MAAM,CAAEC,WAAWjD,EAAIkD,MAAMC,OAAOnD,EAAIoD,YAAY,MAAO9C,MAAM,CAAC,GAAK,QAAQ,CAACF,EAAG,MAAM,CAAC0C,IAAI,QAAQE,MAAM,CAAEK,MAAQrD,EAAIsD,OAAO,KAAKH,OAAOnD,EAAIoD,YAAY,KAAKH,WAAWjD,EAAIuD,SAAUjD,MAAM,CAAC,GAAK,UAAU,CAACN,EAAIwD,GAAG,YAAY,QACjXjD,EAAkB,GCUtB,GACE1H,KADF,WAEI,MAAJ,IAGE4K,MAAF,CAEI,UAAJ,CACM,KAAN,OACM,QAAN,IAEI,OAAJ,CACM,KAAN,OACM,QAAN,IAEI,YAAJ,CACM,KAAN,OACM,QAAN,IAEI,MAAJ,CACM,KAAN,OACM,QAAN,IAEI,QAAJ,CACM,KAAN,OACM,QAAN,KAGEC,QA5BF,WA6BI,IAAIC,EAAMjI,SAASkI,eAAe,OAC9BC,EAAQnI,SAASkI,eAAe,SACpChE,OAAOkE,SAAW,WAChB,EAAN,gDACM,EAAN,kDAGI,EAAJ,gDACI,EAAJ,mDChDgV,I,wBCQ5U7C,EAAY,eACd,EACA,EACAV,GACA,EACA,KACA,WACA,MAIa,EAAAU,E,QCnBA,G,UAAA,CACb8C,MAAO,mBACPC,KAAM,OAENC,SAAS,CACHF,MAAO,OACPG,gBAAgB,CACdH,MAAM,YACNI,KAAK,4BAEPC,MAAM,CACJL,MAAM,OACNI,KAAK,8BAEPE,SAAS,CACRN,MAAM,SACNI,KAAK,4BAENG,mBAAmB,CACjBP,MAAM,OACNI,KAAK,sBAENI,uBAAuB,CACrBR,MAAM,QACNI,KAAK,qBAEPK,YAAY,CACVT,MAAM,OACNI,KAAK,0BAEPM,YAAY,CACVV,MAAM,OACNI,KAAK,2BAEPO,kBAAkB,CAChBX,MAAM,YACNI,KAAK,wBAEPQ,SAAS,CACPZ,MAAM,UACNI,KAAK,2BAIdS,QAAQ,CACNC,OAAO,MACPC,OAAO,MACPC,OAAO,OACPC,OAAO,YACPC,UAAU,8BACVC,MAAM,2BACNC,UAAU,+BACVC,MAAM,mBACNC,UAAU,OACVC,MAAM,qCACNC,UAAU,YACVC,MAAM,iCACNC,QAAQ,OACRzB,KAAM,QAGR0B,SAAS,CACPC,cAAe,OACfd,OAAO,UACPK,MAAM,uDACNJ,OAAO,WACPM,MAAM,oEACNL,OAAO,aACPO,MAAM,iEACNtB,KAAK,OACLyB,QAAQ,OACRG,KAAK,aACLC,MAAM,WACNC,QAAQ,QAGVC,QAAQ,CACN,CACE1H,KAAM,MAER,CACEA,KAAM,OACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,+BAAgCD,GAAI,QAASpC,KAAM,YAAa6H,KAAK,0BAA2BxD,MAAO,GACpH,CAAEhC,UAAW,iCAAkCD,GAAI,QAAQpC,KAAM,UAAW6H,KAAK,8BAA+BxD,MAAO,GACvH,CAAEhC,UAAW,2BAA4BD,GAAI,QAAQpC,KAAM,WAAY6H,KAAK,qBAAsBxD,MAAO,GACzG,CAAEhC,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,QAAS6H,KAAK,sBAAuBxD,MAAO,GACnG,CAAEhC,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,4BAA6BxD,MAAO,GACxG,CAAEhC,UAAW,yBAA0BD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,4BAA6BxD,MAAO,GAC1G,CAAEhC,UAAW,2BAA4BD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,uBAAwBxD,MAAO,GACvG,CAAEhC,UAAW,wBAAyBD,GAAI,QAAQpC,KAAM,YAAa6H,KAAK,uBAAwBxD,MAAO,GACzG,CAAEhC,UAAW,+BAAgCD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,+BAAgCxD,MAAO,KAGvH,CACErE,KAAM,OACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,YACjE,CAAExF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,MAAO6H,KAAK,eAChE,CAAExF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,MAAO6H,KAAK,eAChE,CAAExF,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,kBACpE,CAAExF,UAAW,yBAA0BD,GAAI,QAAQpC,KAAM,YAAa6H,KAAK,2BAG/E,CACE7H,KAAM,OACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,qBAAsBD,GAAI,QAAQpC,KAAM,SAAU8H,KAAK,EAAOD,KAAK,aAChF,CAAExF,UAAW,sBAAuBD,GAAI,QAAQpC,KAAM,UAAW8H,KAAK,EAAMD,KAAK,2CACjF,CAAExF,UAAW,sBAAuBD,GAAI,QAAQpC,KAAM,OAAQ8H,KAAK,EAAMD,KAAK,2CAC9E,CAAExF,UAAW,qBAAsBD,GAAI,QAAQpC,KAAM,aAAc8H,KAAK,EAAMD,KAAK,4CAGvF,CACE7H,KAAM,OACN2H,UAAU,EACVC,QAAS,CACP,CAAE5H,KAAM,QAAS6H,KAAK,uBAEtB,CAAE7H,KAAM,OAAQ6H,KAAK,wBC1Hb,GAEXnC,MAAM,OAQPG,gBAAgB,CACZH,MAAM,YACNc,OAAO,UACPV,KAAK,mEACLiC,QAAQ,6BAIRC,KAAK,CACDtC,MAAM,OACNuC,KAAK,CACD,CACIC,SAAS,MACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,8EACA,kFACA,2DAIZ,CACIH,SAAS,aACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,uDACA,qEACA,gDAIZ,CACIH,SAAS,QACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,uCACA,wEACA,8EACA,mEAIZ,CACIH,SAAS,WACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,kCACA,gGACA,+HACA,4CAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,6BACA,uEACA,2EACA,+CAQpBC,KAAK,CACD5C,MAAM,QACNuC,KAAK,CACD,CACIC,SAAS,MACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,6BACA,0CAIZ,CACIH,SAAS,OACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,sDACA,+BACA,2EAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,uBACA,qJAIZ,CACIH,SAAS,eACTC,aAAc,CACV,CAACC,UAAU,OAAOC,SAAS,CACvB,gEACA,qJACA,4EACA,2CAEJ,CAACD,UAAU,YAAYC,SAAS,CAC5B,iLACA,wDACA,2HACA,iEAEJ,CAACD,UAAU,OAAOC,SAAS,CACvB,8EACA,mHACA,6EAEJ,CAACD,UAAU,YAAYC,SAAS,CAC5B,+GACA,2DACA,kCACA,6DAOpBE,KAAK,CACD7C,MAAM,UACNuC,KAAK,CACD,CACIC,SAAS,QACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,6BACA,wCACA,oDAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,8BACA,mIACA,6FACA,oDAIZ,CACIH,SAAS,iBACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,4FACA,uFAUxBrC,SAAS,CACJN,MAAM,SACNI,KAAK,+HACLiC,QAAQ,MACRE,KAAK,CACD,CACGC,SAAS,KACTC,aAAa,2EAEhB,CACGD,SAAS,KACTC,aAAa,qFAEhB,CACGD,SAAS,KACTC,aAAa,0EAEjB,CACID,SAAS,KACTC,aAAa,oFAEjB,CACID,SAAS,KACTC,aAAa,2EAIpBK,SAAS,OACTC,aAAa,CACT,CACGP,SAAS,UACTC,aAAa,8CAEhB,CACGD,SAAS,UACTC,aAAa,+CAQzBjC,uBAAuB,CAClBR,MAAM,QACNI,KAAK,qHACLiC,QAAS,MACTE,KAAK,CACD,CACGC,SAAS,UACTC,aAAa,iFAEhB,CACGD,SAAS,UACTC,aAAa,gFAEjB,CACID,SAAS,YACTC,aAAa,+DAQpB5B,QAAQ,KACRmC,aAAa,MACbC,aAAa,OAMlB1C,mBAAmB,CACbP,MAAM,OACNI,KAAK,kFACLiC,QAAS,MACTE,KAAK,CACD,CACEC,SAAS,OACTC,aAAa,gFAEf,CACED,SAAS,OACTC,aAAa,4EAEjB,CACID,SAAS,OACTC,aAAa,kEACf,CACED,SAAS,QACTC,aAAa,+EAQzBhC,YAAY,CACRT,MAAM,OACNI,KAAK,mGAGL8C,SAAS,OACTC,MAAM,CACF,CACIX,SAAS,OACTC,aAAa,2EAEjB,CACID,SAAS,QACTC,aAAa,gFAEjB,CACID,SAAS,OACTC,aAAa,+EAGjB,CACID,SAAS,UACTC,aAAa,uFAEjB,CACID,SAAS,KACTC,aAAa,0EAGrBW,SAAS,OACTC,MAAM,CACF,CACIb,SAAS,SACTC,aAAa,8EAEjB,CACID,SAAS,OACTC,aAAa,2EAEjB,CACID,SAAS,OACTC,aAAa,0EAEjB,CACID,SAAS,QACTC,aAAa,8EAEjB,CACID,SAAS,QACTC,aAAa,yEAEjB,CACID,SAAS,KACTC,aAAa,+EAEjB,CACID,SAAS,KACTC,aAAa,2EAEjB,CACID,SAAS,KACTC,aAAa,kFAQzBpC,MAAM,CACFL,MAAM,OACNI,KAAK,iHACLiC,QAAS,MACTE,KAAK,CACD,CACIC,SAAS,OACTC,aAAa,wEAEjB,CACID,SAAS,iBACTC,aAAa,kFAEjB,CACID,SAAS,SACTC,aAAa,yCAGrBK,SAAS,OACRC,aAAa,CACT,CACGP,SAAS,MACTC,aAAa,mHAMzB/B,YAAY,CACNV,MAAM,OACNI,KAAK,uEACLiC,QAAQ,MACRE,KAAK,CACD,CACEC,SAAS,OACTC,aAAa,4EAEf,CACED,SAAS,OACTC,aAAa,mFAEjB,CACID,SAAS,OACTC,aAAa,4EAEjB,CACID,SAAS,OACTC,aAAa,iEAEjB,CACID,SAAS,gBACTC,aAAa,sGAEjB,CACID,SAAS,YACTC,aAAa,2EAEjB,CACID,SAAS,SACTC,aAAa,6EAEjB,CACID,SAAS,SACTC,aAAa,0EAEjB,CACID,SAAS,OACTC,aAAa,iFAEjB,CACID,SAAS,OACTC,aAAa,uHAQzB7B,SAAS,CACHZ,MAAM,UACNI,KAAK,mJACLiC,QAAQ,MACRE,KAAK,CACD,CACEC,SAAS,OACTC,aAAa,8EAEf,CACED,SAAS,OACTC,aAAa,kFAEjB,CACID,SAAS,aACTC,aAAa,yEAEjB,CACID,SAAS,SACTC,aAAa,uFAEjB,CACID,SAAS,OACTC,aAAa,gFAOzB9B,kBAAkB,CACdX,MAAM,YACNI,KAAK,4JACLiC,QAAQ,MACRE,KAAK,CACD,CACIC,SAAS,UACTC,aAAa,iFAEjB,CACID,SAAS,aACTC,aAAa,0GAEjB,CACID,SAAS,YACTC,aAAa,8FAGrB5B,QAAQ,KACRyC,YAAY,YACZC,YAAY,iCACZtD,KAAM,SC9dC,GAEXD,MAAM,OACNwD,QAAQ,uDACRC,QAAQ,mBACRC,WAAW,iDACXC,QAAQ,wCACR1D,KAAK,OAEL2D,IAAI,CACAC,KAAK,UACLC,KAAK,YAKTC,IAAI,CACAP,QAAQ,mJACRQ,QAAQ,OACRC,WAAW,YACXC,UAAU,0DACVC,WAAW,YACXC,UAAU,oDACVC,WAAW,YACXC,eAAe,2CACfC,eAAe,qDACfC,WAAW,YACXC,eAAe,iDACfC,eAAe,qDACfC,WAAW,UACXC,UAAU,CACNzD,MAAM,CACF0D,IAAI,QACJC,IAAI,SAEJC,KAAK,SAGT1D,MAAM,CACFwD,IAAI,QACJC,IAAI,SACJE,MAAM,UACND,KAAK,SAETxD,MAAM,CACFsD,IAAI,QACJC,IAAI,QACJE,MAAM,aAGdC,SAAS,WAIbC,IAAI,CACDC,QAAQ,sJACRlB,WAAW,iBACXmB,aAAa,8CACbC,aAAa,kEACblB,WAAW,iBACXmB,aAAa,0CACbC,aAAa,gGACbvB,QAAQ,WAIXwB,OAAO,CACJxF,MAAM,OACN0D,WAAW,uDACX+B,MAAM,SACNxB,WAAW,iBACXE,WAAW,kBACXE,WAAW,gBACXG,WAAW,kBACXR,QAAQ,WACR0B,OAAO,KACPC,MAAM,OAKTC,kBAAkB,CACd5F,MAAM,YAEN6F,YAAY,yFACZ3B,UAAU,yJACVE,UAAU,sKCtFH,GAEXpE,MAAM,OACNuC,KAAK,CACD,CACIvC,MAAM,gBACNI,KAAK,uDAET,CACIJ,MAAM,iBACNI,KAAK,2DAET,CACIJ,MAAM,kBACNI,KAAK,mECdF,GACX0F,YAAY,OACZ9F,MAAM,WACNI,KAAK,uHACL2F,QAAQ,GACRC,gBAAgB,CAAC,QAAQ,OAAO,QAGhCC,KAAM,OACNC,WAAW,CACP,CACExJ,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,QACL8F,KAAK,+jBAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,QACL8F,KAAK,0QAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAM,QACN8F,KAAM,2ZAER,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,oBACL8F,KAAK,gRAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,kBACL8F,KAAK,qfAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,SACL8F,KAAK,4RAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,QACL8F,KAAK,yJAMXgG,aAAc,kBACdC,aAAc,uBACdC,aAAc,CACZ,CACE5J,GAAI,EACJ6J,aAAc,cACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,yBACA,iCAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,6BACA,wBACA,yCACA,uDAIN,CACEhK,GAAI,EACJ6J,aAAc,YACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,+CACA,oCACA,8BACA,4BAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,mCACA,qBACA,qBACA,gHACA,6CACA,oCACA,iCAIN,CACEhK,GAAI,EACJ6J,aAAc,YACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,8BAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,4CACA,iHACA,mCACA,0CAIN,CACEhK,GAAI,EACJ6J,aAAc,UACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,kEAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,8CACA,kCACA,oEACA,6BACA,iCACA,kHACA,4BAIN,CACEhK,GAAI,EACJ6J,aAAc,cACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,gJAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,iCACA,qCACA,8CACA,yBACA,oBACA,wBACA,oCACA,kCAQR1E,QAAS,CAAC,KAAK,OAAO,SAAS,SAAS,QAAQ,QAAQ,QACxD4E,MAAO,KACPhI,KAAM,KACNiI,WAAW,CACT,CACE,CAAC,GAAG,OACJ,CACE,CACE,CACEnK,GAAG,UACHsD,MAAO,6EACP8G,OAAQ,mKACRC,KAAM,OACNL,QAAS,k2BACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2DACP8G,OAAQ,uGACRC,KAAM,OACNL,QAAS,kjCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,wEACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,i1CACThO,IAAK,uEAEP,CACEgE,GAAG,UACHsD,MAAO,8DACP8G,OAAQ,sDACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2DACP8G,OAAQ,yBACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,0CAIT,CACE,CACEgE,GAAG,UACHsD,MAAO,wDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0vBACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,qFACP8G,OAAQ,2BACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,mDAEP,CACEgE,GAAG,UACHsD,MAAO,iGACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,yuCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,8FACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,27BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,wGACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,45CACThO,IAAK,+CAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,oDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0iCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2EACP8G,OAAQ,8CACRC,KAAM,OACNL,QAAS,88CACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,i4BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,k6BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,6HACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,ghCACThO,IAAK,uEAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,qHACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,0iDACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,4FACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,qzCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,uFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,+0CACThO,IAAK,8CAEP,CACEgE,GAAG,UACHsD,MAAO,mCACP8G,OAAQ,0FACRC,KAAM,GACNL,QAAS,mdACThO,IAAK,6EAEP,CACEgE,GAAG,UACHsD,MAAO,iEACP8G,OAAQ,kBACRC,KAAM,OACNL,QAAS,g/BACThO,IAAK,kDAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,2HACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,4hCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,gFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,iDAKb,CACE,CAAC,EAAE,SACH,CACE,CACE,CACEgE,GAAG,YACHsD,MAAO,8FACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,27BACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,6HACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,ghCACThO,IAAK,sEAEP,CACEgE,GAAG,YACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,k6BACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,qHACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,0iDACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,i4BACThO,IAAK,4DAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,wGACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,45CACThO,IAAK,8CAEP,CACEgE,GAAG,cACHsD,MAAO,4FACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,qzCACThO,IAAK,gDAEP,CACEgE,GAAG,cACHsD,MAAO,uFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,+0CACThO,IAAK,8CAEP,CACEgE,GAAG,cACHsD,MAAO,2HACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,4hCACThO,IAAK,gDAEP,CACEgE,GAAG,cACHsD,MAAO,gFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,iDAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,wEACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,i1CACThO,IAAK,0EAKb,CACE,CAAC,EAAE,SACH,CACE,CACE,CACEgE,GAAG,YACHsD,MAAO,2DACP8G,OAAQ,uGACRC,KAAM,OACNL,QAAS,kjCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,6EACP8G,OAAQ,mKACRC,KAAM,OACNL,QAAS,k2BACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,8DACP8G,OAAQ,sDACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,2DACP8G,OAAQ,yBACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,yCAEP,CACEgE,GAAG,YACHsD,MAAO,wDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0vBACThO,IAAK,iDAGT,CACE,CACEgE,GAAG,YACHsD,MAAO,2EACP8G,OAAQ,8CACRC,KAAM,OACNL,QAAS,88CACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,oDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0iCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,mCACP8G,OAAQ,0FACRC,KAAM,OACNL,QAAS,mdACThO,IAAK,gFAKb,CACE,CAAC,EAAE,YACH,CACE,CACE,CACEgE,GAAG,eACHsD,MAAO,qFACP8G,OAAQ,2BACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,mDAEP,CACEgE,GAAG,eACHsD,MAAO,iEACP8G,OAAQ,kBACRC,KAAM,OACNL,QAAS,g/BACThO,IAAK,oDAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,iGACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,yuCACThO,IAAK,qDC/jBJ,GAEXkL,IAAK,CACHoD,KAAM,KACN9G,SAAS,OACTW,QAAQ,OACRc,SAAS,OACTsF,MAAM,OACNjH,MAAM,YAIRkH,cACAC,kBACAC,iBACAC,kBACAC,gBC7BW,GACXtH,MAAO,kDACPC,KAAM,aAENC,SAAS,CACHF,MAAO,WACPG,gBAAgB,CACdH,MAAM,gCACNI,KAAK,qGAEPC,MAAM,CACJL,MAAM,yBACNI,KAAK,wGAEPE,SAAS,CACRN,MAAM,kBACNI,KAAK,6GAENG,mBAAmB,CACjBP,MAAM,oBACNI,KAAK,yFAENI,uBAAuB,CACrBR,MAAM,eACNI,KAAK,0DAEPK,YAAY,CACVT,MAAM,mBACNI,KAAK,wGAEPM,YAAY,CACVV,MAAM,mBACNI,KAAK,6EAEPO,kBAAkB,CAChBX,MAAM,gBACNI,KAAK,4DAEPQ,SAAS,CACPZ,MAAM,uBACNI,KAAK,qHAIdS,QAAQ,CACNC,OAAO,MACPC,OAAO,MACPC,OAAO,UACPC,OAAO,QACPC,UAAU,8BACVC,MAAM,iDACNC,UAAU,+BACVC,MAAM,6CACNC,UAAU,UACVC,MAAM,sGACNC,UAAU,4CACVC,MAAM,2KACNC,QAAQ,UACRzB,KAAM,iBAGR0B,SAAS,CACPC,cAAe,YACfd,OAAO,wCACPK,MAAM,mJACNJ,OAAO,sCACPM,MAAM,yNACNL,OAAO,gCACPO,MAAM,mKACNtB,KAAK,gBACLyB,QAAQ,UACRG,KAAK,sCACLC,MAAM,sCACNC,QAAQ,gBAGZC,QAAQ,CACN,CACE1H,KAAM,QAER,CACEA,KAAM,WACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,+BAAgCD,GAAI,QAAQpC,KAAM,gCAAiC6H,KAAK,0BAA2BxD,MAAO,GACvI,CAAEhC,UAAW,iCAAkCD,GAAI,QAAQpC,KAAM,uBAAwB6H,KAAK,8BAA+BxD,MAAO,GACpI,CAAEhC,UAAW,2BAA4BD,GAAI,QAAQpC,KAAM,kBAAmB6H,KAAK,qBAAsBxD,MAAO,GAChH,CAAEhC,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,eAAgB6H,KAAK,sBAAuBxD,MAAO,GAC1G,CAAEhC,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,mBAAoB6H,KAAK,4BAA6BxD,MAAO,GACpH,CAAEhC,UAAW,yBAA0BD,GAAI,QAAQpC,KAAM,mBAAoB6H,KAAK,4BAA6BxD,MAAO,GACtH,CAAEhC,UAAW,2BAA4BD,GAAI,QAAQpC,KAAM,yBAA0B6H,KAAK,uBAAwBxD,MAAO,GACzH,CAAEhC,UAAW,wBAAyBD,GAAI,QAAQpC,KAAM,gBAAiB6H,KAAK,uBAAwBxD,MAAO,GAC7G,CAAEhC,UAAW,+BAAgCD,GAAI,QAAQpC,KAAM,oBAAqB6H,KAAK,+BAAgCxD,MAAO,KAGpI,CACErE,KAAM,kBACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,kBAAmB6H,KAAK,YAC5E,CAAExF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,MAAO6H,KAAK,eAChE,CAAExF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,MAAO6H,KAAK,eAChE,CAAExF,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,UAAW6H,KAAK,kBACvE,CAAExF,UAAW,yBAA0BD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,2BAG1E,CACE7H,KAAM,YACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,qBAAsBD,GAAI,QAAQpC,KAAM,YAAa8H,KAAK,EAAOD,KAAK,aACnF,CAAExF,UAAW,sBAAuBD,GAAI,QAAQpC,KAAM,OAAQ8H,KAAK,EAAMD,KAAK,2CAC9E,CAAExF,UAAW,sBAAuBD,GAAI,QAAQpC,KAAM,OAAQ8H,KAAK,EAAMD,KAAK,2CAC9E,CAAExF,UAAW,qBAAsBD,GAAI,QAAQpC,KAAM,MAAO8H,KAAK,EAAMD,KAAK,4CAGhF,CACE7H,KAAM,WACN2H,UAAU,EACVC,QAAS,CACP,CAAE5H,KAAM,oBAAqB6H,KAAK,uBAElC,CAAE7H,KAAM,UAAW6H,KAAK,uBC1HjB,GAEVnC,MAAM,WAQPG,gBAAgB,CACZH,MAAM,gCACNI,KAAK,uOACLiC,QAAQ,6BAIRC,KAAK,CACDtC,MAAM,yBACNuC,KAAK,CACD,CACIC,SAAS,MACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,yNACA,4MACA,mKAIZ,CACIH,SAAS,aACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,iHACA,iMACA,mFAIZ,CACIH,SAAS,QACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,+OACA,yTACA,4OACA,iFAIZ,CACIH,SAAS,WACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,gQACA,sPACA,ueAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,8EACA,ycACA,yJACA,sKAOpBC,KAAK,CACD5C,MAAM,0BACNuC,KAAK,CACD,CACIC,SAAS,MACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,oIACA,0IAIZ,CACIH,SAAS,OACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,iNACA,mEACA,kKAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,qIACA,wXAIZ,CACIH,SAAS,eACTC,aAAc,CACV,CAACC,UAAU,OAAOC,SAAS,CACvB,+RACA,6cACA,8NACA,qHAEH,CAACD,UAAU,YAAYC,SAAS,CAC7B,yhBACA,mMACA,icACA,yIAEJ,CAACD,UAAU,OAAOC,SAAS,CACvB,uUACA,6dACA,gGACA,+HAEJ,CAACD,UAAU,YAAYC,SAAS,CAC5B,2WACA,wQACA,4RAOpBE,KAAK,CACD7C,MAAM,qBACNuC,KAAK,CACD,CACIC,SAAS,QACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,iDACA,4IACA,kKAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,0GACA,2aACA,2QACA,4FAIZ,CACIH,SAAS,iBACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,yWACA,kRAUxBrC,SAAS,CACJN,MAAM,kBACNI,KAAK,oVACLiC,QAAQ,kBACRE,KAAK,CACD,CACGC,SAAS,UACTC,aAAa,iIAEhB,CACGD,SAAS,eACTC,aAAa,8VAEhB,CACGD,SAAS,cACTC,aAAa,8JAEjB,CACID,SAAS,WACTC,aAAa,yJAEjB,CACID,SAAS,cACTC,aAAa,uDAIpBK,SAAS,eACTC,aAAa,CACT,CACGP,SAAS,4CACTC,aAAa,uJAEhB,CACGD,SAAS,4CACTC,aAAa,mMAQzBjC,uBAAuB,CAClBR,MAAM,eACNI,KAAK,wYACLiC,QAAS,kBACTE,KAAK,CACD,CACGC,SAAS,mCACTC,aAAa,0UAEhB,CACGD,SAAS,mCACTC,aAAa,2UAEjB,CACID,SAAS,qCACTC,aAAa,yVAQpB5B,QAAQ,oBACRmC,aAAa,MACbC,aAAa,OAMlB1C,mBAAmB,CACbP,MAAM,oBACNI,KAAK,uRACLiC,QAAS,kBACTE,KAAK,CACD,CACEC,SAAS,kBACTC,aAAa,2RAEf,CACED,SAAS,cACTC,aAAa,mPAEjB,CACID,SAAS,cACTC,aAAa,kOACf,CACED,SAAS,qBACTC,aAAa,iPAQzBhC,YAAY,CACRT,MAAM,mBACNI,KAAK,2cAGL8C,SAAS,oBACTC,MAAM,CACF,CACIX,SAAS,wBACTC,aAAa,mZAEjB,CACID,SAAS,oBACTC,aAAa,wTAEjB,CACID,SAAS,oBACTC,aAAa,0QAGjB,CACID,SAAS,kCACTC,aAAa,kZAEjB,CACID,SAAS,WACTC,aAAa,+PAGrBW,SAAS,oBACTC,MAAM,CACF,CACIb,SAAS,iCACTC,aAAa,kQAEjB,CACID,SAAS,oBACTC,aAAa,0OAEjB,CACID,SAAS,mBACTC,aAAa,gOAEjB,CACID,SAAS,wBACTC,aAAa,yPAEjB,CACID,SAAS,yBACTC,aAAa,yPAEjB,CACID,SAAS,SACTC,aAAa,6QAEjB,CACID,SAAS,SACTC,aAAa,6PAEjB,CACID,SAAS,kBACTC,aAAa,mMAQzBpC,MAAM,CACFL,MAAM,yBACNI,KAAK,iUACLiC,QAAS,kBACTE,KAAK,CACD,CACIC,SAAS,oBACTC,aAAa,gSAEjB,CACID,SAAS,mCACTC,aAAa,6NAEjB,CACID,SAAS,4BACTC,aAAa,gFAGrBK,SAAS,OACRC,aAAa,CACT,CACGP,SAAS,iBACTC,aAAa,+QAMzB/B,YAAY,CACNV,MAAM,mBACNI,KAAK,wNACLiC,QAAQ,kBACRE,KAAK,CACD,CACEC,SAAS,eACTC,aAAa,gQAEf,CACED,SAAS,mBACTC,aAAa,gWAEjB,CACID,SAAS,oBACTC,aAAa,iQAEjB,CACID,SAAS,eACTC,aAAa,gOAEjB,CACID,SAAS,sBACTC,aAAa,yLAEjB,CACID,SAAS,2BACTC,aAAa,yJAEjB,CACID,SAAS,0BACTC,aAAa,iNAEjB,CACID,SAAS,sBACTC,aAAa,2KAEjB,CACID,SAAS,kBACTC,aAAa,kKAEjB,CACID,SAAS,iBACTC,aAAa,8QAQzB7B,SAAS,CACHZ,MAAM,uBACNI,KAAK,ojBACLiC,QAAQ,kBACRE,KAAK,CACD,CACEC,SAAS,qBACTC,aAAa,mRAEf,CACED,SAAS,qBACTC,aAAa,gSAEjB,CACID,SAAS,uBACTC,aAAa,8LAEjB,CACID,SAAS,4CACTC,aAAa,6KAEjB,CACID,SAAS,sBACTC,aAAa,iLAOzB9B,kBAAkB,CACdX,MAAM,gBACNI,KAAK,ygBACLiC,QAAQ,kBACRE,KAAK,CACD,CACIC,SAAS,wCACTC,aAAa,gQAEjB,CACID,SAAS,wCACTC,aAAa,yQAEjB,CACID,SAAS,0CACTC,aAAa,4TAGrB5B,QAAQ,oBACRyC,YAAY,OACZC,YAAY,2KACZtD,KAAM,eC1dK,GAEXD,MAAM,kBACNwD,QAAQ,uKACRC,QAAQ,2DACRC,WAAW,8IACXC,QAAQ,2KACR1D,KAAK,OAEL2D,IAAI,CACAC,KAAK,UACLC,KAAK,WAKTC,IAAI,CACAP,QAAQ,2dACRQ,QAAQ,OACRC,WAAW,iCACXC,UAAU,yKACVC,WAAW,2BACXC,UAAU,4IACVC,WAAW,8BACXC,eAAe,yGACfC,eAAe,iHACfC,WAAW,8BACXC,eAAe,uHACfC,eAAe,4HACfC,WAAW,mBACXC,UAAU,CACNzD,MAAM,CACF0D,IAAI,cACJC,IAAI,sBAEJC,KAAK,oBAGT1D,MAAM,CACFwD,IAAI,YACJC,IAAI,YACJE,MAAM,gBACND,KAAK,aAETxD,MAAM,CACFsD,IAAI,YACJC,IAAI,yBACJE,MAAM,wBAGdC,SAAS,mBAIbC,IAAI,CACDC,QAAQ,uVACRlB,WAAW,gDACXmB,aAAa,0FACbC,aAAa,6HAEblB,WAAW,6CACXmB,aAAa,4FACbC,aAAa,sOACbvB,QAAQ,mBAIXwB,OAAO,CACJxF,MAAM,UACN0D,WAAW,0LACX+B,MAAM,SACNxB,WAAW,mDACXE,WAAW,mDACXE,WAAW,iDACXG,WAAW,wDACXR,QAAQ,sBACR0B,OAAO,iBACPC,MAAM,aAKTC,kBAAkB,CACd5F,MAAM,OACN6F,YAAY,8UACZ3B,UAAU,odACVE,UAAU,0XCvFP,GAEXpE,MAAM,YACNuC,KAAK,CACD,CACIvC,MAAM,OACNc,OAAO,wCACPV,KAAK,oJAET,CACIJ,MAAM,OACNc,OAAO,4CACPV,KAAK,2NAET,CACIJ,MAAM,MACNc,OAAO,6CACPV,KAAK,sKCjBF,GACb0F,YAAY,WACZ9F,MAAM,oBACNI,KAAK,yaACL2F,QAAQ,GACRC,gBAAgB,CAAC,iCAAiC,QAAQ,WAG1DC,KAAM,OACNC,WAAW,CACT,CACExJ,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,eACL8F,KAAK,yzCAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,eACL8F,KAAK,k4BAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAM,eACN8F,KAAM,u0CAER,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,qBACL8F,KAAK,81BAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,mBACL8F,KAAK,ohDAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,mBACL8F,KAAK,gxBAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,cACL8F,KAAK,8WAQTgG,aAAc,oCACdC,aAAc,uBACdC,aAAc,CACZ,CACE5J,GAAI,EACJ6J,aAAc,6CACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,yCACA,2FAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,gFACA,uFACA,+HACA,+FAIN,CACEhK,GAAI,EACJ6J,aAAc,sCACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,6GACA,qJACA,mIACA,qFACA,wFAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,qGACA,wFACA,uEACA,4EACA,2EACA,sEACA,iEACA,mDAIN,CACEhK,GAAI,EACJ6J,aAAc,mCACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,qGAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,8GACA,8TACA,sIACA,6HAIN,CACEhK,GAAI,EACJ6J,aAAc,6BACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,8HAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,2HACA,kKACA,gEACA,4FACA,8KACA,mDAIN,CACEhK,GAAI,EACJ6J,aAAc,6CACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,qOACA,6GACA,sFACA,oEAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,sFACA,mGACA,uJACA,0DACA,8CACA,iFACA,oDAOP1E,QAAS,CAAC,MAAM,mBAAmB,4BAA4B,2BAA2B,eAAe,kBAAkB,kBAC3H4E,MAAO,SACPhI,KAAM,KACNiI,WAAW,CACV,CACE,CAAC,GAAG,OACJ,CACE,CACE,CACEnK,GAAG,UACHsD,MAAO,6EACP8G,OAAQ,mKACRC,KAAM,OACNL,QAAS,k2BACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2DACP8G,OAAQ,uGACRC,KAAM,OACNL,QAAS,kjCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,wEACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,i1CACThO,IAAK,uEAEP,CACEgE,GAAG,UACHsD,MAAO,8DACP8G,OAAQ,sDACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2DACP8G,OAAQ,yBACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,0CAIT,CACE,CACEgE,GAAG,UACHsD,MAAO,wDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0vBACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,qFACP8G,OAAQ,2BACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,mDAEP,CACEgE,GAAG,UACHsD,MAAO,iGACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,yuCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,8FACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,27BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,wGACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,45CACThO,IAAK,+CAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,oDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0iCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2EACP8G,OAAQ,8CACRC,KAAM,OACNL,QAAS,88CACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,i4BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,k6BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,6HACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,ghCACThO,IAAK,uEAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,qHACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,0iDACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,4FACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,qzCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,uFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,+0CACThO,IAAK,8CAEP,CACEgE,GAAG,UACHsD,MAAO,mCACP8G,OAAQ,0FACRC,KAAM,GACNL,QAAS,mdACThO,IAAK,6EAEP,CACEgE,GAAG,UACHsD,MAAO,iEACP8G,OAAQ,kBACRC,KAAM,OACNL,QAAS,g/BACThO,IAAK,kDAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,2HACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,4hCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,gFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,iDAKb,CACE,CAAC,EAAE,SACH,CACE,CACE,CACEgE,GAAG,YACHsD,MAAO,8FACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,27BACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,6HACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,ghCACThO,IAAK,sEAEP,CACEgE,GAAG,YACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,k6BACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,qHACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,0iDACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,i4BACThO,IAAK,4DAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,wGACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,45CACThO,IAAK,8CAEP,CACEgE,GAAG,cACHsD,MAAO,4FACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,qzCACThO,IAAK,gDAEP,CACEgE,GAAG,cACHsD,MAAO,uFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,+0CACThO,IAAK,8CAEP,CACEgE,GAAG,cACHsD,MAAO,2HACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,4hCACThO,IAAK,gDAEP,CACEgE,GAAG,cACHsD,MAAO,gFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,iDAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,wEACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,i1CACThO,IAAK,0EAKb,CACE,CAAC,EAAE,SACH,CACE,CACE,CACEgE,GAAG,YACHsD,MAAO,2DACP8G,OAAQ,uGACRC,KAAM,OACNL,QAAS,kjCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,6EACP8G,OAAQ,mKACRC,KAAM,OACNL,QAAS,k2BACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,8DACP8G,OAAQ,sDACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,2DACP8G,OAAQ,yBACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,yCAEP,CACEgE,GAAG,YACHsD,MAAO,wDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0vBACThO,IAAK,iDAGT,CACE,CACEgE,GAAG,YACHsD,MAAO,2EACP8G,OAAQ,8CACRC,KAAM,OACNL,QAAS,88CACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,oDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0iCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,mCACP8G,OAAQ,0FACRC,KAAM,OACNL,QAAS,mdACThO,IAAK,gFAKb,CACE,CAAC,EAAE,YACH,CACE,CACE,CACEgE,GAAG,eACHsD,MAAO,qFACP8G,OAAQ,2BACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,mDAEP,CACEgE,GAAG,eACHsD,MAAO,iEACP8G,OAAQ,kBACRC,KAAM,OACNL,QAAS,g/BACThO,IAAK,oDAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,iGACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,yuCACThO,IAAK,qDC1kBF,GACXkL,IAAK,CACHoD,KAAM,OACN9G,SAAS,WACTW,QAAQ,kBACRc,SAAS,YACTsF,MAAM,WACNpF,KAAK,QAEPqF,cACAC,kBACAC,iBACAC,kBACAC,gB,yrBCHJ,IAAMC,EAAalK,OAAO3H,UAAUF,KACpC6H,OAAO3H,UAAUF,KAAO,SAAcgS,GACpC,OAAOD,EAAW3R,KAAKsG,KAAMsL,GAAUC,MAAM,SAAA1N,GAAK,OAAGA,KAGvDoD,OAAIC,IAAIsK,QACRvK,OAAIwK,OAAOC,eAAgB,EAE3BzK,OAAID,UAAU,SAAS2K,GAEvBC,EAAOC,WAAW,SAACC,EAAIC,EAAMnE,GAC3BA,IACA,IAAMoE,EAAW,CACf,CAAE3K,KAAM,aAAcb,GAAI,MAAOC,UAAW,uBAC5C,CAAEY,KAAM,WAAYb,GAAI,MAAOC,UAAW,sBAC1C,CAAEY,KAAM,YAAab,GAAI,MAAOC,UAAW,uBAC3C,CAAEY,KAAM,+BAAgCb,GAAI,MAAOC,UAAW,iCAC9D,CAAEY,KAAM,4BAA6Bb,GAAI,MAAOC,UAAW,2BAC3D,CAAEY,KAAM,8BAA+Bb,GAAI,MAAOC,UAAW,mCAC7D,CAAEY,KAAM,cAAeb,GAAI,MAAOC,UAAW,sBAC7C,CAAEY,KAAM,cAAeb,GAAI,MAAOC,UAAW,sBAC7C,CAAEY,KAAM,iBAAkBb,GAAI,MAAOC,UAAW,yBAChD,CAAEY,KAAM,wBAAyBb,GAAI,MAAOC,UAAW,2BACvD,CAAEY,KAAM,4BAA6Bb,GAAI,MAAOC,UAAW,yBAC3D,CAAEY,KAAM,0BAA2Bb,GAAI,MAAOC,UAAW,iCACzD,CAAEY,KAAM,qBAAsBb,GAAI,MAAOC,UAAW,6BACpD,CAAEY,KAAM,sBAAuBb,GAAI,MAAOC,UAAW,yBACrD,CAAEY,KAAM,uBAAwBb,GAAI,MAAOC,UAAW,6BACtD,CAAEY,KAAM,uBAAwBb,GAAI,MAAOC,UAAW,4BAExDuL,EAASC,QAAQ,SAACvJ,GACboJ,EAAGzK,OAASqB,EAAKrB,MAClBd,eAAiBmC,EAAKlC,GAAIkC,EAAKjC,eAKrCQ,OAAIC,IAAIgL,QACR,IAAMC,EAAO,IAAID,OAAQ,CACvBE,OAAQ,KAERC,SAAU,CACHC,KACAC,QAILX,EAAOY,UAAU,WACf7M,OAAO8M,SAAS,EAAE,KAEtB,IAAMC,EAAQ,IAAIlB,OAAKmB,MAAT,KAAmBA,IAEnC,IAAI1L,OAAI,CACN2K,SACAO,KAAKA,EACLO,MAAMA,EACN5M,OAAQ,SAAA8M,GAAC,OAAIA,EAAEC,iBACdC,OAAO,S,2DCzEV,yBAAke,EAAG","file":"js/app.7cb86f6b.js","sourcesContent":[" \t// install a JSONP callback for chunk loading\n \tfunction webpackJsonpCallback(data) {\n \t\tvar chunkIds = data[0];\n \t\tvar moreModules = data[1];\n \t\tvar executeModules = data[2];\n\n \t\t// add \"moreModules\" to the modules object,\n \t\t// then flag all \"chunkIds\" as loaded and fire callback\n \t\tvar moduleId, chunkId, i = 0, resolves = [];\n \t\tfor(;i < chunkIds.length; i++) {\n \t\t\tchunkId = chunkIds[i];\n \t\t\tif(installedChunks[chunkId]) {\n \t\t\t\tresolves.push(installedChunks[chunkId][0]);\n \t\t\t}\n \t\t\tinstalledChunks[chunkId] = 0;\n \t\t}\n \t\tfor(moduleId in moreModules) {\n \t\t\tif(Object.prototype.hasOwnProperty.call(moreModules, moduleId)) {\n \t\t\t\tmodules[moduleId] = moreModules[moduleId];\n \t\t\t}\n \t\t}\n \t\tif(parentJsonpFunction) parentJsonpFunction(data);\n\n \t\twhile(resolves.length) {\n \t\t\tresolves.shift()();\n \t\t}\n\n \t\t// add entry modules from loaded chunk to deferred list\n \t\tdeferredModules.push.apply(deferredModules, executeModules || []);\n\n \t\t// run deferred modules when all chunks ready\n \t\treturn checkDeferredModules();\n \t};\n \tfunction checkDeferredModules() {\n \t\tvar result;\n \t\tfor(var i = 0; i < deferredModules.length; i++) {\n \t\t\tvar deferredModule = deferredModules[i];\n \t\t\tvar fulfilled = true;\n \t\t\tfor(var j = 1; j < deferredModule.length; j++) {\n \t\t\t\tvar depId = deferredModule[j];\n \t\t\t\tif(installedChunks[depId] !== 0) fulfilled = false;\n \t\t\t}\n \t\t\tif(fulfilled) {\n \t\t\t\tdeferredModules.splice(i--, 1);\n \t\t\t\tresult = __webpack_require__(__webpack_require__.s = deferredModule[0]);\n \t\t\t}\n \t\t}\n \t\treturn result;\n \t}\n\n \t// The module cache\n \tvar installedModules = {};\n\n \t// object to store loaded CSS chunks\n \tvar installedCssChunks = {\n \t\t\"app\": 0\n \t}\n\n \t// object to store loaded and loading chunks\n \t// undefined = chunk not loaded, null = chunk preloaded/prefetched\n \t// Promise = chunk loading, 0 = chunk loaded\n \tvar installedChunks = {\n \t\t\"app\": 0\n \t};\n\n \tvar deferredModules = [];\n\n \t// script path function\n \tfunction jsonpScriptSrc(chunkId) {\n \t\treturn __webpack_require__.p + \"js/\" + ({}[chunkId]||chunkId) + \".\" + {\"chunk-150f2a22\":\"7c060552\",\"chunk-48549da8\":\"7eff62c5\",\"chunk-5597a18c\":\"5609a97f\",\"chunk-6b0a8c8b\":\"7dcab209\",\"chunk-7cbeefd3\":\"5bb94f76\",\"chunk-85d5875a\":\"d017c31d\",\"chunk-9a4dc728\":\"d30fc624\",\"chunk-a0684f72\":\"07329ef9\",\"chunk-e8e00e3e\":\"e4354a24\",\"chunk-f5db0f22\":\"6eceb54e\",\"chunk-2e54a88e\":\"a7c81b33\",\"chunk-313ec15e\":\"ffba92c7\",\"chunk-395ac447\":\"5385d2dd\",\"chunk-4e2c65a4\":\"cde0744a\",\"chunk-5c7caa12\":\"a242be8a\",\"chunk-5ec60ba6\":\"8d8cdc28\",\"chunk-638b533c\":\"73943361\",\"chunk-6cb17c0e\":\"b6836812\",\"chunk-70fb8741\":\"18a83dff\",\"chunk-a84b203c\":\"e971ec42\",\"chunk-b9b25720\":\"8296452f\",\"chunk-f884b720\":\"9777babd\"}[chunkId] + \".js\"\n \t}\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n \t// This file contains only the entry chunk.\n \t// The chunk loading function for additional chunks\n \t__webpack_require__.e = function requireEnsure(chunkId) {\n \t\tvar promises = [];\n\n\n \t\t// mini-css-extract-plugin CSS loading\n \t\tvar cssChunks = {\"chunk-150f2a22\":1,\"chunk-48549da8\":1,\"chunk-5597a18c\":1,\"chunk-6b0a8c8b\":1,\"chunk-7cbeefd3\":1,\"chunk-85d5875a\":1,\"chunk-9a4dc728\":1,\"chunk-a0684f72\":1,\"chunk-e8e00e3e\":1,\"chunk-f5db0f22\":1,\"chunk-2e54a88e\":1,\"chunk-313ec15e\":1,\"chunk-395ac447\":1,\"chunk-4e2c65a4\":1,\"chunk-5c7caa12\":1,\"chunk-5ec60ba6\":1,\"chunk-638b533c\":1,\"chunk-6cb17c0e\":1,\"chunk-70fb8741\":1,\"chunk-a84b203c\":1,\"chunk-b9b25720\":1,\"chunk-f884b720\":1};\n \t\tif(installedCssChunks[chunkId]) promises.push(installedCssChunks[chunkId]);\n \t\telse if(installedCssChunks[chunkId] !== 0 && cssChunks[chunkId]) {\n \t\t\tpromises.push(installedCssChunks[chunkId] = new Promise(function(resolve, reject) {\n \t\t\t\tvar href = \"css/\" + ({}[chunkId]||chunkId) + \".\" + {\"chunk-150f2a22\":\"7035f3ad\",\"chunk-48549da8\":\"7510da79\",\"chunk-5597a18c\":\"04c58924\",\"chunk-6b0a8c8b\":\"2dd1ed80\",\"chunk-7cbeefd3\":\"37d37332\",\"chunk-85d5875a\":\"bb49213a\",\"chunk-9a4dc728\":\"ab0a5b59\",\"chunk-a0684f72\":\"b60fe50c\",\"chunk-e8e00e3e\":\"135369ce\",\"chunk-f5db0f22\":\"62cd94a6\",\"chunk-2e54a88e\":\"c6f55108\",\"chunk-313ec15e\":\"219c18b0\",\"chunk-395ac447\":\"9b2ac1f7\",\"chunk-4e2c65a4\":\"226174e8\",\"chunk-5c7caa12\":\"304ec061\",\"chunk-5ec60ba6\":\"02a39819\",\"chunk-638b533c\":\"016888cb\",\"chunk-6cb17c0e\":\"0b9bc176\",\"chunk-70fb8741\":\"05609e40\",\"chunk-a84b203c\":\"071e20c2\",\"chunk-b9b25720\":\"f9ce2cc9\",\"chunk-f884b720\":\"7f6930f1\"}[chunkId] + \".css\";\n \t\t\t\tvar fullhref = __webpack_require__.p + href;\n \t\t\t\tvar existingLinkTags = document.getElementsByTagName(\"link\");\n \t\t\t\tfor(var i = 0; i < existingLinkTags.length; i++) {\n \t\t\t\t\tvar tag = existingLinkTags[i];\n \t\t\t\t\tvar dataHref = tag.getAttribute(\"data-href\") || tag.getAttribute(\"href\");\n \t\t\t\t\tif(tag.rel === \"stylesheet\" && (dataHref === href || dataHref === fullhref)) return resolve();\n \t\t\t\t}\n \t\t\t\tvar existingStyleTags = document.getElementsByTagName(\"style\");\n \t\t\t\tfor(var i = 0; i < existingStyleTags.length; i++) {\n \t\t\t\t\tvar tag = existingStyleTags[i];\n \t\t\t\t\tvar dataHref = tag.getAttribute(\"data-href\");\n \t\t\t\t\tif(dataHref === href || dataHref === fullhref) return resolve();\n \t\t\t\t}\n \t\t\t\tvar linkTag = document.createElement(\"link\");\n \t\t\t\tlinkTag.rel = \"stylesheet\";\n \t\t\t\tlinkTag.type = \"text/css\";\n \t\t\t\tlinkTag.onload = resolve;\n \t\t\t\tlinkTag.onerror = function(event) {\n \t\t\t\t\tvar request = event && event.target && event.target.src || fullhref;\n \t\t\t\t\tvar err = new Error(\"Loading CSS chunk \" + chunkId + \" failed.\\n(\" + request + \")\");\n \t\t\t\t\terr.code = \"CSS_CHUNK_LOAD_FAILED\";\n \t\t\t\t\terr.request = request;\n \t\t\t\t\tdelete installedCssChunks[chunkId]\n \t\t\t\t\tlinkTag.parentNode.removeChild(linkTag)\n \t\t\t\t\treject(err);\n \t\t\t\t};\n \t\t\t\tlinkTag.href = fullhref;\n\n \t\t\t\tvar head = document.getElementsByTagName(\"head\")[0];\n \t\t\t\thead.appendChild(linkTag);\n \t\t\t}).then(function() {\n \t\t\t\tinstalledCssChunks[chunkId] = 0;\n \t\t\t}));\n \t\t}\n\n \t\t// JSONP chunk loading for javascript\n\n \t\tvar installedChunkData = installedChunks[chunkId];\n \t\tif(installedChunkData !== 0) { // 0 means \"already installed\".\n\n \t\t\t// a Promise means \"currently loading\".\n \t\t\tif(installedChunkData) {\n \t\t\t\tpromises.push(installedChunkData[2]);\n \t\t\t} else {\n \t\t\t\t// setup Promise in chunk cache\n \t\t\t\tvar promise = new Promise(function(resolve, reject) {\n \t\t\t\t\tinstalledChunkData = installedChunks[chunkId] = [resolve, reject];\n \t\t\t\t});\n \t\t\t\tpromises.push(installedChunkData[2] = promise);\n\n \t\t\t\t// start chunk loading\n \t\t\t\tvar script = document.createElement('script');\n \t\t\t\tvar onScriptComplete;\n\n \t\t\t\tscript.charset = 'utf-8';\n \t\t\t\tscript.timeout = 120;\n \t\t\t\tif (__webpack_require__.nc) {\n \t\t\t\t\tscript.setAttribute(\"nonce\", __webpack_require__.nc);\n \t\t\t\t}\n \t\t\t\tscript.src = jsonpScriptSrc(chunkId);\n\n \t\t\t\tonScriptComplete = function (event) {\n \t\t\t\t\t// avoid mem leaks in IE.\n \t\t\t\t\tscript.onerror = script.onload = null;\n \t\t\t\t\tclearTimeout(timeout);\n \t\t\t\t\tvar chunk = installedChunks[chunkId];\n \t\t\t\t\tif(chunk !== 0) {\n \t\t\t\t\t\tif(chunk) {\n \t\t\t\t\t\t\tvar errorType = event && (event.type === 'load' ? 'missing' : event.type);\n \t\t\t\t\t\t\tvar realSrc = event && event.target && event.target.src;\n \t\t\t\t\t\t\tvar error = new Error('Loading chunk ' + chunkId + ' failed.\\n(' + errorType + ': ' + realSrc + ')');\n \t\t\t\t\t\t\terror.type = errorType;\n \t\t\t\t\t\t\terror.request = realSrc;\n \t\t\t\t\t\t\tchunk[1](error);\n \t\t\t\t\t\t}\n \t\t\t\t\t\tinstalledChunks[chunkId] = undefined;\n \t\t\t\t\t}\n \t\t\t\t};\n \t\t\t\tvar timeout = setTimeout(function(){\n \t\t\t\t\tonScriptComplete({ type: 'timeout', target: script });\n \t\t\t\t}, 120000);\n \t\t\t\tscript.onerror = script.onload = onScriptComplete;\n \t\t\t\tdocument.head.appendChild(script);\n \t\t\t}\n \t\t}\n \t\treturn Promise.all(promises);\n \t};\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n \t// on error function for async loading\n \t__webpack_require__.oe = function(err) { console.error(err); throw err; };\n\n \tvar jsonpArray = window[\"webpackJsonp\"] = window[\"webpackJsonp\"] || [];\n \tvar oldJsonpFunction = jsonpArray.push.bind(jsonpArray);\n \tjsonpArray.push = webpackJsonpCallback;\n \tjsonpArray = jsonpArray.slice();\n \tfor(var i = 0; i < jsonpArray.length; i++) webpackJsonpCallback(jsonpArray[i]);\n \tvar parentJsonpFunction = oldJsonpFunction;\n\n\n \t// add entry module to deferred list\n \tdeferredModules.push([0,\"chunk-vendors\"]);\n \t// run deferred modules when ready\n \treturn checkDeferredModules();\n","import mod from \"-!../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../node_modules/css-loader/index.js??ref--6-oneOf-1-1!../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=style&index=0&lang=css&\"; export default mod; export * from \"-!../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../node_modules/css-loader/index.js??ref--6-oneOf-1-1!../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=style&index=0&lang=css&\"","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{attrs:{\"id\":\"app\"}},[_c('router-view')],1)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","export * from \"-!cache-loader?{\\\"cacheDirectory\\\":\\\"node_modules/.cache/vue-loader\\\",\\\"cacheIdentifier\\\":\\\"9bd81872-vue-loader-template\\\"}!../node_modules/vue-loader/lib/loaders/templateLoader.js??vue-loader-options!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=template&id=20fc2bd2&\"","const handleClickEvent = (id, eventName, params) => {\n    const clickId = id || '';\n    const options = params || {};\n    BeaconAction.onEvent(clickId, eventName, options);\n    console.log(`id-${clickId}\\neventName:-${eventName}`);\n    // alert(`id-${clickId}\\neventName:-${eventName}`) \n  }\n\nexport {\n    handleClickEvent\n}","import mod from \"-!../node_modules/cache-loader/dist/cjs.js??ref--12-0!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../node_modules/cache-loader/dist/cjs.js??ref--12-0!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./App.vue?vue&type=template&id=20fc2bd2&\"\nimport script from \"./App.vue?vue&type=script&lang=js&\"\nexport * from \"./App.vue?vue&type=script&lang=js&\"\nimport style0 from \"./App.vue?vue&type=style&index=0&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","import Vue from 'vue';\nimport Router from 'vue-router';\n\nVue.use(Router);\n\n\nexport default new Router({\n  routes: [\n    {\n      path: '/',\n      redirect: {\n        name: 'index'\n      }\n    },\n    {\n      path: '/index',\n      name: 'index',\n      component: () => import('./views/index/index.vue')\n    },\n    {\n      path: '/research',\n      component: () => import('./views/index/research.vue'),\n      children: [\n        {\n          path: 'international',\n          name: 'international',\n          component: () => import('./views/research/international.vue')\n        },\n        {\n          path: 'newMedia',\n          name: 'newMedia',\n          component: () => import('./views/research/newMedia.vue')\n        },\n        {\n          path: 'videoCode',\n          name: 'videoCode',\n          component: () => import('./views/research/videoCode.vue')\n        },\n        {\n          path: 'VideoUnderstanding',\n          name: 'VideoUnderstanding',\n          component: () => import('./views/research/VideoUnderstanding.vue')\n        },\n        {\n          path: 'videoProcessing',\n          name: 'videoProcessing',\n          component: () => import('./views/research/videoProcessing.vue')\n        },\n        {\n          path: 'perceptual',\n          name: 'perceptual',\n          component: () => import('./views/research/Perceptual.vue')\n        },\n        {\n          path: 'audioProcessing',\n          name: 'audioProcessing',\n          component: () => import('./views/research/AudioProcessing.vue')\n        },\n        {\n          path: 'audioTransmission',\n          name: 'audioTransmission',\n          component: () => import('./views/research/AudioTransmission.vue')\n        },\n        {\n          path: 'assessment',\n          name: 'assessment',\n          component: () => import('./views/research/Assessment.vue')\n        },\n        {\n          path: '',\n          redirect: 'international'\n        }\n      ]\n    },\n    {\n      path: '/project',\n      name: 'project',\n      component: () => import('./views/index/project.vue')\n    },\n    {\n      path:'/projectTpg',\n      name:'projectTpg',\n      component: ()=> import('./views/project/projectTpg.vue')\n    },\n    {\n      path:'/projectTse',\n      name:'projectTse',\n      component: ()=> import('./views/project/projectTse.vue')\n    },\n    {\n      path:'/projectLiYing',\n      name:'projectLiYing',\n      component: ()=> import('./views/project/projectLiYing.vue')\n    },\n    {\n      path:'/projectAudioAndVideo',\n      name:'projectAudioAndVideo',\n      component: ()=> import('./views/project/projectAudioAndVideo.vue')\n    },\n    {\n      path: '/aboutUs',\n      component: () => import('./views/index/aboutUs.vue'),\n      children:[\n        {\n          path: 'laboratory',\n          name: 'laboratory',\n          component: () => import('./views/aboutUs/laboratory.vue')\n        },\n        {\n          path: 'thesis',\n          name: 'thesis',\n          component: () => import('./views/aboutUs/thesis.vue')\n        },\n        {\n          path: 'joinUs',\n          name: 'joinUs',\n          component: () => import('./views/aboutUs/joinUs.vue')\n        },\n        {\n          path: '',\n          redirect: 'laboratory'\n        }\n      ]\n    },\n    {\n      path: '/solution',\n      name: 'solution',\n      component: () => import('./views/index/solution.vue')\n    }\n  ]})\n","export default {\n    SET_NAVITEM_STATUS: 'SET_NAVITEM_STATUS',\n    SET_NAV_MASK: 'SET_NAV_MASK',\n    SET_BANNER_ISANDROID: 'SET_BANNER_ISANDROID',\n    SET_BANNER_ISIOS: 'SET_BANNER_ISIOS',\n    SET_SCROLLINDECX: 'SET_SCROLLINDECX'\n}","import type from './mutation-type';\n\nconst state = {\n    navItemstatus: {\n        items1: false,\n        items2: false,\n        items3: false,\n        items4: false\n    },\n    isMask: false, // mask\n    isAndroid: false,\n    isiOS: false,\n    scrollIndex: 0 // tabBar\n}\n\nconst mutations = {\n    [type.SET_NAVITEM_STATUS](STATE, data) {\n        STATE.navItemstatus[`items${data.index}`] = data.item;\n    },\n    [type.SET_NAV_MASK](STATAE, data) {\n        STATAE.isMask = data\n    },\n    [type.SET_BANNER_ISANDROID](STATE, data) {\n        STATE.isAndroid = data\n    },\n    [type.SET_BANNER_ISIOS](STATE, data) {\n        STATE.isiOS = data\n    },\n    [type.SET_SCROLLINDECX](STATE, data) {\n        STATE.scrollIndex = data\n    }\n}\nexport default {\n    state,\n    mutations\n}","import mutationState from './mutation';\n\nconst { state, mutations } = { ...mutationState };\n\nexport default {\n    mutations,\n    state\n}","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',[_c('div',{ref:\"out\",staticClass:\"out\",style:({background:_vm.outBg,height:_vm.innerHeight+'px'}),attrs:{\"id\":\"out\"}},[_c('div',{ref:\"inner\",style:({width : _vm.iWidth+'px',height:_vm.innerHeight+'px',background:_vm.innerBg}),attrs:{\"id\":\"inner\"}},[_vm._t(\"default\")],2)])])}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <div id=\"out\" class=\"out\" :style=\"{background:outBg,height:innerHeight+'px'}\" ref=\"out\">\n      <div id=\"inner\" :style=\"{width : iWidth+'px',height:innerHeight+'px',background:innerBg}\" ref=\"inner\">\n        <slot></slot>\n      </div>\n    </div>\n  </div>\n</template>\n<script>\n\nexport default {\n  data(){\n     return{\n     }\n  },\n  props:{\n      // 'outHeight','iWidth','innerHeight','outBg','innerBg'\n      outHeight: {\n        type: String,\n        default: ''\n      },\n      iWidth: {\n        type: String,\n        default: ''\n      },\n      innerHeight: {\n        type: String,\n        default: ''\n      },\n      outBg: {\n        type: String,\n        default: ''\n      },\n      innerBg: {\n        type: String,\n        default: ''\n      }\n  },\n  mounted(){\n    var out = document.getElementById(\"out\")\n    var inner = document.getElementById(\"inner\")\n    window.onresize = function () {\n          inner.style.left=(out.clientWidth-inner.clientWidth)/2+\"px\"\n          inner.style.top=(out.clientHeight-inner.clientHeight)/2+\"px\"\n              }\n             \n          inner.style.left=(out.clientWidth-inner.clientWidth)/2+\"px\"\n          inner.style.top=(out.clientHeight-inner.clientHeight)/2+\"px\"\n          }\n     }\n\n</script>\n<style scoped >\n*{\n  margin: 0px;\n  padding: 0px;\n}\nbody{\n   margin: 0px ! important;\n   padding: 0px;\n}\n #out{\n   position: relative; \n }\n #inner{\n   /* position: absolute; */\n   margin: 0 auto;\n   \n   padding-bottom: 100px;\n }\n \n@media only screen and (max-width:768px){\n   #inner{\n     width: 95% !important; \n   }\n }\n @media only screen and (min-width:768px){\n   #inner{\n     width: 720px !important;\n     \n   }\n }\n @media only screen and (min-width: 992px){\n   #inner{\n     width: 960px !important;\n     \n   }\n }\n  @media only screen and (min-width: 1200px){\n   #inner{\n     width: 1180px !important;\n     \n   }\n }\n</style>","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Layout.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Layout.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Layout.vue?vue&type=template&id=5cbf1b3b&scoped=true&\"\nimport script from \"./Layout.vue?vue&type=script&lang=js&\"\nexport * from \"./Layout.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Layout.vue?vue&type=style&index=0&id=5cbf1b3b&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"5cbf1b3b\",\n  null\n  \n)\n\nexport default component.exports","export default {\n  title: \"\",\n  more: \"\",\n  //  \n  research:{\n        title: '',\n        researchAndMade:{\n          title:\"\",\n          text:\"\"\n         },\n        touch:{  \n          title:\"\",\n          text:\"CPU\"\n        },\n        newMedia:{  \n         title:\"\",\n         text:\"\"\n        },\n        videoUnderstanding:{  \n          title:\"\",\n          text:\"\"\n        },\n         VideoCodingAndDecoding:{  \n           title:\"\",\n           text:\"\"\n         },\n         videoHandle:{  \n           title:\"\",\n           text:\"\"\n         },\n         audioHandle:{  \n           title:\"\",\n           text:\"3A\"\n         },\n         connectionQuality:{  \n           title:\"\",\n           text:\"\"\n         },\n         transfer:{  \n           title:\"\",\n           text:\"\"\n         },\n  },\n   //  \n  project:{\n    title1:\"TPG\",\n    title2:\"TSE\",\n    title3:\"\",\n    title4:\"\",\n    subTitle1:\"TPG (Tiny Portable Graphic)\",\n    text1:\"AVS2\",\n    subTitle2:\"TSE (Tencent Screen Encoder)\",\n    text2:\"\",\n    subTitle3:\"\",\n    text3:\"\",\n    subTitle4:\"\",\n    text4:\"\",\n    lookAll:\"\",\n    more: \"\",\n  },\n   // \n  solution:{\n    solutionTitle: \"\",\n    title1:\"\",\n    text1:\"QQ\",\n    title2:\"\",\n    text2:\" SDK \",\n    title3:\"\",\n    text3:\" SDK \",\n    more:\"\",\n    lookAll:\"\",\n    join:\"\",\n    join1:\"\",\n    joining:\"\"\n  },\n  // mask\n  navList:[\n    {\n      name: ''\n    },\n    {\n      name: '',\n      showList: false,\n      childen: [\n        { eventName: 'click_research_international', id: '86717', name: '', link:'/research/international', index: 0},\n        { eventName: 'click_research_audio_and_video', id: '82091',name: '', link:'/research/audioTransmission', index: 1 },\n        { eventName: 'click_research_immersive', id: '86718',name: '', link:'/research/newMedia', index: 2 },\n        { eventName: 'click_research_codec', id: '86719',name: '', link:'/research/videoCode', index: 3 },\n        { eventName: 'click_research_audio', id: '86716',name: '', link:'/research/audioProcessing', index: 4 },\n        { eventName: 'click_research_process', id: '82088',name: '', link:'/research/videoProcessing', index: 5 },\n        { eventName: 'click_research_perceived', id: '86720',name: '', link:'/research/perceptual', index: 6 },\n        { eventName: 'click_research_assess', id: '82092',name: '', link:'/research/assessment', index: 7 },        \n        { eventName: 'click_research_understanding', id: '82087',name: '', link:'/research/VideoUnderstanding', index: 8 },\n      ]\n    },\n    {\n      name: '',\n      showList: false,\n      childen: [\n        { eventName: 'click_top_project', id: '82082',name: '', link:'/project' },\n        { eventName: 'click_project_TPG', id: '82096',name: 'TPG', link:'/projectTpg' },\n        { eventName: 'click_project_TSE', id: '82097',name: 'TSE', link:'/projectTse' },\n        { eventName: 'click_project_liying', id: '82098',name: '', link:'/projectLiYing' },\n        { eventName: 'click_project_platform', id: '82099',name: '', link:'/projectAudioAndVideo' }\n      ]\n    },\n    {\n      name: '',\n      showList: false,\n      childen: [\n        { eventName: 'click_top_solution', id: '82083',name: '', bgc: false, link:'/solution' },\n        { eventName: 'click_solution_TRTC', id: '82093',name: '', bgc: true, link:'https://cloud.tencent.com/product/trtc '},\n        { eventName: 'click_solution_ILVB', id: '82094',name: '', bgc: true, link:'https://cloud.tencent.com/solution/ilvb' },\n        { eventName: 'click_solution_GME', id: '82095',name: '', bgc: true, link:' https://cloud.tencent.com/product/gme' }\n      ]\n    },\n    {\n      name: '',\n      showList: false,\n      childen: [\n        { name: '', link:'/aboutUs/laboratory' },\n        // { name: '', link:'/aboutUs/thesis' },\n        { name: '', link:'/aboutUs/joinUs' }\n      ]\n    }\n  ]\n}"," export default {\n    //  \n     title:\"\",\n\n    //  home\n\n\n\n\n    // 1. \n    researchAndMade:{\n        title:\"\",\n        title1:\"\",\n        text:\"\",\n        explain:\":\",\n\n        //   tabs    --tab\n        \n        tab1:{\n            title:\"\",\n            list:[\n                {\n                    subTitle:\"VVC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"Versatile Video Coding (VVC)ISO MPEGITU-T VCEGJVET\",\n                            \"VVCHEVC40%-50%Media LabVVCVVC\",\n                            \"JVETVVCVVC\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG-5 EVC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MPEG-5 Essential Video Coding (EVC)MPEG\",\n                            \"MPEG-5 EVCHEVC\",\n                            \"Media LabMPEG-5 EVC\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"AVS-3\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"AVS2002\",\n                            \"AVSAVS-1H.264/AVCAVS-2HEVCH.265\",\n                            \"AVS-3AVS-230%AVS-3AVS-240%\",\n                            \"Media LabAVS-3AVS-3\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG-PCC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MPEG+\",\n                            \"V-PCC (ISO/IEC 23090-9)AVCHEVCVVC\",\n                            \"G-PCC (ISO/IEC 23090-5AR/VR\",\n                            \"Media Lab\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"AVS-PCC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"\",\n                            \"\",\n                            \"AVS2019620198AVS2020\",\n                            \"Media LabAVS-PCC\"\n                        ]}\n                    ]\n                }\n            ]\n        },\n\n        // -- tab \n        tab2:{\n            title:\"\",\n            list:[\n                {\n                    subTitle:\"OCP\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"OCP)API\",\n                            \"Media Lab\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"IETF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"IETF\",\n                            \"IETFIPTCPUDPHTTP\",\n                            \"Media LabIETFAVTQUICMOPSWengerIETF\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"3GPP SA\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"3GPP5G\",\n                            \"Media Lab3GPPSA plenarySA4)Media LabSA1SA2SA6\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG SYSTEMS\",\n                    subContanier: [\n                        {sencTitle:\"OMAF\",sencText:[\n                            \"MPEGOMAFVR360OMAF\",\n                            \"OMAFHASHDynamic Adaptive Streaming over HTTPMTTMPEG Media Transport\",\n                            \"MPEG20191OMAFOMAFOMAFTile360\",\n                            \"Media LabOMAFChoiOMAF\"\n                        ]},\n                        {sencTitle:\"MPEG DASH\",sencText:[\n                            \"MPEG DASH Dynamic Adaptive Streaming over HTTPHASH\",\n                            \"MPEG DASHHTTP1.1, HTTP2\",\n                            \"DASHATSC3.0DVBHbbTV, 3GPP, VR-IF and CTA WAVEMPEG DASHMPEGPart 1\",\n                            \"Media LabDASHSodagarMPEG DASH\"\n                        ]},\n                        {sencTitle:\"CMAF\",sencText:[\n                            \"MPEG CMAFDASHHLS\",\n                            \"CMAFCMAFCDN\",\n                            \"MPEGCMAF4.Media LabCMAFSodagarCMAF\"\n                        ]},\n                        {sencTitle:\"MPEG NBMP\",sencText:[\n                            \"MPEG NBMP Network Based Media Processing\",\n                            \"NBMP\",\n                            \"MPEGNBMP2020\",\n                            \"Media LabNBMPSodagarNBMP\"\n                        ]}\n                    ]\n                }\n            ]\n        },\n        //  -- tab\n        tab3:{\n            title:\"\",\n            list:[\n                {\n                    subTitle:\"MC-IF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MC-IFMPEG\",\n                            \"VVC\",\n                            \"Media LabWenger\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"DASH-IF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"DASHMPEG DASH\",\n                            \"802012DASHDASHdash.js\",\n                            \"DASHMPEG DASH3GPPDVB, W3C, ATSC, CTA WAVE and HbbTVDASH\",\n                            \"Media LabSodagar\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"8K Association\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"8K5G8K8K8K8K5G\",\n                            \"Media LabWengerChoiTCL8K\"\n                        ]}\n                    ]\n                },\n            ]\n        },\n       },\n        \n\n    //  2.\n    newMedia:{\n         title:\"\",\n         text:\"OMAFDASH\",\n         explain:\"\",\n         list:[\n             {\n                subTitle:\"\",\n                subContanier:\"XR\"\n             },\n             {\n                subTitle:\"\",\n                subContanier:\"50%\"\n             },\n             {\n                subTitle:\"\",\n                subContanier:\"VR\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"HLSDASHABRROIPC\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"3D\"\n            },\n         ],\n\n         business:\"\",\n         businessList:[\n             {\n                subTitle:\"-\",\n                subContanier:\"5G\"\n             },\n             {\n                subTitle:\"-VR\",\n                subContanier:\"TOBVR\"\n            }\n         ]\n\n    },\n       \n\n    //   3.\n    VideoCodingAndDecoding:{\n         title:\"\",\n         text:\",\",\n         explain :\"\",\n         list:[\n             {\n                subTitle:\"\",\n                subContanier:\"H.264H.265AVS2\"\n             },\n             {\n                subTitle:\"\",\n                subContanier:\"PC\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            // {\n            //     subTitle:\"\",\n            //     subContanier:\"\"\n            // },\n         ],\n\n         project:\"\",\n         projectName1:\"TPG\",\n         projectName2:\"TSE\"\n    },\n\n    \n\n    //  4.\n    videoUnderstanding:{\n          title:\"\",\n          text:\"\",\n          explain :\"\",\n          list:[\n              {\n                subTitle:\"\",\n                subContanier:\"\"\n              },\n              {\n                subTitle:\"\",\n                subContanier:\"MV\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },{\n                subTitle:\"\",\n                subContanier:\"\"\n            }\n          ]\n    },\n\n    \n\n    //   5.\n    videoHandle:{\n        title:\"\",\n        text:\"\",\n\n\n        explain1:\"\",\n        list1:[\n            {\n                subTitle:\"\",\n                subContanier:\"\",\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"artifact\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            }\n        ],\n        explain2:\"\",\n        list2:[\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            }\n        ]\n\n    },\n\n\n    //  6.\n    touch:{\n        title:\"\",\n        text:\"CPU\",\n        explain :\"\",\n        list:[\n            {\n                subTitle:\"\",\n                subContanier:\" \"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\" CPUGPU, FPGA, ASIC\"\n            }\n        ],\n        business:\"\",\n         businessList:[\n             {\n                subTitle:\"\",\n                subContanier:\".\"\n             }\n         ]\n     },\n\n    //   7.\n    audioHandle:{\n          title:\"\",\n          text:\"3A\",\n          explain:\"\",\n          list:[\n              {\n                subTitle:\"\",\n                subContanier:\"VoIP\"\n              },\n              {\n                subTitle:\"\",\n                subContanier:\"To BTo C\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"-\"\n            },\n            {\n                subTitle:\"3A\",\n                subContanier:\"(1)(AEC),(2)(ANS),(3)(AGC)\"\n            },\n            {\n                subTitle:\"AI/\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"AI\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"AI\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"3D\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n          ]\n    },\n\n   \n\n    // 8.\n    transfer:{\n          title:\"\",\n          text:\"\",\n          explain:\"\",\n          list:[\n              {\n                subTitle:\"\",\n                subContanier:\"\"\n              },\n              {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"QoS/QoE\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n          ]\n    },\n\n\n    // 9.\n    connectionQuality:{\n        title:\"\",\n        text:\"/\",\n        explain:\"\",\n        list:[\n            {\n                subTitle:\"\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"/\",\n                subContanier:\"\"\n            },\n            {\n                subTitle:\"\",\n                subContanier:\"codec\"\n            },\n        ],\n        project:\"\",\n        projectName:\"\",\n        projectText:\"\",\n        more: \"\"\n    },\n\n\n}","export default {\n    // \n    title:\"\",\n    textTPG:\" CDN  TPG /\",\n    textTSE:\"\",\n    textLiYing:\"\",\n    pingtai:\"\",\n    more:\"\",\n\n    nav:{\n        back:\" \",\n        next:\" : \"\n    },\n\n   \n    // TPG\n    tpg:{\n        textTPG:\"TPGAVS2JPG47%PNG60%GIF85%WebP25%TPGWindowsLinuxMaciOSAndroidTPGAVS-P7\",\n        partner:\"\",\n        itemTitle1:\"TPG\",\n        itemText1:\"TPGJPG 47% PNG60%Gif85%WebP25%\",\n        itemTitle2:\"TPG\",\n        itemText2:\"PSNRTPG webP 23.5% libjpeg 46.8%\",\n        itemTitle3:\"TPG\",\n        itemText3Item1:\"1.iOS TPG webP2libjpeg2.3\",\n        itemText3Item2:\"2.Android TPG  webP 1.2 libjpeg 1.6\",\n        itemTitle4:\"TPG\",\n        itemText4Item1:\"1.iOS TPG  webP  2 libjpeg 2.5 \",\n        itemText4Item2:\"2.Android TPG  webP  2  libjpeg  4 \",\n        itemTitle5:\"TPG\",\n        itemText5:{\n            text1:{\n                one:\"\",\n                two:\"1.\",\n                // three:\"2.\",\n                four:\"2.\",\n                // five:\"4....\"\n            },\n            text2:{\n                one:\"\",\n                two:\"1.H5\",\n                three:\"2.\",\n                four:\"3.\",\n            },\n            text3:{\n                one:\"\",\n                two:\"1.\",\n                three:\"2.UI\",\n            }\n        },\n        partners:\"TPG\" \n    },\n\n    // TES\n    tse:{\n       tesText:\"TSETSEIntra Block CopyPalette Modex265-medium55%x265-medium10\",\n       itemTitle1:\"TSEX265\",\n       itemText1one:\"TSEx265-ultrafast20%\",\n       itemText1two:\"TSEx265-ultrafast70%x265-medium55%\",\n       itemTitle2:\"TSEX265\",\n       itemText2one:\"TSEx265-ultrafast88%\",\n       itemText2two:\"IBCPLTTSEx265-ultrafast33% IBCPLTTSEx265-ultrafast50%\", \n       partner:\"TSE\",\n    },\n\n    // \n    liYing:{\n       title:\"\",\n       textLiYing:\"\",\n       vidio:\"\",\n       itemTitle1:\" - \",\n       itemTitle2:\" -  \",\n       itemTitle3:\" - \",\n       itemTitle4:\" - \",\n       partner:\"\", \n       before:\"\",\n       after:\"\"\n    },\n\n\n    // \n    vidioAndaudioTest:{\n        title:\"\",\n        // \n        pingtaiText:\"TMEC\",\n        itemText1:\"TMEC\",\n        itemText2:\"TMEC\",\n    }\n\n}\n","export default {\n    // \n    title:\"\",\n    list:[\n        {\n            title:\"(TRTC)\",\n            text:\"QQ\",\n        },\n        {\n            title:\"(ILVB)\",\n            text:\" SDK \",\n        },\n        {\n            title:\"(GME)\",\n            text:\" SDK \",\n        },\n    ]\n}","export default {\n    bannerTitle:\"\",\n    title:\"\",\n    text:\"\",\n    members:\"\",\n    sideNavBarTitle:[\"\",\"\",\"\"],\n        \n    //-\n    team: \"\",\n    expertList:[\n        {\n          id: 1,\n          imgClass: \"icon-about_image_expert_0\",\n          name:\" \",\n          text:\"5002017QQIoT60200H.265/HEVC v4VVC2014-2015IEEE Signal Processing Society2016-2017Asia-Pacific Signal and Information Processing Association (APSIPA) 2019APSIPA\" \n        },\n        {\n          id: 2,\n          imgClass: \"icon-about_image_expert_1\",\n          name:\" \",\n          text:\" -JEMVVCMPEG-5 EVC/IEEE40300120\" \n        },\n        {\n          id: 3,\n          imgClass: \"icon-about_image_expert_2\",\n          name: \" \",\n          text: \"200620122017VVC201220172009HEVC3D-AVC3D-HEVCJVETVVC12JVETBoGAhG2602030\"\n        },\n        {\n          id: 4,\n          imgClass: \"icon-about_image_expert_3\",\n          name:\"Stephan Wenger \",\n          text:\"Stephan Wenger2018VidyoUB VideoeBrisk Video20062016IETFITU-TMPEGWenger4019891995\" \n        },\n        {\n          id: 5,\n          imgClass: \"icon-about_image_expert_4\",\n          name:\"Iraj Sodagar \",\n          text:\"Iraj Sodagar 201811Windows25Sodagarin ITU-T, JPEG, MPEG, AMQP, W3C, DVB, 3GPP, W3C, CTA WAVE, DLNA AOMedia21MPEG  DASHDASHMPEG CMAFCMAFSodagar1994\" \n        },\n        {\n          id: 6,\n          imgClass: \"icon-about_image_expert_5\",\n          name:\" \",\n          text:\"20172004H.264/AVC, H.265/HEVC, H.266/VVC, MPEG-5, ITU-T IPTV, AVS10020HEVC SCC, VVC AVSMPEG-5\" \n        },\n        {\n          id: 7,\n          imgClass: \"icon-about_image_expert_6\",\n          name:\" \",\n          text:\"IntelApple2201812VVC/H.266 \" \n      }\n    ],\n\n\n    //-\n    contactTitle: \"\",\n    contactEmail: \"medialab@tencent.com\",\n    positionList: [\n      {\n        id: 1,\n        positionName: \"\",\n        address: \"\",\n        duty: {\n          title: \":\",\n          content: [\n            \"- H264H265\",\n            \"- H266/AV1\"\n          ]\n        },\n        jobRequirements: {\n          title: \":\",\n          content: [\n            \"- H264/HEVC\",\n            \"- GPU\",\n            \"- ffmpegx264x265\",\n            \"- \"\n          ]\n        }\n      },\n      {\n        id: 2,\n        positionName: \"\",\n        address: \"\",\n        duty: {\n          title: \":\",\n          content: [\n            \"- , , , , \",\n            \"- , \",\n            \"- x86ARM\",\n            \"- \"\n          ]\n        },\n        jobRequirements: {\n          title: \":\",\n          content: [\n            \"- \",\n            \"- \",\n            \"- CNN\",\n            \"- LeaderboardImageNetKaggle//\",\n            \"-  C++Python \",\n            \"- iOSAndroid\",\n            \"- \"\n          ]\n        }\n      },\n      {\n        id: 3,\n        positionName: \"\",\n        address: \"\",\n        duty: {\n          title: \":\",\n          content: [\n            \"- \",\n          ]\n        },\n        jobRequirements: {\n          title: \":\",\n          content: [\n            \"- C/C++TCP/IPAPP\",\n            \"- FFmpegVLCWebRTCX264JITTER BUFFERFEC\",\n            \"- CDN;\",\n            \"- \"\n          ]\n        }\n      },\n      {\n        id: 4,\n        positionName: \"\",\n        address: \"\",\n        duty: {\n          title: \":\",\n          content: [\n            \"- &\",\n          ]\n        },\n        jobRequirements: {\n          title: \":\",\n          content: [\n            \"- \",\n            \"- \",\n            \"- C/C++Python\",\n            \"- \",\n            \"- \",\n            \"- LeaderboardImageNetKaggle//\",\n            \"- \"\n          ]\n        }\n      },\n      {\n        id: 5,\n        positionName: \"\",\n        address: \"\",\n        duty: {\n          title: \":\",\n          content: [\n            \"- QQFEC\",\n          ]\n        },\n        jobRequirements: {\n          title: \":\",\n          content: [\n            \"- 2\",\n            \"- \",\n            \"- \",\n            \"- \",\n            \"- Qos\",\n            \"- c/c++\",\n            \"- iOSAndroid\",\n            \"- \"\n          ]\n        }\n      }\n    ],\n\n\n    //-\n    navList: [\"\",\"\",\"\",\"\",\"\",\"\",\"3D\"],\n    Total: ' ',\n    item: ' ',\n    thesisList:[\n      [\n        [22,\"all\"],\n        [\n          [\n            {\n              id:'all-3-5',\n              title: \"Multiple Reference Line Coding for Most Probable Modes in Intra Prediction\",\n              author: \"Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand\",\n              time: \"2019\",\n              content: \"This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.\",\n              src: \"https://ieeexplore.ieee.org/document/8712640\"\n            },\n            {\n              id:'all-3-4',\n              title: \"Wide Angular Intra Prediction for Versatile Video Coding\",\n              author: \"Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape\",\n              time: \"2019\",\n              content: \"This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).\",\n              src: \"https://ieeexplore.ieee.org/document/8712681\"\n            },\n            {\n              id:'all-3-1',\n              title: \"Blind image quality assessment based on joint log-contrast statistics\",\n              author: \"Yabin Zhang et at.\",\n              time: \"2019\",\n              content: \"During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.\",\n              src: \"https://www.sciencedirect.com/science/article/pii/S0925231218313432\"\n            },\n            {\n              id:'all-4-1',\n              title: \"Fast Adaptive Multiple Transform for Versatile Video Coding\",\n              author: \"Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu\",\n              time: \"2019\",\n              content: \"The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.\",\n              src: \"https://ieeexplore.ieee.org/document/8712650\"\n            },\n            {\n              id:'all-4-2',\n              title: \"Recent advances in video coding beyond the HEVC standard\",\n              author: \"Xiaozhong Xu, Shan Liu\",\n              time: \"2019\",\n              content: \"The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.\",\n              src: \"https://doi.org/10.1017/ATSIP.2019.11\"\n            },\n            \n          ],\n          [\n            {\n              id:'all-4-3',\n              title: \"Current Picture Referencing in Versatile Video Coding\",\n              author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n              time: \"2019\",\n              content: \"Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.\",\n              src: \"https://ieeexplore.ieee.org/document/8695359\"\n            },\n            {\n              id:'all-5-1',\n              title: \"High Performance Gesture Recognition via Effective and Efficient Temporal Modeling\",\n              author: \"Yang Yi, Feng Li, et al.\",\n              time: \"2019\",\n              content: \"State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization\",\n              src: \"https://www.ijcai.org/proceedings/2019/0141.pdf\"\n            },\n            {\n              id:'all-5-2',\n              title: \"A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.\",\n              author: \"Yabin Zhang, et al\",\n              time: \"2019\",\n              content: \"3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.\",\n              src: \"https://ieeexplore.ieee.org/document/8704253\"\n            },\n            {\n              id:'all-1-1',\n              title: \"A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network\",\n              author: \"sarahqwang\",\n              time: \"2018\",\n              content: \"The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8706582\"\n            },\n            {\n              id:'all-2-1',\n              title: \"Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy\",\n              author: \"zeqilai\",\n              time: \"2018\",\n              content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in todays network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.\",\n              src: \"https://dl.acm.org/citation.cfm?id=3318101\"\n            }\n          ],\n          [\n            {\n              id:'all-3-2',\n              title: \"Intra Block Copy for Next Generation Video Coding\",\n              author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n              time: \"2018\",\n              content: \"Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.\",\n              src: \"https://ieeexplore.ieee.org/document/8551528\"\n            },\n            {\n              id:'all-3-3',\n              title: \"Coupled Primary and Secondary Transform for Next Generation Video Coding\",\n              author: \"Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu\",\n              time: \"2018\",\n              content: \"The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.\",\n              src: \"https://ieeexplore.ieee.org/document/8698635\"\n            },\n            {\n              id:'all-1-5',\n              title: \"Gaussian density guided deep neural network for single-channel speech enhancement\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8168116\"\n            },\n            {\n              id:'all-1-3',\n              title: \"A maximum likelihood approach to deep neural network based speech dereverberation\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8282019\"\n            },\n            {\n              id:'all-1-2',\n              title: \"A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.\",\n              src: \"https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF\"\n            }\n          ],\n          [\n            {\n              id:'all-1-4',\n              title: \"A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemalefemale, malemale, and femalemale mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/7917323\"\n            },\n            {\n              id:'all-2-2',\n              title: \"Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.\",\n              src: \"https://ieeexplore.ieee.org/document/8117568\"\n            },\n            {\n              id:'all-2-3',\n              title: \"Furion: Engineering high-quality immersive virtual reality on today's mobile devices\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).\",\n              src: \"https://dl.acm.org/citation.cfm?id=3117815\"\n            },\n            {\n              id:'all-4-4',\n              title: \"TPG Image Compression Technology\",\n              author: \"ShitaoWangPiaoDingXiaozhengHuangHanjunLiuBinjiLuoXinxingChenYoubaoWuRonggangWang\",\n              time: \"\",\n              content: \"TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression\",\n              src: \"http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238\"\n            },\n            {\n              id:'all-4-5',\n              title: \"Saliency detection with two-level fully convolutional networks\",\n              author: \"Yang Yi, et al.\",\n              time: \"2017\",\n              content: \"This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.\",\n              src: \"https://ieeexplore.ieee.org/document/8019309/\"\n            },\n          ],\n          [\n            {\n              id:'all-2-4',\n              title: \"Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.\",\n              src: \"https://ieeexplore.ieee.org/document/7898362\"\n            },\n            {\n              id:'all-2-5',\n              title: \"Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of users input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.\",\n              src: \"https://dl.acm.org/citation.cfm?id=3234201\"\n            }\n          ]\n        ]\n      ],\n      [\n        [5,\"audio\"],\n        [\n          [\n            {\n              id:'audio-1-1',\n              title: \"A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network\",\n              author: \"sarahqwang\",\n              time: \"2018\",\n              content: \"The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8706582\"\n            },\n            {\n              id:'audio-1-2',\n              title: \"A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.\",\n              src: \"https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF\"\n            },\n            {\n              id:'audio-1-3',\n              title: \"A maximum likelihood approach to deep neural network based speech dereverberation\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8282019\"\n            },\n            {\n              id:'audio-1-4',\n              title: \"A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemalefemale, malemale, and femalemale mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/7917323\"\n            },\n            {\n              id:'audio-1-5',\n              title: \"Gaussian density guided deep neural network for single-channel speech enhancement\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8168116\"\n            }\n          ]\n        ]\n      ],\n      [\n        [5,\"network\"],\n        [\n          [\n            {\n              id:'network-1-1',\n              title: \"Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy\",\n              author: \"zeqilai\",\n              time: \"2018\",\n              content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in todays network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.\",\n              src: \"https://dl.acm.org/citation.cfm?id=3318101\"\n            },\n            {\n              id:'network-1-2',\n              title: \"Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.\",\n              src: \"https://ieeexplore.ieee.org/document/8117568\"\n            },\n            {\n              id:'network-1-3',\n              title: \"Furion: Engineering high-quality immersive virtual reality on today's mobile devices\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).\",\n              src: \"https://dl.acm.org/citation.cfm?id=3117815\"\n            },\n            {\n              id:'network-1-4',\n              title: \"Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.\",\n              src: \"https://ieeexplore.ieee.org/document/7898362\"\n            },\n            {\n              id:'network-1-5',\n              title: \"Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of users input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.\",\n              src: \"https://dl.acm.org/citation.cfm?id=3234201\"\n            },\n          ]\n        ]\n      ],\n      [\n        [1,\"quality\"],\n        [\n          [\n            {\n              id:'quality-1-1',\n              title: \"Blind image quality assessment based on joint log-contrast statistics\",\n              author: \"Yabin Zhang et at.\",\n              time: \"2019\",\n              content: \"During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.\",\n              src: \"https://www.sciencedirect.com/science/article/pii/S0925231218313432\"\n            }\n          ]\n        ]\n      ],\n      [\n        [8,\"video\"],\n        [\n          [\n            {\n              id:'video-1-3',\n              title: \"Wide Angular Intra Prediction for Versatile Video Coding\",\n              author: \"Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape\",\n              time: \"2019\",\n              content: \"This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).\",\n              src: \"https://ieeexplore.ieee.org/document/8712681\"\n            },\n            {\n              id:'video-1-4',\n              title: \"Multiple Reference Line Coding for Most Probable Modes in Intra Prediction\",\n              author: \"Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand\",\n              time: \"2019\",\n              content: \"This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.\",\n              src: \"https://ieeexplore.ieee.org/document/8712640\"\n            },\n            {\n              id:'video-1-5',\n              title: \"Fast Adaptive Multiple Transform for Versatile Video Coding\",\n              author: \"Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu\",\n              time: \"2019\",\n              content: \"The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.\",\n              src: \"https://ieeexplore.ieee.org/document/8712650\"\n            },\n            {\n              id:'video-2-1',\n              title: \"Recent advances in video coding beyond the HEVC standard\",\n              author: \"Xiaozhong Xu, Shan Liu\",\n              time: \"2019\",\n              content: \"The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.\",\n              src: \"https://doi.org/10.1017/ATSIP.2019.11\"\n            },\n            {\n              id:'video-2-2',\n              title: \"Current Picture Referencing in Versatile Video Coding\",\n              author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n              time: \"2019\",\n              content: \"Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.\",\n              src: \"https://ieeexplore.ieee.org/document/8695359\"\n            }\n          ],\n          [\n            {\n              id:'video-1-2',\n              title: \"Coupled Primary and Secondary Transform for Next Generation Video Coding\",\n              author: \"Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu\",\n              time: \"2018\",\n              content: \"The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.\",\n              src: \"https://ieeexplore.ieee.org/document/8698635\"\n            },\n            {\n              id:'video-1-1',\n              title: \"Intra Block Copy for Next Generation Video Coding\",\n              author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n              time: \"2018\",\n              content: \"Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.\",\n              src: \"https://ieeexplore.ieee.org/document/8551528\"\n            },\n            {\n              id:'video-2-3',\n              title: \"TPG Image Compression Technology\",\n              author: \"ShitaoWangPiaoDingXiaozhengHuangHanjunLiuBinjiLuoXinxingChenYoubaoWuRonggangWang\",\n              time: \"2017\",\n              content: \"TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression\",\n              src: \"http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238\"\n            }\n          ]\n        ]\n      ],\n      [\n        [2,\"computer\"],\n        [\n          [\n            {\n              id:'computer-1-2',\n              title: \"High Performance Gesture Recognition via Effective and Efficient Temporal Modeling\",\n              author: \"Yang Yi, Feng Li, et al.\",\n              time: \"2019\",\n              content: \"State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization\",\n              src: \"https://www.ijcai.org/proceedings/2019/0141.pdf\"\n            },\n            {\n              id:'computer-1-1',\n              title: \"Saliency detection with two-level fully convolutional networks\",\n              author: \"Yang Yi, et al.\",\n              time: \"2017\",\n              content: \"This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.\",\n              src: \"https://ieeexplore.ieee.org/document/8019309/\"\n            }\n          ]\n        ]\n      ],\n      [\n        [1,\"dianYun\"],\n        [\n          [\n            {\n              id:'dianYun-1-1',\n              title: \"A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.\",\n              author: \"Yabin Zhang, et al\",\n              time: \"2019\",\n              content: \"3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.\",\n              src: \"https://ieeexplore.ieee.org/document/8704253\"\n            } \n          ]\n        ]\n      ]\n    ]\n  }\n\n\n","import homeLanguge from \"./pages/zh/homePageLanguge\"\nimport researchLanguge from \"./pages/zh/researchPageLanguge\"\nimport projectLanguge from \"./pages/zh/projectPageLanguge\"\nimport solutionLanguge from \"./pages/zh/solutionPageLanguge\"\nimport aboutLanguge from \"./pages/zh/aboutPageLanguge\"\n\n\n//  \n\n\n\n\n\nexport default {\n  // \n    nav: {\n      home: \"\",\n      research:\"\",\n      project:\"\",\n      solution:\"\",\n      about:\"\",\n      title:\"\"\n    },\n\n    // \n    homeLanguge,\n    researchLanguge,\n    projectLanguge,\n    solutionLanguge,\n    aboutLanguge\n\n\n\n\n\n\n    // \n    // home:{\n      //  \n    //   \tresearch:{\n    //        researchAndMade:{\n    //          title:\"\",\n    //          text:\"\"\n    //         },\n    //        touch:{  \n    //          title:\"\",\n    //          text:\"\"\n    //        },\n    //        newMedia:{  \n    //         title:\"\",\n    //         text:\"\"\n    //       },\n    //        videoUnderstanding:{  \n    //         title:\"\",\n    //         text:\"\"\n    //       },\n    //       VideoCodingAndDecoding:{  \n    //         title:\"\",\n    //         text:\"\"\n    //       },\n    //       videoHandle:{  \n    //         title:\"\",\n    //         text:\"\"\n    //       },\n    //       audioHandle:{  \n    //         title:\"\",\n    //         text:\"3A\"\n    //       },\n    //       connectionQuality:{  \n    //         title:\"\",\n    //         text:\"\"\n    //       },\n    //       transfer:{  \n    //         title:\"\",\n    //         text:\"\"\n    //       },\n    // },\n\n        //  \n        // project:{\n        //    title1:\"TPG\",\n        //    title2:\"TSE\",\n        //    title3:\"\",\n        //    title4:\"\",\n        //    text1:\"TPGAVS2 \",\n        //    text2:\"JPG47%PNG60%Gif85%\",\n        //    text3:\"Webp25%\",\n        //    more:\"\"\n        // },\n        // \n  //       solution:{\n  //          title1:\"Tencent Real-Time CommunicationTRTC)\",\n  //          text1:\"QQ\",\n  //          title2:\"\",\n  //          text2:\" SDK \",\n  //          title3:\"Gaming Multimedia EngineGME)\",\n  //          text3:\" SDK \",\n  //          more:\"\",\n  //          total:\"\",\n  //          join:\"\",\n  //          joining:\"\"\n  //       }\n  // }\n}\n  ","export default {\n    title: \"Tencent Media Lab, may the world be around you.\",\n    more: \"Learn More\",\n    //  \n    research:{\n          title: 'Research',\n          researchAndMade:{\n            title:\"International Standardization\",\n            text:\"International standardization research covering multimedia compression, system, and communication\"\n           },\n          touch:{  \n            title:\"Heterogeneous Platform\",\n            text:\"Provide higher efficiency and low latency compute power than traditional parallel CPU architecture. \"\n          },\n          newMedia:{  \n           title:\"Immersive Media\",\n           text:\"Immersive experience through complete pipeline of media capture, compression, communication, and playback\"\n          },\n          videoUnderstanding:{  \n            title:\"Intelligent Media\",\n            text:\"High level analysis and understanding of video semantics through AI and deep learning\"\n          },\n           VideoCodingAndDecoding:{  \n             title:\"Video Coding\",\n             text:\"Higher compression ratio and better channel adaptation\"\n           },\n           videoHandle:{  \n             title:\"Video Processing\",\n             text:\"Video spatial and temporal processing that works seamlessly with video understanding and compression\"\n           },\n           audioHandle:{  \n             title:\"Audio Processing\",\n             text:\"Multi-Party 3A solution for real-time communications among various scenes\"\n           },\n           connectionQuality:{  \n             title:\"Multimedia QA\",\n             text:\"Quality assessment platform for audio and video contents\"\n           },\n           transfer:{  \n             title:\"Real-Time Multimedia\",\n             text:\"Network robustness against volatile bandwidth and package loss, through network condition detection and feedback\"\n           }\n    },\n     //  \n    project:{\n      title1:\"TPG\",\n      title2:\"TSE\",\n      title3:\"KANASKY\",\n      title4:\"TMEC \",\n      subTitle1:\"TPG (Tiny Portable Graphic)\",\n      text1:\"A new image compression standard based on AVS2\",\n      subTitle2:\"TSE (Tencent Screen Encoder)\",\n      text2:\"Optimized video encoder for screen content\",\n      subTitle3:\"KANASKY\",\n      text3:\"Human perceptual video service based on AI and big data, which produces prettier yet smaller video.\",\n      subTitle4:\"TMECTencent Multimedia Evaluation Cloud\",\n      text4:\"Provide ability for efficient automatical AV(audio/video) quality assesment, conducting full automatical presssure assesment at audio and video quality in multi aspect.\",\n      lookAll:\"See All\",\n      more: \"Find out more\",\n    },\n     // \n    solution:{\n      solutionTitle: \"Solutions\",\n      title1:\"TRTCTencent Real-Time Communication\",\n      text1:\"With 10+ years serving QQ Real-Time video communications, TRTC dedicates to provide high quality and low cost solutions for multimedia business.\",\n      title2:\"Solution for Interactive Webcasting\",\n      text2:\"A brand new all-in-one solution for multi-way audio video interaction. Audio interaction with the host, split view display, cross-platform one-to-many, multi-way high definition webcasting via mobile webcasting SDK\",\n      title3:\"GMEGaming Multimedia Engine\",\n      text3:\"All-In-One voice communication solution for gaming. Highly optimized for various gaming scenes, full functionalities, easy integration. One single SDK fits all.\",\n      more:\"Find out more\",\n      lookAll:\"See All\",\n      join:\"Tencent Media Lab Job Opportunities\",\n      join1:\"Tencent Media Lab Job Opportunities\",\n      joining:\"Join Us Now!\"\n    },\n    // mask\n  navList:[\n    {\n      name: 'Home'\n    },\n    {\n      name: 'Research',\n      showList: false,\n      childen: [\n        { eventName: 'click_research_international', id: '86717',name: 'International Standardization', link:'/research/international', index: 0},\n        { eventName: 'click_research_audio_and_video', id: '82091',name: 'Real-Time Multimedia', link:'/research/audioTransmission', index: 1 },\n        { eventName: 'click_research_immersive', id: '86718',name: 'Immersive Media', link:'/research/newMedia', index: 2 },\n        { eventName: 'click_research_codec', id: '86719',name: 'Video Coding', link:'/research/videoCode', index: 3 },\n        { eventName: 'click_research_audio', id: '86716',name: 'Audio Processing', link:'/research/audioProcessing', index: 4 },\n        { eventName: 'click_research_process', id: '82088',name: 'Video Processing', link:'/research/videoProcessing', index: 5 },\n        { eventName: 'click_research_perceived', id: '86720',name: 'Heterogeneous Platform', link:'/research/perceptual', index: 6 },\n        { eventName: 'click_research_assess', id: '82092',name: 'Multimedia QA', link:'/research/assessment', index: 7 },        \n        { eventName: 'click_research_understanding', id: '82087',name: 'Intelligent Media', link:'/research/VideoUnderstanding', index: 8 },\n      ]\n    },\n    {\n      name: 'Research Topics',\n      showList: false,\n      childen: [\n        { eventName: 'click_top_project', id: '82082',name: 'Research Topics', link:'/project' },\n        { eventName: 'click_project_TPG', id: '82096',name: 'TPG', link:'/projectTpg' },\n        { eventName: 'click_project_TSE', id: '82097',name: 'TSE', link:'/projectTse' },\n        { eventName: 'click_project_liying', id: '82098',name: 'KANASKY', link:'/projectLiYing' },\n        { eventName: 'click_project_platform', id: '82099',name: 'TMEC', link:'/projectAudioAndVideo' }\n      ]\n    },\n    {\n      name: 'Solutions',\n      showList: false,\n      childen: [\n        { eventName: 'click_top_solution', id: '82083',name: 'Solutions', bgc: false, link:'/solution' },\n        { eventName: 'click_solution_TRTC', id: '82093',name: 'TRTC', bgc: true, link:'https://cloud.tencent.com/product/trtc '},\n        { eventName: 'click_solution_ILVB', id: '82094',name: 'ILVB', bgc: true, link:'https://cloud.tencent.com/solution/ilvb' },\n        { eventName: 'click_solution_GME', id: '82095',name: 'GME', bgc: true, link:' https://cloud.tencent.com/product/gme' }\n      ]\n    },\n    {\n      name: 'About Us',\n      showList: false,\n      childen: [\n        { name: 'Tencent Media Lab', link:'/aboutUs/laboratory' },\n        // { name: 'Paper', link:'/aboutUs/thesis' },\n        { name: 'Join us', link:'/aboutUs/joinUs' }\n      ]\n    }\n  ]\n  }","export default {\n    //  \n     title:\"Research\",\n\n    //  home\n\n\n\n\n    // 1. \n    researchAndMade:{\n        title:\"International Standardization\",\n        text:\"We focus on research and development of multimedia standards, including media compression, transmission and systems. Meanwhile, we actively participate in industry consortia promoting the industry development with technologies.\",\n        explain:\":\",\n\n        //   tabs    --tab\n        \n        tab1:{\n            title:\"Multimedia Compression\",\n            list:[\n                {\n                    subTitle:\"VVC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"Versatile Video Coding (VVC) is a video compression standard being developed for finalization around 2020 by the Joint Video Experts Team (JVET) which is a united video expert team of the MPEG  and the ITU-T VCEG. \",\n                            \"VVC will be the successor to High Efficiency Video Coding (HEVC) and targets at 40%-50% bit-rate reduction over HEVC. Media Lab actively contributes to VVC and many proposals have been adopted in VVC. \",\n                            \"Media Lab holds several key positions in JVET. Dr. Shan Liu is one of the VVC standard editors. Dr. Xiang Li is one of the VVC reference software co-chairs.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG-5 EVC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MPEG-5 Essential Video Coding (EVC) is another video coding standard that is expected to be completed in 2020.\",\n                            \"The standard is to consist of a royalty-free subset and individually switchable enhancements. With the enhancements, the coding efficiency of MPEG-5 EVC will be noticeably higher than HEVC. \",\n                            \"Media Labs screen content coding technology has been adopted in MPEG-5 EVC.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"AVS-3\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"Audio Video coding Standard (AVS) workgroup of China was authorized to be established by the Science and Technology Department under the former Ministry of Industry and Information Technology of Peoples Republic of China in June 2002. \",\n                            \"A series of successful video coding standards have been developed by this workgroup, among which AVS-1 is comparable to H.264/AVC standard in coding efficiency; AVS-2 is comparable to H.265/HEVC standard; AVS-3 v1 has been finalized and can reportedly achieve ~30% bit-rate reduction when compared with AVS-2. \",\n                            \"AVS-3 v2 development is ongoing and targets at 40% bit-rate reduction when compared with AVS-2. Media Lab is actively participating in AVS-3 development and takes leadership as one of the chairs for screen content coding activities. \",\n                            \"Media Labs screen content coding technology has been adopted in AVS-3 v2.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG-PCC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MPEG (the Moving Picture Experts Group) is developing two video coding standards to address the needs for efficient dissemination of 3D point cloud data which consists of the position and the attribute information of each point in 3D objects or scenes. \",\n                            \"The first standard is V-PCC (ISO/IEC 23090-9 Video-based Point Cloud Compression), which performs a conversion of the dense 3D point cloud into a 2D video that can be compressed by leveraging existing video codecs such as AVC, HEVC, and VVC.  \",\n                            \"The second one is G-PCC (ISO/IEC 23090-5 Geometry-based Point Cloud Compression), which is more appropriate for sparse point clouds and directly codes the 3D geometry and associated attributes representing large point clouds. These two compression standards find many promising areas of relevance such as AR/VR, autonomous driving, map services, cultural heritage, and industrial applications. Media Lab is actively participating in both standards leading several sub-activities. \"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"AVS-PCC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"A point cloud is a set of 3D points used to represent 3D objects or scenes.\",\n                            \"It includes not only geometry information but also attribute information such as color or reflectance, etc. Point clouds have been widely used in many areas such as autonomous driving, geographic information systems (GIS), and culture heritage preservations, etc.  AVS point cloud coding sub-group (AVS-PCC) was established in June 2019 in AVS workgroup to investigate use cases, technique requirements and technique solutions of point cloud compression.\",\n                            \"It is expected that a Call for Evidence (CfE) will be issued after August meeting in 2019 and a Call for Proposal (CfP) will be issued in early 2020. \",\n                            \"Media Lab is actively participating in AVS-PCC and also takes leadership role as one of three joint chairs of the sub-group and chairs of several activities.\"\n                         ]}\n                    ]\n                }\n            ]\n        },\n        // -- tab \n        tab2:{\n            title:\"Transmission and System\",\n            list:[\n                {\n                    subTitle:\"OCP\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"The Open Compute Project (OCP, http:// https://www.opencompute.org) standardizes datacenter hardware and certain low-level APIs. \",\n                            \"Media Lab contributes primarily towards accelerator hardware and API projects, in particular for video and point cloud compression.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"IETF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"The mission of the IETF (http://www.ietf.org) is to make the Internet work better by producing high quality, relevant technical documents that influence the way people design, use, and manage the Internet. \",\n                            \"Standards produced by the IETF include IP, TCP, UDP, and HTTP.  \",\n                            \"Media Lab participates in subgroups with a focus on media transport, including AVT, QUIC, and MOPS.  Dr. Wenger also holds the position of an IETF Trustee.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"3GPP SA\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"3GPP is, at present, the dominant standardization organization for the 5th the fifth generation cellular network technology (5G). \",\n                            \"Media Lab participates primarily in the System Architecture groups, namely in SA plenary and SA4 (codec).  The former has certain oversight functions, the latter standardizes audio/video codecs and their interface to the transport protocol stack.  Media Lab also follows other working groups including SA1, SA2, SA6, and some of the Core Technology (CT) working groups.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG SYSTEMS\",\n                    subContanier: [\n                        {sencTitle:\"OMAF\",sencText:[\n                            \"MPEG Omnidirectional Media Format (OMAF, ISO/IEC ISO/IEC 23090-2) provides a specification for delivery of VR/360 video and multimedia content.  In an OMAF experience, the viewers perspective is from the center of the sphere looking outward towards the inside surface of the sphere. \",\n                            \"OMAF 1st edition specification defines projection and packing schemes for mapping 3D content to 2D content for encoding, storage of the encoded content in ISO Based File Format, and finally encapsulation, signaling and streaming of the encoded content with Dynamic Adaptive Streaming over HTTP (DASH) as well as MPEG Media Transport (MMT). It also includes a few media and presentation profiles by defining specific codecs and combinations for each profile. \",\n                            \"MPEG published the 1st edition of OMAF in January 2019 and currently working on the 2nd edition. The 2nd edition will include overlay support as well as late binding of tile-streaming of 360 Video among other features. \",\n                            \"Media Lab is participating in the 2nd edition development of OMAF. Dr. Choi is one of the OMAF standard editors.\"\n                        ]},\n                         {sencTitle:\"MPEG DASH\",sencText:[\n                            \"MPEG Dynamic Adaptive Streaming over HTTP (DASH) standard (ISO/IEC 23009) is a set of specifications primarily for interoperable streaming of multimedia content over the internet. The core specification defines a manifest and a segment format. The manifest describes the location of segmented content resources and their variation with different bitrates so that the DASH client can adapt to the variable network bandwidth by switching between various bitrates. The segment format defines the fragmented file format for media content.\",\n                            \"MPEG DASH is codec and protocol agnostic, i.e. it can support any video and audio codec and media types, as well as various delivery protocols such as HTTP1.1, HTTP2, multicast, and broadcast.\",\n                            \"DASH has been widely adopted by industry and consortia, including ATSC 3.0, DVB, HbbTV, 3GPP, VR-IF and CTA WAVE. MPEG DASH specification has several parts. MPEG is about to publish the 4th edition of the core spec (Part 1) and is currently working on new extensions and additional parts. Dr. Iraj Sodagar has been the MPEG DASH subgroup chair in MPEG since the start of DASH standardization in 2009. He is also co-editors of DASH specifications.\",\n                            \"Media Lab actively participates in the DASH standard developments, and several of its submissions were adopted into the 4th edition.\"\n                        ]},\n                        {sencTitle:\"CMAF\",sencText:[\n                            \"MPEG Common Media Format (ISO/IEC 23000-19) is an encoding format to be used by the various streaming protocol. It provides a fragmented file format along with encoding constraints for creating multi-rate encoded content, which can be used with MPEG-DASH as well as other streaming formats such as HTTP Live Streaming (HLS).\",\n                            \"CMAF defines the file format constraints, the common encryption modes and encoding constraints for its media profiles. It also defines a set of commonly used media profiles (with defined codec, profile, level, and other characteristics). With CMAF, a service provider can encode the content once and deliver it using different streaming protocols or delivery platforms. Therefore CMAF reduces the content encoding cost as well as the CDN efficiency and cost during delivery.\",\n                            \"MPEG is about to publish the 2nd edition of CMAF and is currently working on the 3rd edition.\",\n                            \"Dr. Sodagar was the first co-chair of CMAF subgroup in MPEG and Media Lab actively participates in CMAF spec developments.\"\n                        ]},\n                        {sencTitle:\"MPEG NBMP\",sencText:[\n                            \"MPEG Network-Based Media Processing (NBMP, ISO/IEC 23090-8) provides a standard for building, deploying and managing media workflows over Cloud platforms. MPEG NBMP enables a service provider to define a media workflow and request the NBMP Workflow Manager to build it using a repository of pre-built media functions without writing a single line of software. \",\n                            \"NBMP is platform-agnostic i.e. it can deploy services on any private or public cloud platform or a hybrid of various Cloud platforms. It establishes the workflow, manages the tasks, monitors the media pipeline and provides reports back to the service provider. \",\n                            \"MPEG is currently developing the NBMP specification with a target publication date of H1/2020. 4.Media Lab is one of the key contributors to NBMP specification, with dozen adopted submissions into the specification and Dr. Iraj Sodagar is the co-editor of NBMP specification.\"\n                        ]},\n                    ]\n                }\n            ]\n        },\n        //  -- tab\n        tab3:{\n            title:\"Industry Consortia\",\n            list:[\n                { \n                    subTitle:\"MC-IF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MC-IF furthers the adoption of MPEG Standards.\",\n                            \"It initially focuses on VVC, by establishing them as well-accepted and widely used standards for the benefit of consumers and industry.  \",\n                            \"Media Lab, on Tencents behalf was a founder of this organization, and Dr. Wenger sits on its Board of Directors and chairs the IP Ecosystem working group.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"DASH-IF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"The DASH Industry Forum (DASH-IF) is an industry forum to foster the deployment of MPEG-DASH standard. \",\n                            \"DASH-IF currently has 80 member companies and since 2012 has developed several implementation guidelines and specifications on DASH deployment, content protection exchange format for server-side encryption services, extensive test vector database, and conformance tools for on-demand and live content, DASH live streaming simulator, packaging tools, and test platforms, dash.js open-source client, and several white papers. \",\n                            \"DASH-IF collaborates closely with MPEG on the DASH spec developments as well as with other consortia such as 3GPP, DVB, W3C, ATSC, CTA WAVE and HbbTV on DASH consortia-related specifications and deployments. Dr. Iraj Sodagar is DASH-IF President/Chairman of Board.\",\n                            \"Media Lab is leading and contributing to some of the technical activities in DASH-IF.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"8K Association\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"8K Association is an organization to address the concerns of the 8K ecosystem over 5G network. The missions of 8K Association are promoting 8K TVs and 8K Content to consumers and professionals, developing technical requirements and specification for 8K visual quality over 5G, and facilitating communication within 8K and 5G ecosystem for commercialization. \",\n                            \"Tencent Media Lab. was invited as a Principal member & Board of Director (BoD) candidate of 8K Association. Now, Dr. Stephan Wenger and Dr. Byeongdoo Choi are coordinating the collaboration with global companies including Samsung, Intel, TCL, to lead 8K-ecosystem.\"\n                        ]}\n                    ]\n                }\n            ]\n        }\n       },\n        \n\n    //  2.\n    newMedia:{\n         title:\"Immersive Media\",\n         text:\"Our team seeks to expand the possibilities of how people interact with information and media through fully immersive virtual reality.  We are infusing cutting edge technologies in capture, compression, playback, etc. into our products to create end to end solutions, allowing users to experience the world through brand new perspectives.\",\n         explain:\"Function Points\",\n         list:[\n             {\n                subTitle:\"Capture\",\n                subContanier:\"Research data capture methods with a focus on improving video quality through algorithms used for stitching, projection, etc.\"\n             },\n             {\n                subTitle:\"Transmission\",\n                subContanier:\"Transfer of multi-dimensional data for immersive virtual reality places significantly greater demands on bandwidth than traditional media.  As a result, stability and quality are both important hurdles.By focusing on ROI transmitting algorithms, our team was able to bandwidth requirements by 50% while maintaining the quality of the data stream.\"\n             },\n             {\n                subTitle:\"Transcoding\",\n                subContanier:\"Our VR transcoding server is compatible with a range of end-client needs, supporting adaptive bitrate, multiple formats, and differing processing methods.\"\n            },\n            {\n                subTitle:\"Playback\",\n                subContanier:\"Multi-Platform playback in HLS/DASH protocols is integrated with ABR, ROI, and projection algorithms.  Mobile, PC, and tablet products are supported.\"\n            },\n            {\n                subTitle:\"Interactive\",\n                subContanier:\"Interactive 3D models worked with customized data.\"\n            },\n         ],\n\n         business:\"Partnerships\",\n         businessList:[\n             {\n                subTitle:\"Cultural Tourism  Smart Town\t\t\t\t\t\t\",\n                subContanier:\"We created a completely new travel experience by highlighting cultural and local specialties while personalizing services with our immersive guide.\"\n             },\n             {\n                subTitle:\"EducationCollaborating with our partner,\",\n                subContanier:\"We transformed the students experiences by bringing study subjects to life with VR technology.  The new teaching methods generated greater student enthusiasm and improved learning results.\"\n            }\n         ]\n\n    },\n       \n\n    //   3.\n    VideoCodingAndDecoding:{\n         title:\"Video Coding\",\n         text:\"Video contents occupy a majority of communication information nowadays. The storage and delivery of raw video are under great pressure due to the massive size of uncompressed data. To alleviate such a pressure, it is critical to achieve higher compression ratio and better channel adaptivity. Video coding standard and technology are constantly evolving, empowering a lot of related areas.\",\n         explain :\"Function Points\",\n         list:[\n             {\n                subTitle:\"Software Video Coding Technology\",\n                subContanier:\"We have high efficiency real-time codecs for H.264, H.265 and AVS2 that run on any platform. High Definition (HD) Real-Time encoding funtionality is well supported. Our codecs have been widely used in all kinds of real-time video communication scenarios, offering great compression efficiency and industry-leading performance.\"\n             },\n             {\n                subTitle:\"Hardware Video Coding Technology\",\n                subContanier:\"We have various hardware codecs targeting different platforms, including but not limited to PC, mobile. We have a set of intelligent algorithms taking coding efficiency and hardware complexity into consideration for switching between software and hardware codecs. As a result, the user experience can be improved significantly.\"\n            },\n            {\n                subTitle:\"Network Adaptive Coding Technology\",\n                subContanier:\"Combining the network with video coding, the encoder parameters can be adjusted automatically according to the real-time feedback of the network. For example, when the network is in bad condition, the encoders reduce bit rate or frame rate to ensure better transmission quality such that the network resilience can be improved considerably.\"\n            },\n            // {\n            //     subTitle:\"Dynamic Coding Strategy\",\n            //     subContanier:\" We dynamically adjust encoder parameters (including but not limited to resolution, frame rate, bit rate) to adapt to the network fluctuation and the unstable performance of terminal devices.\"\n            // },\n         ],\n\n         project:\"Invention project\",\n         projectName1:\"TPG\",\n         projectName2:\"TSE\"\n    },\n\n    \n\n    //  4.\n    videoUnderstanding:{\n          title:\"Intelligent Media\",\n          text:\"The multimedia analysis and understanding framework based on the cross-modal (image, audio, text, voice) temporal algorithm is built to realize media label, scene classification, event detection, video summary, multimedia description, multimedia retrieval and other functions.\",\n          explain :\"Function Points\",\n          list:[\n              {\n                subTitle:\"Video Highlight\",\n                subContanier:\"Analyze multimodal information such as visual, audio, and text in video, and use the Deep Learning based time series modeling technology to detect exciting actions and events, and automatically output highlighted video. Currently supports games, sports, variety shows and others.\"\n              },\n              {\n                subTitle:\"Smart Cover\",\n                subContanier:\"Use the highlight video and image aesthetic evaluation techniques to select the best keyframes or key clips as video covers to enhance video click conversion and user experience. Currently supports game, sports, MV, short video and others.\"\n            },\n            {\n                subTitle:\"Video Scene\",\n                subContanier:\" Using deep learning based scene recognition technology to intelligently analyze video content, capture information such as characters, scenes,  and texts in the video, and automatically generate video tags and categories.\"\n            },{\n                subTitle:\"Saliency Detection\",\n                subContanier:\"Using Deep Learning based semantic segmentation and object detection technology to detect and segment the region of interest of the video, accurately locate the human eye's attention area in the video, and provide favorable information\"\n            }\n          ]\n    },\n\n    \n\n    //   5.\n    videoHandle:{\n        title:\"Video Processing\",\n        text:\"We have been working on video processing for many years and accumulated a large amount of technology. Both mobile and backstage, both real-time and offline, both video and image, both traditional algorithms and artificial intelligence, our researches cover almost all fields of video processing. Meanwhile, each algorithm is deeply optimized for performance and effects,  and finally make it a valuable technology which can be truly applied on the product.\",\n\n\n        explain1:\"Video Restoration\",\n        list1:[\n            {\n                subTitle:\"Intelligent Denoising\",\n                subContanier:\"Real-time intelligent denoising:  We use the predictied information from the previous frame, to intelligently denoise on the current frame according to the noise intensity;Offline intelligent denoising: We combine deep learning with the predicted information from previous and subsequent frames, to search the optimal matching block for joint denoising,  and finally obtain the best denoised effect.\",\n            },\n            {\n                subTitle:\"Artifacts Removal\",\n                subContanier:\"Using deep learning,  we can not only remove the artifacts such as block effect, edge burr, which produced by the coding compression, but also ensure that the main edge can be preserved and the weak edge details can be restored,  so that the algorithm can greatly improve the subjective quality of the video.\"\n            },\n            {\n                subTitle:\"Color Restoration\",\n                subContanier:\"Using deep learning to perform high-intensity repairs on damaged frames in the video, we can restore the color of the damaged part to optimal status, thereby the color of the entire video would be restored and the subjective quality of the video can be improved.\"\n\n            },\n            {\n                subTitle:\"Intelligent Video Stabilization\",\n                subContanier:\"Real-time video stabilization: We combine the anti-shake algorithm with the location information of the hardware device on the mobile terminal to anti-shake, and make the video stable in real time;Offline video stabilization: Intelligently detecting the shaking level, we utilize the information of the previous and subsequent frames to estimate motion on the current frame, in order to anti-shake.\"\n            },\n            {\n                subTitle:\"Dehazing\",\n                subContanier:\"Based on the dark channel prior, our technique solves the problem of overexposure and the adaptation on flat area, and adapts to all scenes of each video. And after deep optimization of video on performance, our algorithm consume almost no resources.\"\n            }\n        ],\n        explain2:\"Video Enhancement\",\n        list2:[\n            {\n                subTitle:\"Intelligent Detail Enhancement\",\n                subContanier:\"According to the visual effects of the human eyes, we enhance the details on the regions that human eyes interest intelligently. It can enhance the image quality of ROI, achieve a clearer effect of the detailsand can be more beneficial for video coding.\"\n            },\n            {\n                subTitle:\"Color Enhancement\",\n                subContanier:\"Through the player's display of the original color of different videos, we adaptively adjust the different colors in the video to improve the saturation, in order to make the video content more rich and enhance the visual effects.\"\n            },\n            {\n                subTitle:\"Super-Resolution\",\n                subContanier:\"Using deep learning technology, we deeply optimize the video for different scenes of different videos. Under the premise of increasing the video resolution, our SR algorithm also greatly enhance the details in the video.\"\n            },\n            {\n                subTitle:\"Frame Rate Upsampling\",\n                subContanier:\"During the video playback, we have increased the frame rate of the video, resulting in smoother video and more detailed image. Enhanced the saturation and exquisiteness of the video,  the visual enjoyment of watching the video would be improved.\"\n            },\n            {\n                subTitle:\"Dark Scene Enhancement\",\n                subContanier:\"using cutting-edge technology combined with excellent self-research ability, we optimize the effects of various dark scene, and adapt to actual scenes. Our algorithm can achieve real-time processing of video, with consuming almost no resources .\"\n            },\n            {\n                subTitle:\"Beauty\",\n                subContanier:\"Our algorithm beautify face accurately, ensuring that the background is still clear, we can not only enhance the three-dimensional sense of the human face, eyebrows, lips and other facial features, but also erase the defects on faces, brighten eyes and whiten skin.\"\n            },\n            {\n                subTitle:\"Filter\",\n                subContanier:\"We design a variety of filters in different scenes and styles. In the case of ensuring good visual effects, our filters can present different styles, make the video quality closer to the human eyes and improve the color saturation and exquisiteness.\"\n            },\n            {\n                subTitle:\"Special Effects\",\n                subContanier:\"We can transform different scenes and different styles into the comic style, sketch style and so on, which make the video much more instereting and vivid,  and then increase the click rate.\"\n            }\n        ]\n\n    },\n\n\n    //  6.\n    touch:{\n        title:\"Heterogeneous Platform\",\n        text:\"Video encode/decode and processing needs more and more compute power,  and there is no single solution can fit various application and performance requirement.  Heterogeneous computing is the key technology and it can provide higher efficiency and low latency compute power than traditional parallel CPU architecture. \",\n        explain :\"Function Points\",\n        list:[\n            {\n                subTitle:\"Video Transcoding\",\n                subContanier:\"Transcode the video stream from one format to one or multiple video streams with different formats, can support differnt format, resolution, bitrate, framerate and configuration etc, and can also support different usage scenarios like real-time live broadcasting or non real-time VOD.\"\n            },\n            {\n                subTitle:\"Video Processing for Video Codec\",\n                subContanier:\"We perform video assisted processing by closely combining the corresponding codec information, including intelligent video enhancement used codec information,  smart erasing on video that facilitates coding and so on.\"\n            },\n            {\n                subTitle:\"Hardware platform support\",\n                subContanier:\"Can support different hardware platform, including CPU, GPU, FPGA, ASIC etc\"\n            }\n        ],\n        business:\"\",\n         businessList:[\n             {\n                subTitle:\"Cloud  Gaming \",\n                subContanier:\"Cloud gaming is a type of online gaming that aims to provide smooth and direct playability to end users of games across various devices. This could include a host gaming server capable of executing a gaming engine and streaming the gaming data to the client device.\"\n             }\n         ]\n     },\n\n    //   7.\n    audioHandle:{\n          title:\"Audio Processing\",\n          text:\"We are focusing on designing low-complexity and high-robust solutions for elimating the quality problems in voice communications for diversed usecases, to enrich the audio experiences in office and mobile devices.\",\n          explain:\"Function Points\",\n          list:[\n              {\n                subTitle:\"Audio Engine\",\n                subContanier:\"It focus on inventing world-leading audio compressions system in VoIP with high-efficient and high-quality. The solution is highly jointly correlated to  the optimizations in channel aspect to  improve the end-to-end experiences with error resillience.\"\n              },\n              {\n                subTitle:\"Machine Learning\",\n                subContanier:\"It  is targeted into design novel algorithms in elimiating complex glitches and non-stationary noise and improving the quality and intelligibility by combining state-of-art machine learning technologies. The applications include but not limit, quality improvement for end-to-end voice communication, voice beautification, and voice conversion, etc.\"\n            },\n            {\n                subTitle:\"Audio Engineering\",\n                subContanier:\"It focus on building high-efficiency coding and engineering capabability to fullfil different business and technical requirements. The outcomes include high-quality audio solutions covering server and clients, including the real-time quality monitoring.\"\n            },\n            {\n                subTitle:\"Voice Engine\",\n                subContanier:\"It is target on next-generation source coding with multi-rate QoE-control and jointly-source-channel coding, to help further improve the end-to-end subjective experience with robust QoE-control based on channel feedback.\"\n            },\n            {\n                subTitle:\"Pre-Processing (3A)\",\n                subContanier:\"It includes a set of 3A processing (i.e., Automatic Echo Cancellations, Automatic Noise Cancellations, and Automatic Gain Control) to elimate diversed artifacts in capturing aspect.\"\n            },\n            {\n                subTitle:\"AI Denoising/Separations\",\n                subContanier:\"It combines deep learning methos to detect and remove the atternuations in transmitted signal, to improve the subjective quality and intelligibility.\"\n            },\n            {\n                subTitle:\"AI Sound Beautification\",\n                subContanier:\"It targets to improve the sining sound quality, including but not limit detect word-error and word-missing, improve the tune, to improve the definitions of the signing sound with better aesthetic feelings.\"\n            },\n            {\n                subTitle:\"AI Voice Conversion\",\n                subContanier:\"It combines deep learning methods to convert the original sound to another form with another speaker's features. It changes the tones and timbre with natural approach.\"\n            },\n            {\n                subTitle:\"3D Sound Effect\",\n                subContanier:\"It append the artificial spatial features into original monophonic sound to enable the spatiness feeling, e.g., the effects of directional sound is perceived.\"\n            },\n            {\n                subTitle:\"Voice Morphing\",\n                subContanier:\"It provides the capababilities to morphing the sound and outcome interesting sound effect (e.g., lolita, robot, cartoons, etc). The configurations is able to customized to generate different morphing effect to improve the interesting in human-machine interactions.\"\n            },\n          ]\n    },\n\n   \n\n    // 8.\n    transfer:{\n          title:\"Real-Time Multimedia\",\n          text:\"Streaming High-Quality multimedia contents requires stable network conditions with significant available bandwidth, and the instability of underlying network affects the user-perceived experience. How to efficiently and accurately measure the network condition and make appropriate congestion control decision is still an unsolved problem for both academia and industry. The networking group of Tencent Media Lab focuses on designing and implementing systematical, adaptive and reliable solutions for the network-level issues for Real-Time multimedia streaming.\",\n          explain:\"Function Points\",\n          list:[\n              {\n                subTitle:\"Congestion Control\",\n                subContanier:\"We studied the State-of-the-Art congestion control algorithms from both academia and industry.  After measuring and evaluating the network traces collected from Tencent products, we design novel congestion control to make proper decision under various network conditions.\"\n              },\n              {\n                subTitle:\"Network Resilience\",\n                subContanier:\"A user may suffer various network instabilities in the wild, such as packet loss, high delay or even intermittent connections. We have proposed the intelligent network resilience algorithms to accommodate network instability and guarantee smooth and high-quality multimedia streaming.\"\n            },\n            {\n                subTitle:\"QoS/QoE Optimiaztion\",\n                subContanier:\"We perform the optimization based on the media context and network conditions to achieve the best trade-off among clarity, smoothness, and latency under constrained network environments.\"\n            },\n            {\n                subTitle:\"Flow control for Multi-User Communication\",\n                subContanier:\"We adopt two strategies to optimize the quality under multi-party conferencing: AI for simple scenarios, and subjective-test based fine adjustment for complex scenarios.\"\n            },\n            {\n                subTitle:\"Network Measurement\",\n                subContanier:\"We develop algorithms to learn network behaviors from the traces without personal information. This helps us emulate Bad-Cases offline and detect network condition online.\"\n            },\n          ]\n    },\n\n\n    // 9.\n    connectionQuality:{\n        title:\"Multimedia QA\",\n        text:\"We strive to develop end-to-end Audio/Video Quality Assessment algorithms with an emphasis on deep learning based techniques. Our research covers various multimedia contents in real-world applications. We evaluate the performance of Standard/In-House codecs on different tasks. Particularly, we are endavoring to study Pre/Post-Processing techniques for the purpose of quality enchancement. The aim is to apply quality monitoring to our product pipeline and increase the user experience of our multimedia applications.\",\n        explain:\"Function Points\",\n        list:[\n            {\n                subTitle:\"No-Reference Audio Quality Assessment\",\n                subContanier:\"Machine learning based techniques to assess the quality of real-time audio. On one hand, we evalute the degree of jerking, lagging, legibility when the network is not ideal. On the other hand, we measure the the effects of audio enhancement techniques.\"\n            },\n            {\n                subTitle:\"No-Reference Video Quality Assessment\",\n                subContanier:\"Deep learning based techniques to assess the quality of videos. The system is designed for applications either capturing with low-end consumer cameras, or embedded with quality enhancement filters, or both. Our method has launched in several video applications.\"\n            },\n            {\n                subTitle:\"Full-Reference Video Quality Assessment\",\n                subContanier:\"Deep learning based techniques to precisely measure the quality of compressed videos. The system is suitable for applications where the pristine reference is available and provides a trade-off between bit rates and viewing experience. Our method achieves STOA performance on datasets compressed standard codecs.\"\n            },\n        ],\n        project:\"Invention project\",\n        projectName:\"TMEC\",\n        projectText:\"Provide ability for efficient automatical AV(audio/video) quality assesment, conducting full automatical presssure assesment at audio and video quality in multi aspect.\",\n        more: \"learn more\"\n    },\n\n\n}","\n    export default {\n        // \n        title:\"Research Topics\",\n        textTPG:\"The ability to transcode from traditonal CDN to TPG images, which esures visual effect and reduces data/bandwith cost, saving data for massive image related scenes.\",\n        textTSE:\"The encoder specific for content optimization on screen.\",\n        textLiYing:\"Innovative video service platform, which intelligently enhances or compresses video using AI and big data based on human visual technology.\",\n        pingtai:\"Provide ability for efficient automatical AV(audio/video) quality assesment, conducting full automatical presssure assesment at audio and video quality in multi aspect.\",\n        more:\"More\",\n    \n        nav:{\n            back:\"BACK : \",\n            next:\"NEXT : \"\n        },\n    \n       \n        // TPG\n        tpg:{\n            textTPG:\"TPG is an image compression technology, leveraging AVS2 encoding kernel underneath. It provides high compression efficiency, which is 47% higher than JPG, 60% higher than GIF, and 25% higher than WebP. TPG has all kinds of image compression functionalities, including alpha channel, animation encoding and decoding. It can be used across various platforms, such as Windows , Linux , Mac , iOS and Android. TPG has been declared as a national standard as a part of AVS2-P7.\",\n            partner:\"\",\n            itemTitle1:\"Compression Performance of TPG\",\n            itemText1:\"Under the same quality of the compressed images, the compression ratio of TPG is 47% higher than JPG,60% higher than PNG, 85% higher than GIF and 25% higher than WebP\",\n            itemTitle2:\"Coding Efficiency of TPG\",\n            itemText2:\"Under the same PSNR value of the compressed images, the coding efficiency of TPG is 23.5% higher than WebP, and 46.8% higher than libjpeg\",\n            itemTitle3:\"Decoding Performance of TPG\",\n            itemText3Item1:\"1.The decoding speed of TPG is 2 times faster than webP, 2.3 times faster than libjpeg on iOS platform\",\n            itemText3Item2:\" 2. The decoding speed of TPG is 1.2 times faster than webP, 1.6 times faster than libjpeg on Android platform\",\n            itemTitle4:\"Decoding Used Memory of TPG\",\n            itemText4Item1:\"1.iOS platform: the decoding memory occupation of TPG is 2 times larger than webP, and 2.5 times larger than libjpeg\",\n            itemText4Item2:\"2. Android platform: the decoding memory occupation of TPG is 2 times larger than webP, and 1.6 times larger than libjpeg\",\n            itemTitle5:\"TPG Applications\",\n            itemText5:{\n                text1:{\n                    one:\"Social apps\",\n                    two:\"1.QZone photo album\",\n                    // three:\"2.Moments\",\n                    four:\"2.Group pictures\",\n                    // five:\"4....\"\n                },\n                text2:{\n                    one:\"News apps\",\n                    two:\"1.H5 page\",\n                    three:\"2.News Client\",\n                    four:\"3.Browser\",\n                },\n                text3:{\n                    one:\"Game apps\",\n                    two:\"1.Installation Package\",\n                    three:\"2.Game UI Resources\",\n                }\n            },\n            partners:\"Partners of TPG\"  \n        },\n    \n        // TES\n        tse:{\n           tesText:\"Tencent Screen encoder(TSE) is designed for screen content encoding. TSE adds intra block copy and palette mode support based on conventional encoding algorithms. The coding efficiency of TSE is 55% higher compared to x265-medium mode, and the encoding speed is 10 times faster than x265-media mode. TSE can be used in real-time conditions.\",\n           itemTitle1:\"Coding Efficiency Comparision of TSE and X265\",\n           itemText1one:\"Camera Sequences: The coding efficiency of TSE is 20% higher than x265-ultrafast mode\t\",\n           itemText1two:\"Screen Sequences:The coding efficiency of TSE is 70% higher than x265-ultrafast mode, and 55% higher than x265-medium mode\",\n          \n           itemTitle2:\"Encoding Speed Comparision of TSE and X265\",\n           itemText2one:\"Camera Sequences:The encoding time of TSE is around 88% of x265-ultrafast mode on average\",\n           itemText2two:\"Screen Sequances:1. Turn off IBC and PLT: The encoding time of TSE is about 33% of that of x265-ultrafast mode on average; 2. Turn on IBC and PLT: The encoding time of TSE is about 50% of that of x265-ultrafast mode on average \",\n           partner:\"Partners of TSE\",\n        },\n    \n        // \n        liYing:{\n           title:\"KANASKY\",\n           textLiYing:\"Kanasky is a next generation video service platform, which intelligently produces smaller yet prettier video using human perception, artificial intelligence and big data technologies.\",\n           vidio:\"\",\n           itemTitle1:\"Human Perceptual Technology - Saliency Detection\",\n           itemTitle2:\"Human Perceptual Technology - Artifact Reduction\",\n           itemTitle3:\"Human Perceptual Technology - Super Resolution\",\n           itemTitle4:\"Human Perceptual Technology - Old Picture Restoration\",\n           partner:\"Partners of KANASKY\", \n           before:\"Original Image\",\n           after:\"Processed\"\n        },\n    \n    \n        // \n        vidioAndaudioTest:{\n            title:\"TMEC\",\n            pingtaiText:\"TMEC(Tencent Multimedia Evaluation Cloud) is an One-Stop service platform for evaluating the QoE of multimedia, it can make the evaluation more efficient, various commercial equipment and self-developed tools are integrated in this platform.The Real-Time call app can be evaluated under different virtual net environment with TMEC.\",\n            itemText1:\"The professional equipments and laboratory are usually very necessary for evaluation of multimedia, such as network emulation, relevant media protocal meter, etc. On the other hand, the competence requirement and learning curve of test enare relatively higher. TMEC is a platform founded on above mentioned equipments and techniques. It supports remote task submission, automated execution and report generation which can improve the evaluation procedure efficently\",\n            itemText2:\"The design concept of TMEC is providing a general, fair, accurate and reliable real-time call evaluation methodology and technique support.Besides of real-time call e2e evaluation solution, some general tools are also provided as cloud tools for other evaluation purposes, such as vdieo reference assessment, video Non-Reference assessment, audio echo assessment and so on.\",\n        }\n    }\n    ","export default {\n    // \n    title:\"Solutions\",\n    list:[\n        {\n            title:\"TRTC\",\n            title1:\"Tencent Real-Time Communication(TRTC)\",\n            text:\"With 10+ years serving QQ Real-Time video communications, TRTC dedicates to provide high quality and low cost solutions for multimedia business.\",\n        },\n        {\n            title:\"ILVB\",\n            title1:\"Interactive Live Video Broadcasting(ILVB)\",\n            text:\"A brand new All-In-One solution for Multi-Way audio video interaction. Audio interaction with the host, split view display, cross-platform one-to-many, Multi-Way high definition webcasting via mobile webcasting SDK.\",\n        },\n        {\n            title:\"GME\",\n            title1:\"Tencent Cloud Game Multimedia Engine (GME)\",\n            text:\"All-In-One voice communication solution for gaming. Highly optimized for various gaming scenes, full functionalities, easy integration. One single SDK fits all.\",\n        },\n    ]\n}","export default {\n  bannerTitle:\"About Us\",\n  title:\"Tencent Media Lab\",\n  text:\"Tencent Media Lab (TML) dedicates to Cutting-Edge research on audio and video technologies, including real-time video communication, advaced audio and video codec algorithms and standardization, computer vision, image processing, and multimedia quality evaluation. TML is recognized as an elite pioneer in the multimedia industry, providing total solutions with leading technologies across various multimedia applications.\",\n  members:\"\",\n  sideNavBarTitle:[\"Tencent Media Lab Introduction\",\"Paper\",\"Join us\"],\n\n  //-\n  team: \"Team\",\n  expertList:[\n    {\n      id: 1,\n      imgClass: \"icon-about_image_expert_0\",\n      name:\"Dr. Shan Liu\",\n      text:\"Shan Liu is a Distinguished Scientist and General Manager at Tencent where she heads the Tencent Media Lab. Prior to joining Tencent she was the Chief Scientist and Head of America Media Lab at Futurewei Technologies. She was formerly Director of Multimedia Technology Division at MediaTek USA. She was also formerly with MERL, Sony and IBM. Dr. Liu is the inventor of more than 200 US and global patent applications and the author of more than 60 journal and conference articles. She actively contributes to international standards such as ITU-T H.265 | ISO/IEC HEVC, MPEG-DASH, MPEG-I, etc. and served as co-Editor of H.265/HEVC v4 and MPEG-I VVC. She was in technical and organizing committees, or an invited speaker, at various international conferences including IEEE ICIP, VCIP, ICNC, ICME, MIPR and ACM Multimedia. She served in Industrial Relationship Committee of IEEE Signal Processing Society 2014-2015. She was the VP of Industrial Relations and Development of Asia-Pacific Signal and Information Processing Association (APSIPA) 2016-2017 and was named the Industrial Distinguished Leader of APSIPA in 2018. Dr. Liu obtained her B.Eng. degree in Electronics Engineering from Tsinghua University, Beijing, China and M.S. and Ph.D. degrees in Electrical Engineering from University of Southern California, Los Angeles, USA.\" \n    },\n    {\n      id: 2,\n      imgClass: \"icon-about_image_expert_1\",\n      name:\"Dr. Xiang Li\",\n      text:\"Xiang Li received the B.Sc. and M.Sc. degrees in electronic engineering from Tsinghua University, Beijing, China, and the Dr.-Ing degree in Electrical, Electronic and Communication Engineering from University of Erlangen-Nuremberg, Germany. He is currently a senior principal researcher and the head of video coding standards in Tencents Media Lab. Before joining Tencent, he was with Qualcomm and Siemens. Dr. Li has been working in the field of video compression for years and is an active contributor to international video coding standards. He served as chair and co-chair in a number of Ad Hoc groups, core experiments, including the co-chair of JEM reference software, VVC reference software, and co-editor of MPEG-5 EVC. Dr. Li is a senior member of IEEE. He has published over 40 journal and conference papers, 300+ standard contributions, and hold 120+ US granted and pending patents.\" \n    },\n    {\n      id: 3,\n      imgClass: \"icon-about_image_expert_2\",\n      name: \"Dr. Xin Zhao\",\n      text: \"Dr. Xin Zhao received the B.S. degree in electronic engineering from Tsinghua University, Beijing, China, in 2006, and the Ph.D. degree in computer applications from Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China.In 2017, he joined Tencent, Palo Alto, CA, USA, where he is currently a Principal Researcher, focusing on the research and development of VVC standard, and leading a team developing new video coding standard. From 2012 to 2017, he was a Staff Engineer with Qualcomm, San Diego, CA, USA. Since 2009, he has been actively contributing to the development of international video coding standards and their extensions, such as HEVC, 3D extensions to H.264/AVC and HEVC standards, and VVC with Joint Video Exploration Team (JVET).Dr. Zhao has over 12 years experience on video coding and processing algorithms. He has contributed over 200 proposals to multiple video coding standards, including around 30 adopted proposals. Dr. Zhao has been serving as the chairs of core experiments and multiple Break-out-Groups and Ad-hoc Group established in JVET. He is the inventor of over 260 filed patent applications with around 20 granted patents, and author of over 30 papers published in top-tier academic journals and conferences. His current research interests include image and video coding, video processing.\"\n    },\n    {\n      id: 4,\n      imgClass: \"icon-about_image_expert_3\",\n      name:\"Dr. Stephan Wenger\",\n      text:\"Dr. Stephan Wenger is Sr. Director of Intellectual Property and Standards in Tencent America LLC.  Until early 2018, he was VP IP & Standards at Vidyo Inc., a leading supplier of unified collaboration solutions, and before that, he was active in Nokia's Intellectual Property Rights department and research center.  He has also helped start companies in the field of multimedia coding and served on the Board of Directors of UB Video Inc. and eBrisk Video until their successful acquisitions in 2006 and 2016, respectively.  He is also very active in the standardization for new Multimedia technologies, especially in the IETF, ITU-T, and MPEG.   Dr. Wenger has been inventor of some 40 patents with many more pending. He received the diploma and Dr.-Ing degrees in Computer Science from Technische Universitt Berlin, Germany, in 1989 and 1995 respectively.\" \n    },\n    {\n      id: 5,\n      imgClass: \"icon-about_image_expert_4\",\n      name:\"Dr. Iraj Sodagar\",\n      text:\"Iraj Sodagar joined Tencent Americas Media Lab in November 2018 as a Principal Researcher. Before Tencent, he was with Microsoft for 10 years as a Principal Multimedia System Architect. At Microsoft, he worked with various research and product groups in the development and standardization of multimedia technologies. He was responsible for Windows multimedia delivery strategy and alignment of products and standardization, as well as coordination of all streaming standardization cross Microsoft. Throughout the last 25 years, Dr. Sodagar has participated, led and managed various R&D projects, advanced architecture design and product development including image and video coding, media indexing and analysis, media storage, transport and delivery, and media transcoding on cloud.  He built and managed technical teams for R&D, software development and productization, and standardization of developed technology. He had various leadership positions and engagements in ITU-T, JPEG, MPEG, AMQP, W3C, DVB, 3GPP, W3C, CTA WAVE, DLNA and AOMedia organizations. He also started Wireless Media Forum, the first consortia on mobile video delivery in early 2000s. More recently, he has been the chair of MPEGs DASH subgroup from its very start. He is also the founder, and the President and Chairman of Board of DASH-Industry Forum (DASH-IF). He was also the co-chair of MPEGs CMAF subgroup from the start until publication of its 1st edition.He received the Ph.D. degree in electrical engineering from Georgia Institute of Technology, Atlanta, in 1994.\" \n    },\n    {\n      id: 6,\n      imgClass: \"icon-about_image_expert_5\",\n      name:\"Dr. Xiaozhong Xu\",\n      text:\"Dr.Xiaozhong Xu joined Tencent Media Lab in 2017.He received his B.S. and Ph.D. degrees both from Department of Electronic Egnineering, Tsinghua University. He started working on video coding research and standardization since 2004. He has participated in multiple national and international video coding standard activities, including H.264/AVC, H.265/HEVC, H.266/VVC, MPEG-5, ITU-T IPTV and AVS. For those standardization activities, he have submitted more than 100 technical contributions and had more than 20 of them been adopted by the related standard committees. Dr. Xu has been actively promoting the standardization work by serving as a co-chair for screen content coding sub-groups in H.265/HEVC, H.266/VVC and AVS standards, and as a co-editor for MPEG-5 specification.\" \n    },\n    {\n      id: 7,\n      imgClass: \"icon-about_image_expert_6\",\n      name:\"Dr. Bin Zhu\",\n      text:\"Bin graduated from Iowa State University with Ph.D in Electrical & Computer Engineering. After graduation, Bin worked in several semiconductor start ups and later joined Intel and Apple to lead the team to develop core video products and technologies.  Bin joined Tencent in Dec, 2018 and lead the team to develop next generation(VVC/H266) video codec products.\" \n  }\n],\n\n\n\n\n  //-\n  contactTitle: \"Please contact  Tencent Media Lab\",\n  contactEmail: \"medialab@tencent.com\",\n  positionList: [\n    {\n      id: 1,\n      positionName: \"Video Kernel Developing Principal Engineer\",\n      address: \"Working Place Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"- Optimize H264H265 coding algorithm.\",\n          \"- Follow up frontier coding standard establishment in industry, such as H266, AV1,etc.\"\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- Expert knowledge in video coding basic algorithms, such as H264, HEVC, etc.\",\n          \"- Experience in assembly optimization and GPU acceleration development is preferred.\",\n          \"- Expert in open-source framework such as ffmpegx264 and x265 with the ability of mobile hardware acceleration development.\",\n          \"- Solid knowledge in math and data structure, good written and oral communication skills.\"\n        ]\n      }\n    },\n    {\n      id: 2,\n      positionName: \"Video Processing Principal Engineer\",\n      address: \"Working Place Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"- Fundamental video processing algorithms, video enhancement and assessment, cross platform solution, etc.\",\n          \"- Video processing algorithm development, including video denoiser, artifact reduction, color and edge enhancement, frame rate up conversion, etc.\",\n          \"- Familiar with temporal technologies and solutions for video processing, in both fields of deep learning and signal processing.\",\n          \"- Video processing algorithm optimization across various platforms, e.g. x86, ARM.\",\n          \"- Good knowledge on quality assessment standards and methods for video enhancement.\"\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- Expert knowledge in video,  image, and signal processing principles, algorithms, and techniques.\",\n          \"- Expert knowledge in quality assessment standards and methods for video enhancement.\",\n          \"- Expert knowledge in Convolutional Neural Network (CNN) techniques.\",\n          \"- Familiarity with video compression standards and codec would be a plus.\",\n          \"- Familiarity with iOS and Android software development would be a plus.\",\n          \"- Excellent software design, problem solving, and debugging skills.\",\n          \"- Solid programming skills and C/C++, Python coding abilities.\",\n          \"- Good written and oral communication skills.\"\n        ]\n      }\n    },\n    {\n      id: 3,\n      positionName: \"Video Network Principal Engineer\",\n      address: \"Working Place Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"- Network algorithm research and developement on auido/video for global real-time communication.\",\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- C/C++ coding abilities, familiar with TCP/IP protocol stacks, experience in high loading APP development.\",\n          \"- Expert in audio/video coding and open-source framework such as FFmpeg, VLC, WebRTC, X264, etc. Expert in online audio/video core technology, such as JITTER BUFFER and FEC, along with bandwidth prediction and code rate self-adaption technology. Experience in online audio/video communication projects is preferred.\",\n          \"- Expert in network deployment and operation. Experience in oversea acceleration, CDN maintainance and edge computing is preferred.\",\n          \"- Expert in network metrics. Experience in research on FCC Speed Test principles, protocol and reliazaion is preferred.\"\n        ]\n      }\n    },\n    {\n      id: 4,\n      positionName: \"Audio Algorithm Researcher\",\n      address: \"Working Place Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"- Fondamental audio algorithm, echo cancellation, noise suppressionvoice conversion and microphone array technology,etc.\",\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- Master or doctor degree in computier engineering, sensory science, computer science, physics, maths or related fields.\",\n          \"- Solid programming skills and C/C++, Python coding abilities. Capable of adaptating and optmizing machine learning algorithm among different mobile platforms.\",\n          \"- Experience in audio signal processing and content analysis.\",\n          \"- Experience in pattern recognition, machine learning, linguistics and signal processing.\",\n          \"- Experience in academic competitions like Kaggle or frontier labs in audio/video signal process/machine learning/data mining. Top on significant dataset such as ImageNet.\",\n          \"- Good written and oral communication skills.\"\n        ]\n      }\n    },\n    {\n      id: 5,\n      positionName: \"Audio Kernel Developing Principal Engineer\",\n      address: \"Working Place Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"-Audio commucation core coding for QQ and Tencent Cloud, before-and-after algorithm, multi-platform voice engine, before algorithm realted to voice recognition, such as mobile corss-platform voice acquisition and broadcasting.\",\n          \"-Before-and-after algorithm for echo cancellation, noise suppression, silence detection, auto gain, etch. \",\n          \"-Network transmission adaption, FEC strategy, buffering control, error concealment.\",\n          \"-Core coding optimization in adaptation to different platforms.\"\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- Master degree in computer and comminication with working experience over 2 years.\",\n          \"- Expert knowledge in digital signal processing, audio coding standard and audio core algorithm.\",\n          \"- Experience in before-and-after algorithm for echo cancellation, noise suppression, silence detection, auto gain and  frequency response equalizer.\",\n          \"- Expert in audio network transmission and Qos control.\",\n          \"- Expert in C/C++ and software development.\",\n          \"- Experience in mobile(iOS/Andorid) development and optimization is preferred.\",\n          \"- Good written and oral communication skills.\"\n        ]\n      }\n    }\n  ],\n\n   //-\n   navList: [\"All\",\"Speech and Audio\",\"Transmission Optimization\",\"Image quality assessment\",\"Video Coding\",\"Computer Vision\",\"3D point cloud\"],\n   Total: 'Total ',\n   item: '  ',\n   thesisList:[\n    [\n      [22,\"all\"],\n      [\n        [\n          {\n            id:'all-3-5',\n            title: \"Multiple Reference Line Coding for Most Probable Modes in Intra Prediction\",\n            author: \"Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand\",\n            time: \"2019\",\n            content: \"This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.\",\n            src: \"https://ieeexplore.ieee.org/document/8712640\"\n          },\n          {\n            id:'all-3-4',\n            title: \"Wide Angular Intra Prediction for Versatile Video Coding\",\n            author: \"Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape\",\n            time: \"2019\",\n            content: \"This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).\",\n            src: \"https://ieeexplore.ieee.org/document/8712681\"\n          },\n          {\n            id:'all-3-1',\n            title: \"Blind image quality assessment based on joint log-contrast statistics\",\n            author: \"Yabin Zhang et at.\",\n            time: \"2019\",\n            content: \"During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.\",\n            src: \"https://www.sciencedirect.com/science/article/pii/S0925231218313432\"\n          },\n          {\n            id:'all-4-1',\n            title: \"Fast Adaptive Multiple Transform for Versatile Video Coding\",\n            author: \"Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu\",\n            time: \"2019\",\n            content: \"The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.\",\n            src: \"https://ieeexplore.ieee.org/document/8712650\"\n          },\n          {\n            id:'all-4-2',\n            title: \"Recent advances in video coding beyond the HEVC standard\",\n            author: \"Xiaozhong Xu, Shan Liu\",\n            time: \"2019\",\n            content: \"The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.\",\n            src: \"https://doi.org/10.1017/ATSIP.2019.11\"\n          },\n          \n        ],\n        [\n          {\n            id:'all-4-3',\n            title: \"Current Picture Referencing in Versatile Video Coding\",\n            author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n            time: \"2019\",\n            content: \"Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.\",\n            src: \"https://ieeexplore.ieee.org/document/8695359\"\n          },\n          {\n            id:'all-5-1',\n            title: \"High Performance Gesture Recognition via Effective and Efficient Temporal Modeling\",\n            author: \"Yang Yi, Feng Li, et al.\",\n            time: \"2019\",\n            content: \"State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization\",\n            src: \"https://www.ijcai.org/proceedings/2019/0141.pdf\"\n          },\n          {\n            id:'all-5-2',\n            title: \"A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.\",\n            author: \"Yabin Zhang, et al\",\n            time: \"2019\",\n            content: \"3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.\",\n            src: \"https://ieeexplore.ieee.org/document/8704253\"\n          },\n          {\n            id:'all-1-1',\n            title: \"A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network\",\n            author: \"sarahqwang\",\n            time: \"2018\",\n            content: \"The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8706582\"\n          },\n          {\n            id:'all-2-1',\n            title: \"Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy\",\n            author: \"zeqilai\",\n            time: \"2018\",\n            content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in todays network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.\",\n            src: \"https://dl.acm.org/citation.cfm?id=3318101\"\n          }\n        ],\n        [\n          {\n            id:'all-3-2',\n            title: \"Intra Block Copy for Next Generation Video Coding\",\n            author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n            time: \"2018\",\n            content: \"Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.\",\n            src: \"https://ieeexplore.ieee.org/document/8551528\"\n          },\n          {\n            id:'all-3-3',\n            title: \"Coupled Primary and Secondary Transform for Next Generation Video Coding\",\n            author: \"Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu\",\n            time: \"2018\",\n            content: \"The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.\",\n            src: \"https://ieeexplore.ieee.org/document/8698635\"\n          },\n          {\n            id:'all-1-5',\n            title: \"Gaussian density guided deep neural network for single-channel speech enhancement\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8168116\"\n          },\n          {\n            id:'all-1-3',\n            title: \"A maximum likelihood approach to deep neural network based speech dereverberation\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8282019\"\n          },\n          {\n            id:'all-1-2',\n            title: \"A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.\",\n            src: \"https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF\"\n          }\n        ],\n        [\n          {\n            id:'all-1-4',\n            title: \"A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemalefemale, malemale, and femalemale mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/7917323\"\n          },\n          {\n            id:'all-2-2',\n            title: \"Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.\",\n            src: \"https://ieeexplore.ieee.org/document/8117568\"\n          },\n          {\n            id:'all-2-3',\n            title: \"Furion: Engineering high-quality immersive virtual reality on today's mobile devices\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).\",\n            src: \"https://dl.acm.org/citation.cfm?id=3117815\"\n          },\n          {\n            id:'all-4-4',\n            title: \"TPG Image Compression Technology\",\n            author: \"ShitaoWangPiaoDingXiaozhengHuangHanjunLiuBinjiLuoXinxingChenYoubaoWuRonggangWang\",\n            time: \"\",\n            content: \"TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression\",\n            src: \"http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238\"\n          },\n          {\n            id:'all-4-5',\n            title: \"Saliency detection with two-level fully convolutional networks\",\n            author: \"Yang Yi, et al.\",\n            time: \"2017\",\n            content: \"This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.\",\n            src: \"https://ieeexplore.ieee.org/document/8019309/\"\n          },\n        ],\n        [\n          {\n            id:'all-2-4',\n            title: \"Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.\",\n            src: \"https://ieeexplore.ieee.org/document/7898362\"\n          },\n          {\n            id:'all-2-5',\n            title: \"Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of users input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.\",\n            src: \"https://dl.acm.org/citation.cfm?id=3234201\"\n          }\n        ]\n      ]\n    ],\n    [\n      [5,\"audio\"],\n      [\n        [\n          {\n            id:'audio-1-1',\n            title: \"A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network\",\n            author: \"sarahqwang\",\n            time: \"2018\",\n            content: \"The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8706582\"\n          },\n          {\n            id:'audio-1-2',\n            title: \"A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.\",\n            src: \"https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF\"\n          },\n          {\n            id:'audio-1-3',\n            title: \"A maximum likelihood approach to deep neural network based speech dereverberation\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8282019\"\n          },\n          {\n            id:'audio-1-4',\n            title: \"A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemalefemale, malemale, and femalemale mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/7917323\"\n          },\n          {\n            id:'audio-1-5',\n            title: \"Gaussian density guided deep neural network for single-channel speech enhancement\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8168116\"\n          }\n        ]\n      ]\n    ],\n    [\n      [5,\"network\"],\n      [\n        [\n          {\n            id:'network-1-1',\n            title: \"Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy\",\n            author: \"zeqilai\",\n            time: \"2018\",\n            content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in todays network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.\",\n            src: \"https://dl.acm.org/citation.cfm?id=3318101\"\n          },\n          {\n            id:'network-1-2',\n            title: \"Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.\",\n            src: \"https://ieeexplore.ieee.org/document/8117568\"\n          },\n          {\n            id:'network-1-3',\n            title: \"Furion: Engineering high-quality immersive virtual reality on today's mobile devices\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).\",\n            src: \"https://dl.acm.org/citation.cfm?id=3117815\"\n          },\n          {\n            id:'network-1-4',\n            title: \"Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.\",\n            src: \"https://ieeexplore.ieee.org/document/7898362\"\n          },\n          {\n            id:'network-1-5',\n            title: \"Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of users input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.\",\n            src: \"https://dl.acm.org/citation.cfm?id=3234201\"\n          },\n        ]\n      ]\n    ],\n    [\n      [1,\"quality\"],\n      [\n        [\n          {\n            id:'quality-1-1',\n            title: \"Blind image quality assessment based on joint log-contrast statistics\",\n            author: \"Yabin Zhang et at.\",\n            time: \"2019\",\n            content: \"During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.\",\n            src: \"https://www.sciencedirect.com/science/article/pii/S0925231218313432\"\n          }\n        ]\n      ]\n    ],\n    [\n      [8,\"video\"],\n      [\n        [\n          {\n            id:'video-1-3',\n            title: \"Wide Angular Intra Prediction for Versatile Video Coding\",\n            author: \"Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape\",\n            time: \"2019\",\n            content: \"This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).\",\n            src: \"https://ieeexplore.ieee.org/document/8712681\"\n          },\n          {\n            id:'video-1-4',\n            title: \"Multiple Reference Line Coding for Most Probable Modes in Intra Prediction\",\n            author: \"Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand\",\n            time: \"2019\",\n            content: \"This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.\",\n            src: \"https://ieeexplore.ieee.org/document/8712640\"\n          },\n          {\n            id:'video-1-5',\n            title: \"Fast Adaptive Multiple Transform for Versatile Video Coding\",\n            author: \"Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu\",\n            time: \"2019\",\n            content: \"The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.\",\n            src: \"https://ieeexplore.ieee.org/document/8712650\"\n          },\n          {\n            id:'video-2-1',\n            title: \"Recent advances in video coding beyond the HEVC standard\",\n            author: \"Xiaozhong Xu, Shan Liu\",\n            time: \"2019\",\n            content: \"The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.\",\n            src: \"https://doi.org/10.1017/ATSIP.2019.11\"\n          },\n          {\n            id:'video-2-2',\n            title: \"Current Picture Referencing in Versatile Video Coding\",\n            author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n            time: \"2019\",\n            content: \"Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.\",\n            src: \"https://ieeexplore.ieee.org/document/8695359\"\n          }\n        ],\n        [\n          {\n            id:'video-1-2',\n            title: \"Coupled Primary and Secondary Transform for Next Generation Video Coding\",\n            author: \"Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu\",\n            time: \"2018\",\n            content: \"The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.\",\n            src: \"https://ieeexplore.ieee.org/document/8698635\"\n          },\n          {\n            id:'video-1-1',\n            title: \"Intra Block Copy for Next Generation Video Coding\",\n            author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n            time: \"2018\",\n            content: \"Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.\",\n            src: \"https://ieeexplore.ieee.org/document/8551528\"\n          },\n          {\n            id:'video-2-3',\n            title: \"TPG Image Compression Technology\",\n            author: \"ShitaoWangPiaoDingXiaozhengHuangHanjunLiuBinjiLuoXinxingChenYoubaoWuRonggangWang\",\n            time: \"2017\",\n            content: \"TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression\",\n            src: \"http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238\"\n          }\n        ]\n      ]\n    ],\n    [\n      [2,\"computer\"],\n      [\n        [\n          {\n            id:'computer-1-2',\n            title: \"High Performance Gesture Recognition via Effective and Efficient Temporal Modeling\",\n            author: \"Yang Yi, Feng Li, et al.\",\n            time: \"2019\",\n            content: \"State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization\",\n            src: \"https://www.ijcai.org/proceedings/2019/0141.pdf\"\n          },\n          {\n            id:'computer-1-1',\n            title: \"Saliency detection with two-level fully convolutional networks\",\n            author: \"Yang Yi, et al.\",\n            time: \"2017\",\n            content: \"This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.\",\n            src: \"https://ieeexplore.ieee.org/document/8019309/\"\n          }\n        ]\n      ]\n    ],\n    [\n      [1,\"dianYun\"],\n      [\n        [\n          {\n            id:'dianYun-1-1',\n            title: \"A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.\",\n            author: \"Yabin Zhang, et al\",\n            time: \"2019\",\n            content: \"3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.\",\n            src: \"https://ieeexplore.ieee.org/document/8704253\"\n          } \n        ]\n      ]\n    ]\n  ]\n}\n\n\n\n\n\n\n\n\n\n\n","import homeLanguge from \"./pages/en/homePageLanguge\"\nimport researchLanguge from \"./pages/en/researchPageLanguge\"\nimport projectLanguge from \"./pages/en/projectPageLanguge\"\nimport solutionLanguge from \"./pages/en/solutionPageLanguge\"\nimport aboutLanguge from \"./pages/en/aboutPageLanguge\"\n\nexport default {\n    nav: {\n      home: 'Home', //\n      research:\"Research\",\n      project:\"Research Topics\",\n      solution:\"Solutions\",\n      about:\"About Us\",\n      join:\"join\"\n    },\n    homeLanguge,\n    researchLanguge,\n    projectLanguge,\n    solutionLanguge,\n    aboutLanguge\n  }\n  ","import '@babel/polyfill'\nimport Vue from 'vue';\nimport Vuex from 'vuex';\nimport App from './App.vue';\nimport router from './router';\nimport Store from './store';\nimport Layout from './components/Layout.vue';\n\nimport 'bootstrap/dist/css/bootstrap.min.css';\n// zh ,en\nimport zh from \"../public/languge/zh.js\";\nimport en from \"../public/languge/en.js\";\nimport VueI18n from 'vue-i18n';\nimport { handleClickEvent } from './util/util.js';\n\nimport Router from 'vue-router'\nconst routerPush = Router.prototype.push\nRouter.prototype.push = function push(location) {\n  return routerPush.call(this, location).catch(error=> error)\n}\n\nVue.use(Vuex);\nVue.config.productionTip = false;\n\nVue.component('Layout',Layout);\n\nrouter.beforeEach((to, from, next) => {\n  next()\n  const toRouter = [\n    { path: '/researchl', id: 87517, eventName: 'access_top_research' },\n    { path: '/project', id: 87516, eventName: 'access_top_project' },\n    { path: '/solution', id: 87515, eventName: 'access_top_solution' },\n    { path: '/research/VideoUnderstanding', id: 87514, eventName: 'access_research_understanding' },\n    { path: '/research/videoProcessing', id: 87513, eventName: 'access_research_process' },\n    { path: '/research/audioTransmission', id: 87511, eventName: 'access_research_audio_and_video' },\n    { path: '/projectTpg', id: 87506, eventName: 'access_project_TPG' },\n    { path: '/projectTse', id: 87505, eventName: 'access_project_TSE' },\n    { path: '/projectLiYing', id: 87504, eventName: 'access_project_liying' },\n    { path: '/projectAudioAndVideo', id: 87503, eventName: 'access_project_platform' },\n    { path: '/research/audioProcessing', id: 87502, eventName: 'access_research_audio' },\n    { path: '/research/international', id: 87501, eventName: 'access_research_international' },\n    { path: '/research/newMedia', id: 87500, eventName: 'access_research_immersive' },\n    { path: '/research/videoCode', id: 87499, eventName: 'access_research_codec' },\n    { path: '/research/perceptual', id: 87498, eventName: 'access_research_perceived' },\n    { path: '/research/assessment', id: 87497, eventName: 'access_research_assess1' },\n  ]\n  toRouter.forEach((item) => {\n    if(to.path === item.path) {\n      handleClickEvent(item.id, item.eventName)\n    }\n  })\n})\n\nVue.use(VueI18n); // \nconst i18n = new VueI18n({\n  locale: 'zh',    // \n  //this.$i18n.locale // locale\n  messages: {\n         zh,\n         en\n       }\n    });\n\n    router.afterEach(() => {\n      window.scrollTo(0,0)\n  });\n  const store = new Vuex.Store({...Store});\n\nnew Vue({\n  router,\n  i18n:i18n,\n  store:store,\n  render: h => h(App)\n}).$mount('#app')\n","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/index.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Layout.vue?vue&type=style&index=0&id=5cbf1b3b&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/index.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Layout.vue?vue&type=style&index=0&id=5cbf1b3b&scoped=true&lang=css&\""],"sourceRoot":""}