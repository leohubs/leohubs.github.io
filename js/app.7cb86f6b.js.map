{"version":3,"sources":["webpack:///webpack/bootstrap","webpack:///./src/App.vue?4241","webpack:///./src/App.vue?ef7a","webpack:///./src/App.vue?e250","webpack:///./src/util/util.js","webpack:///./src/App.vue?1160","webpack:///./src/App.vue?bff9","webpack:///./src/router.js","webpack:///./src/store/mutation-type.js","webpack:///./src/store/mutation.js","webpack:///./src/store/index.js","webpack:///./src/components/Layout.vue?e745","webpack:///src/components/Layout.vue","webpack:///./src/components/Layout.vue?448b","webpack:///./src/components/Layout.vue?aaa1","webpack:///./public/languge/pages/zh/homePageLanguge.js","webpack:///./public/languge/pages/zh/researchPageLanguge.js","webpack:///./public/languge/pages/zh/projectPageLanguge.js","webpack:///./public/languge/pages/zh/solutionPageLanguge.js","webpack:///./public/languge/pages/zh/aboutPageLanguge.js","webpack:///./public/languge/zh.js","webpack:///./public/languge/pages/en/homePageLanguge.js","webpack:///./public/languge/pages/en/researchPageLanguge.js","webpack:///./public/languge/pages/en/projectPageLanguge.js","webpack:///./public/languge/pages/en/solutionPageLanguge.js","webpack:///./public/languge/pages/en/aboutPageLanguge.js","webpack:///./public/languge/en.js","webpack:///./src/main.js","webpack:///./src/components/Layout.vue?0cd8"],"names":["webpackJsonpCallback","data","moduleId","chunkId","chunkIds","moreModules","executeModules","i","resolves","length","installedChunks","push","Object","prototype","hasOwnProperty","call","modules","parentJsonpFunction","shift","deferredModules","apply","checkDeferredModules","result","deferredModule","fulfilled","j","depId","splice","__webpack_require__","s","installedModules","installedCssChunks","jsonpScriptSrc","p","exports","module","l","e","promises","cssChunks","Promise","resolve","reject","href","fullhref","existingLinkTags","document","getElementsByTagName","tag","dataHref","getAttribute","rel","existingStyleTags","linkTag","createElement","type","onload","onerror","event","request","target","src","err","Error","code","parentNode","removeChild","head","appendChild","then","installedChunkData","promise","onScriptComplete","script","charset","timeout","nc","setAttribute","clearTimeout","chunk","errorType","realSrc","error","undefined","setTimeout","all","m","c","d","name","getter","o","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","oe","console","jsonpArray","window","oldJsonpFunction","slice","render","_vm","this","_h","$createElement","_c","_self","attrs","staticRenderFns","handleClickEvent","id","eventName","params","clickId","options","BeaconAction","onEvent","log","component","Vue","use","Router","routes","path","redirect","children","SET_NAVITEM_STATUS","SET_NAV_MASK","SET_BANNER_ISANDROID","SET_BANNER_ISIOS","SET_SCROLLINDECX","state","navItemstatus","items1","items2","items3","items4","isMask","isAndroid","isiOS","scrollIndex","mutations","STATE","index","item","STATAE","mutationState","ref","staticClass","style","background","outBg","height","innerHeight","width","iWidth","innerBg","_t","props","mounted","out","getElementById","inner","onresize","title","more","research","researchAndMade","text","touch","newMedia","videoUnderstanding","VideoCodingAndDecoding","videoHandle","audioHandle","connectionQuality","transfer","project","title1","title2","title3","title4","subTitle1","text1","subTitle2","text2","subTitle3","text3","subTitle4","text4","lookAll","solution","solutionTitle","join","join1","joining","navList","showList","childen","link","bgc","explain","tab1","list","subTitle","subContanier","sencTitle","sencText","tab2","tab3","business","businessList","projectName1","projectName2","explain1","list1","explain2","list2","projectName","projectText","textTPG","textTSE","textLiYing","pingtai","nav","back","next","tpg","partner","itemTitle1","itemText1","itemTitle2","itemText2","itemTitle3","itemText3Item1","itemText3Item2","itemTitle4","itemText4Item1","itemText4Item2","itemTitle5","itemText5","one","two","four","three","partners","tse","tesText","itemText1one","itemText1two","itemText2one","itemText2two","liYing","vidio","before","after","vidioAndaudioTest","pingtaiText","bannerTitle","members","sideNavBarTitle","team","expertList","imgClass","contactTitle","contactEmail","positionList","positionName","address","duty","content","jobRequirements","Total","thesisList","author","time","home","about","homeLanguge","researchLanguge","projectLanguge","solutionLanguge","aboutLanguge","routerPush","location","catch","Vuex","config","productionTip","Layout","router","beforeEach","to","from","toRouter","forEach","VueI18n","i18n","locale","messages","zh","en","afterEach","scrollTo","store","Store","h","App","$mount"],"mappings":"aACE,SAASA,EAAqBC,GAQ7B,IAPA,IAMIC,EAAUC,EANVC,EAAWH,EAAK,GAChBI,EAAcJ,EAAK,GACnBK,EAAiBL,EAAK,GAIHM,EAAI,EAAGC,EAAW,GACpCD,EAAIH,EAASK,OAAQF,IACzBJ,EAAUC,EAASG,GAChBG,EAAgBP,IAClBK,EAASG,KAAKD,EAAgBP,GAAS,IAExCO,EAAgBP,GAAW,EAE5B,IAAID,KAAYG,EACZO,OAAOC,UAAUC,eAAeC,KAAKV,EAAaH,KACpDc,EAAQd,GAAYG,EAAYH,IAG/Be,GAAqBA,EAAoBhB,GAE5C,MAAMO,EAASC,OACdD,EAASU,OAATV,GAOD,OAHAW,EAAgBR,KAAKS,MAAMD,EAAiBb,GAAkB,IAGvDe,IAER,SAASA,IAER,IADA,IAAIC,EACIf,EAAI,EAAGA,EAAIY,EAAgBV,OAAQF,IAAK,CAG/C,IAFA,IAAIgB,EAAiBJ,EAAgBZ,GACjCiB,GAAY,EACRC,EAAI,EAAGA,EAAIF,EAAed,OAAQgB,IAAK,CAC9C,IAAIC,EAAQH,EAAeE,GACG,IAA3Bf,EAAgBgB,KAAcF,GAAY,GAE3CA,IACFL,EAAgBQ,OAAOpB,IAAK,GAC5Be,EAASM,EAAoBA,EAAoBC,EAAIN,EAAe,KAGtE,OAAOD,EAIR,IAAIQ,EAAmB,GAGnBC,EAAqB,CACxB,IAAO,GAMJrB,EAAkB,CACrB,IAAO,GAGJS,EAAkB,GAGtB,SAASa,EAAe7B,GACvB,OAAOyB,EAAoBK,EAAI,OAAS,GAAG9B,IAAUA,GAAW,IAAM,CAAC,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,YAAYA,GAAW,MAI5rB,SAASyB,EAAoB1B,GAG5B,GAAG4B,EAAiB5B,GACnB,OAAO4B,EAAiB5B,GAAUgC,QAGnC,IAAIC,EAASL,EAAiB5B,GAAY,CACzCK,EAAGL,EACHkC,GAAG,EACHF,QAAS,IAUV,OANAlB,EAAQd,GAAUa,KAAKoB,EAAOD,QAASC,EAAQA,EAAOD,QAASN,GAG/DO,EAAOC,GAAI,EAGJD,EAAOD,QAKfN,EAAoBS,EAAI,SAAuBlC,GAC9C,IAAImC,EAAW,GAIXC,EAAY,CAAC,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,EAAE,iBAAiB,GAC9aR,EAAmB5B,GAAUmC,EAAS3B,KAAKoB,EAAmB5B,IACzB,IAAhC4B,EAAmB5B,IAAkBoC,EAAUpC,IACtDmC,EAAS3B,KAAKoB,EAAmB5B,GAAW,IAAIqC,QAAQ,SAASC,EAASC,GAIzE,IAHA,IAAIC,EAAO,QAAU,GAAGxC,IAAUA,GAAW,IAAM,CAAC,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,WAAW,iBAAiB,YAAYA,GAAW,OACpqByC,EAAWhB,EAAoBK,EAAIU,EACnCE,EAAmBC,SAASC,qBAAqB,QAC7CxC,EAAI,EAAGA,EAAIsC,EAAiBpC,OAAQF,IAAK,CAChD,IAAIyC,EAAMH,EAAiBtC,GACvB0C,EAAWD,EAAIE,aAAa,cAAgBF,EAAIE,aAAa,QACjE,GAAe,eAAZF,EAAIG,MAAyBF,IAAaN,GAAQM,IAAaL,GAAW,OAAOH,IAErF,IAAIW,EAAoBN,SAASC,qBAAqB,SACtD,IAAQxC,EAAI,EAAGA,EAAI6C,EAAkB3C,OAAQF,IAAK,CAC7CyC,EAAMI,EAAkB7C,GACxB0C,EAAWD,EAAIE,aAAa,aAChC,GAAGD,IAAaN,GAAQM,IAAaL,EAAU,OAAOH,IAEvD,IAAIY,EAAUP,SAASQ,cAAc,QACrCD,EAAQF,IAAM,aACdE,EAAQE,KAAO,WACfF,EAAQG,OAASf,EACjBY,EAAQI,QAAU,SAASC,GAC1B,IAAIC,EAAUD,GAASA,EAAME,QAAUF,EAAME,OAAOC,KAAOjB,EACvDkB,EAAM,IAAIC,MAAM,qBAAuB5D,EAAU,cAAgBwD,EAAU,KAC/EG,EAAIE,KAAO,wBACXF,EAAIH,QAAUA,SACP5B,EAAmB5B,GAC1BkD,EAAQY,WAAWC,YAAYb,GAC/BX,EAAOoB,IAERT,EAAQV,KAAOC,EAEf,IAAIuB,EAAOrB,SAASC,qBAAqB,QAAQ,GACjDoB,EAAKC,YAAYf,KACfgB,KAAK,WACPtC,EAAmB5B,GAAW,KAMhC,IAAImE,EAAqB5D,EAAgBP,GACzC,GAA0B,IAAvBmE,EAGF,GAAGA,EACFhC,EAAS3B,KAAK2D,EAAmB,QAC3B,CAEN,IAAIC,EAAU,IAAI/B,QAAQ,SAASC,EAASC,GAC3C4B,EAAqB5D,EAAgBP,GAAW,CAACsC,EAASC,KAE3DJ,EAAS3B,KAAK2D,EAAmB,GAAKC,GAGtC,IACIC,EADAC,EAAS3B,SAASQ,cAAc,UAGpCmB,EAAOC,QAAU,QACjBD,EAAOE,QAAU,IACb/C,EAAoBgD,IACvBH,EAAOI,aAAa,QAASjD,EAAoBgD,IAElDH,EAAOZ,IAAM7B,EAAe7B,GAE5BqE,EAAmB,SAAUd,GAE5Be,EAAOhB,QAAUgB,EAAOjB,OAAS,KACjCsB,aAAaH,GACb,IAAII,EAAQrE,EAAgBP,GAC5B,GAAa,IAAV4E,EAAa,CACf,GAAGA,EAAO,CACT,IAAIC,EAAYtB,IAAyB,SAAfA,EAAMH,KAAkB,UAAYG,EAAMH,MAChE0B,EAAUvB,GAASA,EAAME,QAAUF,EAAME,OAAOC,IAChDqB,EAAQ,IAAInB,MAAM,iBAAmB5D,EAAU,cAAgB6E,EAAY,KAAOC,EAAU,KAChGC,EAAM3B,KAAOyB,EACbE,EAAMvB,QAAUsB,EAChBF,EAAM,GAAGG,GAEVxE,EAAgBP,QAAWgF,IAG7B,IAAIR,EAAUS,WAAW,WACxBZ,EAAiB,CAAEjB,KAAM,UAAWK,OAAQa,KAC1C,MACHA,EAAOhB,QAAUgB,EAAOjB,OAASgB,EACjC1B,SAASqB,KAAKC,YAAYK,GAG5B,OAAOjC,QAAQ6C,IAAI/C,IAIpBV,EAAoB0D,EAAItE,EAGxBY,EAAoB2D,EAAIzD,EAGxBF,EAAoB4D,EAAI,SAAStD,EAASuD,EAAMC,GAC3C9D,EAAoB+D,EAAEzD,EAASuD,IAClC7E,OAAOgF,eAAe1D,EAASuD,EAAM,CAAEI,YAAY,EAAMC,IAAKJ,KAKhE9D,EAAoBmE,EAAI,SAAS7D,GACX,qBAAX8D,QAA0BA,OAAOC,aAC1CrF,OAAOgF,eAAe1D,EAAS8D,OAAOC,YAAa,CAAEC,MAAO,WAE7DtF,OAAOgF,eAAe1D,EAAS,aAAc,CAAEgE,OAAO,KAQvDtE,EAAoBuE,EAAI,SAASD,EAAOE,GAEvC,GADU,EAAPA,IAAUF,EAAQtE,EAAoBsE,IAC/B,EAAPE,EAAU,OAAOF,EACpB,GAAW,EAAPE,GAA8B,kBAAVF,GAAsBA,GAASA,EAAMG,WAAY,OAAOH,EAChF,IAAII,EAAK1F,OAAO2F,OAAO,MAGvB,GAFA3E,EAAoBmE,EAAEO,GACtB1F,OAAOgF,eAAeU,EAAI,UAAW,CAAET,YAAY,EAAMK,MAAOA,IACtD,EAAPE,GAA4B,iBAATF,EAAmB,IAAI,IAAIM,KAAON,EAAOtE,EAAoB4D,EAAEc,EAAIE,EAAK,SAASA,GAAO,OAAON,EAAMM,IAAQC,KAAK,KAAMD,IAC9I,OAAOF,GAIR1E,EAAoB8E,EAAI,SAASvE,GAChC,IAAIuD,EAASvD,GAAUA,EAAOkE,WAC7B,WAAwB,OAAOlE,EAAO,YACtC,WAA8B,OAAOA,GAEtC,OADAP,EAAoB4D,EAAEE,EAAQ,IAAKA,GAC5BA,GAIR9D,EAAoB+D,EAAI,SAASgB,EAAQC,GAAY,OAAOhG,OAAOC,UAAUC,eAAeC,KAAK4F,EAAQC,IAGzGhF,EAAoBK,EAAI,GAGxBL,EAAoBiF,GAAK,SAAS/C,GAA2B,MAApBgD,QAAQ5B,MAAMpB,GAAYA,GAEnE,IAAIiD,EAAaC,OAAO,gBAAkBA,OAAO,iBAAmB,GAChEC,EAAmBF,EAAWpG,KAAK8F,KAAKM,GAC5CA,EAAWpG,KAAOX,EAClB+G,EAAaA,EAAWG,QACxB,IAAI,IAAI3G,EAAI,EAAGA,EAAIwG,EAAWtG,OAAQF,IAAKP,EAAqB+G,EAAWxG,IAC3E,IAAIU,EAAsBgG,EAI1B9F,EAAgBR,KAAK,CAAC,EAAE,kBAEjBU,K,6ECtQT,yBAAqb,EAAG,G,oCCAxb,IAAI8F,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,MAAM,CAAC,GAAK,QAAQ,CAACF,EAAG,gBAAgB,IAC9IG,EAAkB,GCDtB,iE,oCCAA,oCAAMC,EAAmB,SAACC,EAAIC,EAAWC,GACrC,IAAMC,EAAUH,GAAM,GAChBI,EAAUF,GAAU,GAC1BG,aAAaC,QAAQH,EAASF,EAAWG,GACzCnB,QAAQsB,IAAR,cAAmBJ,EAAnB,wBAA0CF,M,2DCJ9C,yBAA8T,eAAG,G,oCCAjU,oDAQIO,EAAY,eACd,aACA,OACA,QACA,EACA,KACA,KACA,MAIa,aAAAA,E,4iDChBfC,OAAIC,IAAIC,QAGO,I,EAAA,MAAIA,OAAO,CACxBC,OAAQ,CACN,CACEC,KAAM,IACNC,SAAU,CACRlD,KAAM,UAGV,CACEiD,KAAM,SACNjD,KAAM,QACN4C,UAAW,kBAAM,uFAEnB,CACEK,KAAM,YACNL,UAAW,kBAAM,sFACjBO,SAAU,CACR,CACEF,KAAM,gBACNjD,KAAM,gBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,WACNjD,KAAM,WACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,YACNjD,KAAM,YACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,qBACNjD,KAAM,qBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,kBACNjD,KAAM,kBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,aACNjD,KAAM,aACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,kBACNjD,KAAM,kBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,oBACNjD,KAAM,oBACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,aACNjD,KAAM,aACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,GACNC,SAAU,mBAIhB,CACED,KAAM,WACNjD,KAAM,UACN4C,UAAW,kBAAM,uFAEnB,CACEK,KAAK,cACLjD,KAAK,aACL4C,UAAW,kBAAK,uFAElB,CACEK,KAAK,cACLjD,KAAK,aACL4C,UAAW,kBAAK,uFAElB,CACEK,KAAK,iBACLjD,KAAK,gBACL4C,UAAW,kBAAK,uFAElB,CACEK,KAAK,wBACLjD,KAAK,uBACL4C,UAAW,kBAAK,uFAElB,CACEK,KAAM,WACNL,UAAW,kBAAM,sFACjBO,SAAS,CACP,CACEF,KAAM,aACNjD,KAAM,aACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,SACNjD,KAAM,SACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,SACNjD,KAAM,SACN4C,UAAW,kBAAM,kDAEnB,CACEK,KAAM,GACNC,SAAU,gBAIhB,CACED,KAAM,YACNjD,KAAM,WACN4C,UAAW,kBAAM,0FC/HR,GACXQ,mBAAoB,qBACpBC,aAAc,eACdC,qBAAsB,uBACtBC,iBAAkB,mBAClBC,iBAAkB,oB,wHCHtB,IAAMC,EAAQ,CACVC,cAAe,CACXC,QAAQ,EACRC,QAAQ,EACRC,QAAQ,EACRC,QAAQ,GAEZC,QAAQ,EACRC,WAAW,EACXC,OAAO,EACPC,YAAa,GAGXC,GAAS,SACVrG,EAAKsF,mBADK,SACegB,EAAO5J,GAC7B4J,EAAMV,cAAN,eAA4BlJ,EAAK6J,QAAW7J,EAAK8J,OAF1C,IAIVxG,EAAKuF,aAJK,SAISkB,EAAQ/J,GACxB+J,EAAOR,OAASvJ,IALT,IAOVsD,EAAKwF,qBAPK,SAOiBc,EAAO5J,GAC/B4J,EAAMJ,UAAYxJ,IARX,IAUVsD,EAAKyF,iBAVK,SAUaa,EAAO5J,GAC3B4J,EAAMH,MAAQzJ,IAXP,IAaVsD,EAAK0F,iBAbK,SAaaY,EAAO5J,GAC3B4J,EAAMF,YAAc1J,IAdb,GAiBA,GACXiJ,QACAU,a,4qBChC8BK,GAA1Bf,E,EAAAA,MAAOU,E,EAAAA,UAEA,GACXA,YACAV,SCNA,EAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACA,EAAG,MAAM,CAAC0C,IAAI,MAAMC,YAAY,MAAMC,MAAM,CAAEC,WAAWjD,EAAIkD,MAAMC,OAAOnD,EAAIoD,YAAY,MAAO9C,MAAM,CAAC,GAAK,QAAQ,CAACF,EAAG,MAAM,CAAC0C,IAAI,QAAQE,MAAM,CAAEK,MAAQrD,EAAIsD,OAAO,KAAKH,OAAOnD,EAAIoD,YAAY,KAAKH,WAAWjD,EAAIuD,SAAUjD,MAAM,CAAC,GAAK,UAAU,CAACN,EAAIwD,GAAG,YAAY,QACjXjD,EAAkB,GCUtB,GACE1H,KADF,WAEI,MAAJ,IAGE4K,MAAF,CAEI,UAAJ,CACM,KAAN,OACM,QAAN,IAEI,OAAJ,CACM,KAAN,OACM,QAAN,IAEI,YAAJ,CACM,KAAN,OACM,QAAN,IAEI,MAAJ,CACM,KAAN,OACM,QAAN,IAEI,QAAJ,CACM,KAAN,OACM,QAAN,KAGEC,QA5BF,WA6BI,IAAIC,EAAMjI,SAASkI,eAAe,OAC9BC,EAAQnI,SAASkI,eAAe,SACpChE,OAAOkE,SAAW,WAChB,EAAN,gDACM,EAAN,kDAGI,EAAJ,gDACI,EAAJ,mDChDgV,I,wBCQ5U7C,EAAY,eACd,EACA,EACAV,GACA,EACA,KACA,WACA,MAIa,EAAAU,E,QCnBA,G,UAAA,CACb8C,MAAO,mBACPC,KAAM,OAENC,SAAS,CACHF,MAAO,OACPG,gBAAgB,CACdH,MAAM,YACNI,KAAK,4BAEPC,MAAM,CACJL,MAAM,OACNI,KAAK,8BAEPE,SAAS,CACRN,MAAM,SACNI,KAAK,4BAENG,mBAAmB,CACjBP,MAAM,OACNI,KAAK,sBAENI,uBAAuB,CACrBR,MAAM,QACNI,KAAK,qBAEPK,YAAY,CACVT,MAAM,OACNI,KAAK,0BAEPM,YAAY,CACVV,MAAM,OACNI,KAAK,2BAEPO,kBAAkB,CAChBX,MAAM,YACNI,KAAK,wBAEPQ,SAAS,CACPZ,MAAM,UACNI,KAAK,2BAIdS,QAAQ,CACNC,OAAO,MACPC,OAAO,MACPC,OAAO,OACPC,OAAO,YACPC,UAAU,8BACVC,MAAM,2BACNC,UAAU,+BACVC,MAAM,mBACNC,UAAU,OACVC,MAAM,qCACNC,UAAU,YACVC,MAAM,iCACNC,QAAQ,OACRzB,KAAM,QAGR0B,SAAS,CACPC,cAAe,OACfd,OAAO,UACPK,MAAM,uDACNJ,OAAO,WACPM,MAAM,oEACNL,OAAO,aACPO,MAAM,iEACNtB,KAAK,OACLyB,QAAQ,OACRG,KAAK,aACLC,MAAM,WACNC,QAAQ,QAGVC,QAAQ,CACN,CACE1H,KAAM,MAER,CACEA,KAAM,OACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,+BAAgCD,GAAI,QAASpC,KAAM,YAAa6H,KAAK,0BAA2BxD,MAAO,GACpH,CAAEhC,UAAW,iCAAkCD,GAAI,QAAQpC,KAAM,UAAW6H,KAAK,8BAA+BxD,MAAO,GACvH,CAAEhC,UAAW,2BAA4BD,GAAI,QAAQpC,KAAM,WAAY6H,KAAK,qBAAsBxD,MAAO,GACzG,CAAEhC,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,QAAS6H,KAAK,sBAAuBxD,MAAO,GACnG,CAAEhC,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,4BAA6BxD,MAAO,GACxG,CAAEhC,UAAW,yBAA0BD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,4BAA6BxD,MAAO,GAC1G,CAAEhC,UAAW,2BAA4BD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,uBAAwBxD,MAAO,GACvG,CAAEhC,UAAW,wBAAyBD,GAAI,QAAQpC,KAAM,YAAa6H,KAAK,uBAAwBxD,MAAO,GACzG,CAAEhC,UAAW,+BAAgCD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,+BAAgCxD,MAAO,KAGvH,CACErE,KAAM,OACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,YACjE,CAAExF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,MAAO6H,KAAK,eAChE,CAAExF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,MAAO6H,KAAK,eAChE,CAAExF,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,kBACpE,CAAExF,UAAW,yBAA0BD,GAAI,QAAQpC,KAAM,YAAa6H,KAAK,2BAG/E,CACE7H,KAAM,OACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,qBAAsBD,GAAI,QAAQpC,KAAM,SAAU8H,KAAK,EAAOD,KAAK,aAChF,CAAExF,UAAW,sBAAuBD,GAAI,QAAQpC,KAAM,UAAW8H,KAAK,EAAMD,KAAK,2CACjF,CAAExF,UAAW,sBAAuBD,GAAI,QAAQpC,KAAM,OAAQ8H,KAAK,EAAMD,KAAK,2CAC9E,CAAExF,UAAW,qBAAsBD,GAAI,QAAQpC,KAAM,aAAc8H,KAAK,EAAMD,KAAK,4CAGvF,CACE7H,KAAM,OACN2H,UAAU,EACVC,QAAS,CACP,CAAE5H,KAAM,QAAS6H,KAAK,uBAEtB,CAAE7H,KAAM,OAAQ6H,KAAK,wBC1Hb,GAEXnC,MAAM,OAQPG,gBAAgB,CACZH,MAAM,YACNc,OAAO,UACPV,KAAK,mEACLiC,QAAQ,6BAIRC,KAAK,CACDtC,MAAM,OACNuC,KAAK,CACD,CACIC,SAAS,MACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,8EACA,kFACA,2DAIZ,CACIH,SAAS,aACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,uDACA,qEACA,gDAIZ,CACIH,SAAS,QACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,uCACA,wEACA,8EACA,mEAIZ,CACIH,SAAS,WACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,kCACA,gGACA,+HACA,4CAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,6BACA,uEACA,2EACA,+CAQpBC,KAAK,CACD5C,MAAM,QACNuC,KAAK,CACD,CACIC,SAAS,MACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,6BACA,0CAIZ,CACIH,SAAS,OACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,sDACA,+BACA,2EAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,uBACA,qJAIZ,CACIH,SAAS,eACTC,aAAc,CACV,CAACC,UAAU,OAAOC,SAAS,CACvB,gEACA,qJACA,4EACA,2CAEJ,CAACD,UAAU,YAAYC,SAAS,CAC5B,iLACA,wDACA,2HACA,iEAEJ,CAACD,UAAU,OAAOC,SAAS,CACvB,8EACA,mHACA,6EAEJ,CAACD,UAAU,YAAYC,SAAS,CAC5B,+GACA,2DACA,kCACA,6DAOpBE,KAAK,CACD7C,MAAM,UACNuC,KAAK,CACD,CACIC,SAAS,QACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,6BACA,wCACA,oDAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,8BACA,mIACA,6FACA,oDAIZ,CACIH,SAAS,iBACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,4FACA,uFAUxBrC,SAAS,CACJN,MAAM,SACNI,KAAK,+HACLiC,QAAQ,MACRE,KAAK,CACD,CACGC,SAAS,KACTC,aAAa,2EAEhB,CACGD,SAAS,KACTC,aAAa,qFAEhB,CACGD,SAAS,KACTC,aAAa,0EAEjB,CACID,SAAS,KACTC,aAAa,oFAEjB,CACID,SAAS,KACTC,aAAa,2EAIpBK,SAAS,OACTC,aAAa,CACT,CACGP,SAAS,UACTC,aAAa,8CAEhB,CACGD,SAAS,UACTC,aAAa,+CAQzBjC,uBAAuB,CAClBR,MAAM,QACNI,KAAK,qHACLiC,QAAS,MACTE,KAAK,CACD,CACGC,SAAS,UACTC,aAAa,iFAEhB,CACGD,SAAS,UACTC,aAAa,gFAEjB,CACID,SAAS,YACTC,aAAa,+DAQpB5B,QAAQ,KACRmC,aAAa,MACbC,aAAa,OAMlB1C,mBAAmB,CACbP,MAAM,OACNI,KAAK,kFACLiC,QAAS,MACTE,KAAK,CACD,CACEC,SAAS,OACTC,aAAa,gFAEf,CACED,SAAS,OACTC,aAAa,4EAEjB,CACID,SAAS,OACTC,aAAa,kEACf,CACED,SAAS,QACTC,aAAa,+EAQzBhC,YAAY,CACRT,MAAM,OACNI,KAAK,mGAGL8C,SAAS,OACTC,MAAM,CACF,CACIX,SAAS,OACTC,aAAa,2EAEjB,CACID,SAAS,QACTC,aAAa,gFAEjB,CACID,SAAS,OACTC,aAAa,+EAGjB,CACID,SAAS,UACTC,aAAa,uFAEjB,CACID,SAAS,KACTC,aAAa,0EAGrBW,SAAS,OACTC,MAAM,CACF,CACIb,SAAS,SACTC,aAAa,8EAEjB,CACID,SAAS,OACTC,aAAa,2EAEjB,CACID,SAAS,OACTC,aAAa,0EAEjB,CACID,SAAS,QACTC,aAAa,8EAEjB,CACID,SAAS,QACTC,aAAa,yEAEjB,CACID,SAAS,KACTC,aAAa,+EAEjB,CACID,SAAS,KACTC,aAAa,2EAEjB,CACID,SAAS,KACTC,aAAa,kFAQzBpC,MAAM,CACFL,MAAM,OACNI,KAAK,iHACLiC,QAAS,MACTE,KAAK,CACD,CACIC,SAAS,OACTC,aAAa,wEAEjB,CACID,SAAS,iBACTC,aAAa,kFAEjB,CACID,SAAS,SACTC,aAAa,yCAGrBK,SAAS,OACRC,aAAa,CACT,CACGP,SAAS,MACTC,aAAa,mHAMzB/B,YAAY,CACNV,MAAM,OACNI,KAAK,uEACLiC,QAAQ,MACRE,KAAK,CACD,CACEC,SAAS,OACTC,aAAa,4EAEf,CACED,SAAS,OACTC,aAAa,mFAEjB,CACID,SAAS,OACTC,aAAa,4EAEjB,CACID,SAAS,OACTC,aAAa,iEAEjB,CACID,SAAS,gBACTC,aAAa,sGAEjB,CACID,SAAS,YACTC,aAAa,2EAEjB,CACID,SAAS,SACTC,aAAa,6EAEjB,CACID,SAAS,SACTC,aAAa,0EAEjB,CACID,SAAS,OACTC,aAAa,iFAEjB,CACID,SAAS,OACTC,aAAa,uHAQzB7B,SAAS,CACHZ,MAAM,UACNI,KAAK,mJACLiC,QAAQ,MACRE,KAAK,CACD,CACEC,SAAS,OACTC,aAAa,8EAEf,CACED,SAAS,OACTC,aAAa,kFAEjB,CACID,SAAS,aACTC,aAAa,yEAEjB,CACID,SAAS,SACTC,aAAa,uFAEjB,CACID,SAAS,OACTC,aAAa,gFAOzB9B,kBAAkB,CACdX,MAAM,YACNI,KAAK,4JACLiC,QAAQ,MACRE,KAAK,CACD,CACIC,SAAS,UACTC,aAAa,iFAEjB,CACID,SAAS,aACTC,aAAa,0GAEjB,CACID,SAAS,YACTC,aAAa,8FAGrB5B,QAAQ,KACRyC,YAAY,YACZC,YAAY,iCACZtD,KAAM,SC9dC,GAEXD,MAAM,OACNwD,QAAQ,uDACRC,QAAQ,mBACRC,WAAW,iDACXC,QAAQ,wCACR1D,KAAK,OAEL2D,IAAI,CACAC,KAAK,UACLC,KAAK,YAKTC,IAAI,CACAP,QAAQ,mJACRQ,QAAQ,OACRC,WAAW,YACXC,UAAU,0DACVC,WAAW,YACXC,UAAU,oDACVC,WAAW,YACXC,eAAe,2CACfC,eAAe,qDACfC,WAAW,YACXC,eAAe,iDACfC,eAAe,qDACfC,WAAW,UACXC,UAAU,CACNzD,MAAM,CACF0D,IAAI,QACJC,IAAI,SAEJC,KAAK,SAGT1D,MAAM,CACFwD,IAAI,QACJC,IAAI,SACJE,MAAM,UACND,KAAK,SAETxD,MAAM,CACFsD,IAAI,QACJC,IAAI,QACJE,MAAM,aAGdC,SAAS,WAIbC,IAAI,CACDC,QAAQ,sJACRlB,WAAW,iBACXmB,aAAa,8CACbC,aAAa,kEACblB,WAAW,iBACXmB,aAAa,0CACbC,aAAa,gGACbvB,QAAQ,WAIXwB,OAAO,CACJxF,MAAM,OACN0D,WAAW,uDACX+B,MAAM,SACNxB,WAAW,iBACXE,WAAW,kBACXE,WAAW,gBACXG,WAAW,kBACXR,QAAQ,WACR0B,OAAO,KACPC,MAAM,OAKTC,kBAAkB,CACd5F,MAAM,YAEN6F,YAAY,yFACZ3B,UAAU,yJACVE,UAAU,sKCtFH,GAEXpE,MAAM,OACNuC,KAAK,CACD,CACIvC,MAAM,gBACNI,KAAK,uDAET,CACIJ,MAAM,iBACNI,KAAK,2DAET,CACIJ,MAAM,kBACNI,KAAK,mECdF,GACX0F,YAAY,OACZ9F,MAAM,WACNI,KAAK,uHACL2F,QAAQ,GACRC,gBAAgB,CAAC,QAAQ,OAAO,QAGhCC,KAAM,OACNC,WAAW,CACP,CACExJ,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,QACL8F,KAAK,+jBAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,QACL8F,KAAK,0QAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAM,QACN8F,KAAM,2ZAER,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,oBACL8F,KAAK,gRAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,kBACL8F,KAAK,qfAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,SACL8F,KAAK,4RAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,QACL8F,KAAK,yJAMXgG,aAAc,kBACdC,aAAc,uBACdC,aAAc,CACZ,CACE5J,GAAI,EACJ6J,aAAc,cACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,yBACA,iCAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,6BACA,wBACA,yCACA,uDAIN,CACEhK,GAAI,EACJ6J,aAAc,YACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,+CACA,oCACA,8BACA,4BAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,mCACA,qBACA,qBACA,gHACA,6CACA,oCACA,iCAIN,CACEhK,GAAI,EACJ6J,aAAc,YACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,8BAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,4CACA,iHACA,mCACA,0CAIN,CACEhK,GAAI,EACJ6J,aAAc,UACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,kEAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,8CACA,kCACA,oEACA,6BACA,iCACA,kHACA,4BAIN,CACEhK,GAAI,EACJ6J,aAAc,cACdC,QAAS,UACTC,KAAM,CACJzG,MAAO,QACP0G,QAAS,CACP,gJAGJC,gBAAiB,CACf3G,MAAO,QACP0G,QAAS,CACP,iCACA,qCACA,8CACA,yBACA,oBACA,wBACA,oCACA,kCAQR1E,QAAS,CAAC,KAAK,OAAO,SAAS,SAAS,QAAQ,QAAQ,QACxD4E,MAAO,KACPhI,KAAM,KACNiI,WAAW,CACT,CACE,CAAC,GAAG,OACJ,CACE,CACE,CACEnK,GAAG,UACHsD,MAAO,6EACP8G,OAAQ,mKACRC,KAAM,OACNL,QAAS,k2BACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2DACP8G,OAAQ,uGACRC,KAAM,OACNL,QAAS,kjCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,wEACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,i1CACThO,IAAK,uEAEP,CACEgE,GAAG,UACHsD,MAAO,8DACP8G,OAAQ,sDACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2DACP8G,OAAQ,yBACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,0CAIT,CACE,CACEgE,GAAG,UACHsD,MAAO,wDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0vBACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,qFACP8G,OAAQ,2BACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,mDAEP,CACEgE,GAAG,UACHsD,MAAO,iGACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,yuCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,8FACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,27BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,wGACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,45CACThO,IAAK,+CAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,oDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0iCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2EACP8G,OAAQ,8CACRC,KAAM,OACNL,QAAS,88CACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,i4BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,k6BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,6HACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,ghCACThO,IAAK,uEAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,qHACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,0iDACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,4FACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,qzCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,uFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,+0CACThO,IAAK,8CAEP,CACEgE,GAAG,UACHsD,MAAO,mCACP8G,OAAQ,0FACRC,KAAM,GACNL,QAAS,mdACThO,IAAK,6EAEP,CACEgE,GAAG,UACHsD,MAAO,iEACP8G,OAAQ,kBACRC,KAAM,OACNL,QAAS,g/BACThO,IAAK,kDAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,2HACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,4hCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,gFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,iDAKb,CACE,CAAC,EAAE,SACH,CACE,CACE,CACEgE,GAAG,YACHsD,MAAO,8FACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,27BACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,6HACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,ghCACThO,IAAK,sEAEP,CACEgE,GAAG,YACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,k6BACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,qHACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,0iDACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,i4BACThO,IAAK,4DAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,wGACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,45CACThO,IAAK,8CAEP,CACEgE,GAAG,cACHsD,MAAO,4FACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,qzCACThO,IAAK,gDAEP,CACEgE,GAAG,cACHsD,MAAO,uFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,+0CACThO,IAAK,8CAEP,CACEgE,GAAG,cACHsD,MAAO,2HACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,4hCACThO,IAAK,gDAEP,CACEgE,GAAG,cACHsD,MAAO,gFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,iDAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,wEACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,i1CACThO,IAAK,0EAKb,CACE,CAAC,EAAE,SACH,CACE,CACE,CACEgE,GAAG,YACHsD,MAAO,2DACP8G,OAAQ,uGACRC,KAAM,OACNL,QAAS,kjCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,6EACP8G,OAAQ,mKACRC,KAAM,OACNL,QAAS,k2BACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,8DACP8G,OAAQ,sDACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,2DACP8G,OAAQ,yBACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,yCAEP,CACEgE,GAAG,YACHsD,MAAO,wDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0vBACThO,IAAK,iDAGT,CACE,CACEgE,GAAG,YACHsD,MAAO,2EACP8G,OAAQ,8CACRC,KAAM,OACNL,QAAS,88CACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,oDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0iCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,mCACP8G,OAAQ,0FACRC,KAAM,OACNL,QAAS,mdACThO,IAAK,gFAKb,CACE,CAAC,EAAE,YACH,CACE,CACE,CACEgE,GAAG,eACHsD,MAAO,qFACP8G,OAAQ,2BACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,mDAEP,CACEgE,GAAG,eACHsD,MAAO,iEACP8G,OAAQ,kBACRC,KAAM,OACNL,QAAS,g/BACThO,IAAK,oDAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,iGACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,yuCACThO,IAAK,qDC/jBJ,GAEXkL,IAAK,CACHoD,KAAM,KACN9G,SAAS,OACTW,QAAQ,OACRc,SAAS,OACTsF,MAAM,OACNjH,MAAM,YAIRkH,cACAC,kBACAC,iBACAC,kBACAC,gBC7BW,GACXtH,MAAO,kDACPC,KAAM,aAENC,SAAS,CACHF,MAAO,WACPG,gBAAgB,CACdH,MAAM,gCACNI,KAAK,qGAEPC,MAAM,CACJL,MAAM,yBACNI,KAAK,wGAEPE,SAAS,CACRN,MAAM,kBACNI,KAAK,6GAENG,mBAAmB,CACjBP,MAAM,oBACNI,KAAK,yFAENI,uBAAuB,CACrBR,MAAM,eACNI,KAAK,0DAEPK,YAAY,CACVT,MAAM,mBACNI,KAAK,wGAEPM,YAAY,CACVV,MAAM,mBACNI,KAAK,6EAEPO,kBAAkB,CAChBX,MAAM,gBACNI,KAAK,4DAEPQ,SAAS,CACPZ,MAAM,uBACNI,KAAK,qHAIdS,QAAQ,CACNC,OAAO,MACPC,OAAO,MACPC,OAAO,UACPC,OAAO,QACPC,UAAU,8BACVC,MAAM,iDACNC,UAAU,+BACVC,MAAM,6CACNC,UAAU,UACVC,MAAM,sGACNC,UAAU,4CACVC,MAAM,2KACNC,QAAQ,UACRzB,KAAM,iBAGR0B,SAAS,CACPC,cAAe,YACfd,OAAO,wCACPK,MAAM,mJACNJ,OAAO,sCACPM,MAAM,yNACNL,OAAO,gCACPO,MAAM,mKACNtB,KAAK,gBACLyB,QAAQ,UACRG,KAAK,sCACLC,MAAM,sCACNC,QAAQ,gBAGZC,QAAQ,CACN,CACE1H,KAAM,QAER,CACEA,KAAM,WACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,+BAAgCD,GAAI,QAAQpC,KAAM,gCAAiC6H,KAAK,0BAA2BxD,MAAO,GACvI,CAAEhC,UAAW,iCAAkCD,GAAI,QAAQpC,KAAM,uBAAwB6H,KAAK,8BAA+BxD,MAAO,GACpI,CAAEhC,UAAW,2BAA4BD,GAAI,QAAQpC,KAAM,kBAAmB6H,KAAK,qBAAsBxD,MAAO,GAChH,CAAEhC,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,eAAgB6H,KAAK,sBAAuBxD,MAAO,GAC1G,CAAEhC,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,mBAAoB6H,KAAK,4BAA6BxD,MAAO,GACpH,CAAEhC,UAAW,yBAA0BD,GAAI,QAAQpC,KAAM,mBAAoB6H,KAAK,4BAA6BxD,MAAO,GACtH,CAAEhC,UAAW,2BAA4BD,GAAI,QAAQpC,KAAM,yBAA0B6H,KAAK,uBAAwBxD,MAAO,GACzH,CAAEhC,UAAW,wBAAyBD,GAAI,QAAQpC,KAAM,gBAAiB6H,KAAK,uBAAwBxD,MAAO,GAC7G,CAAEhC,UAAW,+BAAgCD,GAAI,QAAQpC,KAAM,oBAAqB6H,KAAK,+BAAgCxD,MAAO,KAGpI,CACErE,KAAM,kBACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,kBAAmB6H,KAAK,YAC5E,CAAExF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,MAAO6H,KAAK,eAChE,CAAExF,UAAW,oBAAqBD,GAAI,QAAQpC,KAAM,MAAO6H,KAAK,eAChE,CAAExF,UAAW,uBAAwBD,GAAI,QAAQpC,KAAM,UAAW6H,KAAK,kBACvE,CAAExF,UAAW,yBAA0BD,GAAI,QAAQpC,KAAM,OAAQ6H,KAAK,2BAG1E,CACE7H,KAAM,YACN2H,UAAU,EACVC,QAAS,CACP,CAAEvF,UAAW,qBAAsBD,GAAI,QAAQpC,KAAM,YAAa8H,KAAK,EAAOD,KAAK,aACnF,CAAExF,UAAW,sBAAuBD,GAAI,QAAQpC,KAAM,OAAQ8H,KAAK,EAAMD,KAAK,2CAC9E,CAAExF,UAAW,sBAAuBD,GAAI,QAAQpC,KAAM,OAAQ8H,KAAK,EAAMD,KAAK,2CAC9E,CAAExF,UAAW,qBAAsBD,GAAI,QAAQpC,KAAM,MAAO8H,KAAK,EAAMD,KAAK,4CAGhF,CACE7H,KAAM,WACN2H,UAAU,EACVC,QAAS,CACP,CAAE5H,KAAM,oBAAqB6H,KAAK,uBAElC,CAAE7H,KAAM,UAAW6H,KAAK,uBC1HjB,GAEVnC,MAAM,WAQPG,gBAAgB,CACZH,MAAM,gCACNI,KAAK,uOACLiC,QAAQ,6BAIRC,KAAK,CACDtC,MAAM,yBACNuC,KAAK,CACD,CACIC,SAAS,MACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,yNACA,4MACA,mKAIZ,CACIH,SAAS,aACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,iHACA,iMACA,mFAIZ,CACIH,SAAS,QACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,+OACA,yTACA,4OACA,iFAIZ,CACIH,SAAS,WACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,gQACA,sPACA,ueAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,8EACA,ycACA,yJACA,sKAOpBC,KAAK,CACD5C,MAAM,0BACNuC,KAAK,CACD,CACIC,SAAS,MACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,oIACA,0IAIZ,CACIH,SAAS,OACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,iNACA,mEACA,kKAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,qIACA,wXAIZ,CACIH,SAAS,eACTC,aAAc,CACV,CAACC,UAAU,OAAOC,SAAS,CACvB,+RACA,6cACA,8NACA,qHAEH,CAACD,UAAU,YAAYC,SAAS,CAC7B,yhBACA,mMACA,icACA,yIAEJ,CAACD,UAAU,OAAOC,SAAS,CACvB,uUACA,6dACA,gGACA,+HAEJ,CAACD,UAAU,YAAYC,SAAS,CAC5B,2WACA,wQACA,4RAOpBE,KAAK,CACD7C,MAAM,qBACNuC,KAAK,CACD,CACIC,SAAS,QACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,iDACA,4IACA,kKAIZ,CACIH,SAAS,UACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,0GACA,2aACA,2QACA,4FAIZ,CACIH,SAAS,iBACTC,aAAa,CACT,CAACC,UAAU,GAAGC,SAAS,CACnB,yWACA,kRAUxBrC,SAAS,CACJN,MAAM,kBACNI,KAAK,oVACLiC,QAAQ,kBACRE,KAAK,CACD,CACGC,SAAS,UACTC,aAAa,iIAEhB,CACGD,SAAS,eACTC,aAAa,8VAEhB,CACGD,SAAS,cACTC,aAAa,8JAEjB,CACID,SAAS,WACTC,aAAa,yJAEjB,CACID,SAAS,cACTC,aAAa,uDAIpBK,SAAS,eACTC,aAAa,CACT,CACGP,SAAS,4CACTC,aAAa,uJAEhB,CACGD,SAAS,4CACTC,aAAa,mMAQzBjC,uBAAuB,CAClBR,MAAM,eACNI,KAAK,wYACLiC,QAAS,kBACTE,KAAK,CACD,CACGC,SAAS,mCACTC,aAAa,0UAEhB,CACGD,SAAS,mCACTC,aAAa,2UAEjB,CACID,SAAS,qCACTC,aAAa,yVAQpB5B,QAAQ,oBACRmC,aAAa,MACbC,aAAa,OAMlB1C,mBAAmB,CACbP,MAAM,oBACNI,KAAK,uRACLiC,QAAS,kBACTE,KAAK,CACD,CACEC,SAAS,kBACTC,aAAa,2RAEf,CACED,SAAS,cACTC,aAAa,mPAEjB,CACID,SAAS,cACTC,aAAa,kOACf,CACED,SAAS,qBACTC,aAAa,iPAQzBhC,YAAY,CACRT,MAAM,mBACNI,KAAK,2cAGL8C,SAAS,oBACTC,MAAM,CACF,CACIX,SAAS,wBACTC,aAAa,mZAEjB,CACID,SAAS,oBACTC,aAAa,wTAEjB,CACID,SAAS,oBACTC,aAAa,0QAGjB,CACID,SAAS,kCACTC,aAAa,kZAEjB,CACID,SAAS,WACTC,aAAa,+PAGrBW,SAAS,oBACTC,MAAM,CACF,CACIb,SAAS,iCACTC,aAAa,kQAEjB,CACID,SAAS,oBACTC,aAAa,0OAEjB,CACID,SAAS,mBACTC,aAAa,gOAEjB,CACID,SAAS,wBACTC,aAAa,yPAEjB,CACID,SAAS,yBACTC,aAAa,yPAEjB,CACID,SAAS,SACTC,aAAa,6QAEjB,CACID,SAAS,SACTC,aAAa,6PAEjB,CACID,SAAS,kBACTC,aAAa,mMAQzBpC,MAAM,CACFL,MAAM,yBACNI,KAAK,iUACLiC,QAAS,kBACTE,KAAK,CACD,CACIC,SAAS,oBACTC,aAAa,gSAEjB,CACID,SAAS,mCACTC,aAAa,6NAEjB,CACID,SAAS,4BACTC,aAAa,gFAGrBK,SAAS,OACRC,aAAa,CACT,CACGP,SAAS,iBACTC,aAAa,+QAMzB/B,YAAY,CACNV,MAAM,mBACNI,KAAK,wNACLiC,QAAQ,kBACRE,KAAK,CACD,CACEC,SAAS,eACTC,aAAa,gQAEf,CACED,SAAS,mBACTC,aAAa,gWAEjB,CACID,SAAS,oBACTC,aAAa,iQAEjB,CACID,SAAS,eACTC,aAAa,gOAEjB,CACID,SAAS,sBACTC,aAAa,yLAEjB,CACID,SAAS,2BACTC,aAAa,yJAEjB,CACID,SAAS,0BACTC,aAAa,iNAEjB,CACID,SAAS,sBACTC,aAAa,2KAEjB,CACID,SAAS,kBACTC,aAAa,kKAEjB,CACID,SAAS,iBACTC,aAAa,8QAQzB7B,SAAS,CACHZ,MAAM,uBACNI,KAAK,ojBACLiC,QAAQ,kBACRE,KAAK,CACD,CACEC,SAAS,qBACTC,aAAa,mRAEf,CACED,SAAS,qBACTC,aAAa,gSAEjB,CACID,SAAS,uBACTC,aAAa,8LAEjB,CACID,SAAS,4CACTC,aAAa,6KAEjB,CACID,SAAS,sBACTC,aAAa,iLAOzB9B,kBAAkB,CACdX,MAAM,gBACNI,KAAK,ygBACLiC,QAAQ,kBACRE,KAAK,CACD,CACIC,SAAS,wCACTC,aAAa,gQAEjB,CACID,SAAS,wCACTC,aAAa,yQAEjB,CACID,SAAS,0CACTC,aAAa,4TAGrB5B,QAAQ,oBACRyC,YAAY,OACZC,YAAY,2KACZtD,KAAM,eC1dK,GAEXD,MAAM,kBACNwD,QAAQ,uKACRC,QAAQ,2DACRC,WAAW,8IACXC,QAAQ,2KACR1D,KAAK,OAEL2D,IAAI,CACAC,KAAK,UACLC,KAAK,WAKTC,IAAI,CACAP,QAAQ,2dACRQ,QAAQ,OACRC,WAAW,iCACXC,UAAU,yKACVC,WAAW,2BACXC,UAAU,4IACVC,WAAW,8BACXC,eAAe,yGACfC,eAAe,iHACfC,WAAW,8BACXC,eAAe,uHACfC,eAAe,4HACfC,WAAW,mBACXC,UAAU,CACNzD,MAAM,CACF0D,IAAI,cACJC,IAAI,sBAEJC,KAAK,oBAGT1D,MAAM,CACFwD,IAAI,YACJC,IAAI,YACJE,MAAM,gBACND,KAAK,aAETxD,MAAM,CACFsD,IAAI,YACJC,IAAI,yBACJE,MAAM,wBAGdC,SAAS,mBAIbC,IAAI,CACDC,QAAQ,uVACRlB,WAAW,gDACXmB,aAAa,0FACbC,aAAa,6HAEblB,WAAW,6CACXmB,aAAa,4FACbC,aAAa,sOACbvB,QAAQ,mBAIXwB,OAAO,CACJxF,MAAM,UACN0D,WAAW,0LACX+B,MAAM,SACNxB,WAAW,mDACXE,WAAW,mDACXE,WAAW,iDACXG,WAAW,wDACXR,QAAQ,sBACR0B,OAAO,iBACPC,MAAM,aAKTC,kBAAkB,CACd5F,MAAM,OACN6F,YAAY,8UACZ3B,UAAU,odACVE,UAAU,0XCvFP,GAEXpE,MAAM,YACNuC,KAAK,CACD,CACIvC,MAAM,OACNc,OAAO,wCACPV,KAAK,oJAET,CACIJ,MAAM,OACNc,OAAO,4CACPV,KAAK,2NAET,CACIJ,MAAM,MACNc,OAAO,6CACPV,KAAK,sKCjBF,GACb0F,YAAY,WACZ9F,MAAM,oBACNI,KAAK,yaACL2F,QAAQ,GACRC,gBAAgB,CAAC,iCAAiC,QAAQ,WAG1DC,KAAM,OACNC,WAAW,CACT,CACExJ,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,eACL8F,KAAK,yzCAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,eACL8F,KAAK,k4BAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAM,eACN8F,KAAM,u0CAER,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,qBACL8F,KAAK,81BAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,mBACL8F,KAAK,ohDAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,mBACL8F,KAAK,gxBAEP,CACE1D,GAAI,EACJyJ,SAAU,4BACV7L,KAAK,cACL8F,KAAK,8WAQTgG,aAAc,oCACdC,aAAc,uBACdC,aAAc,CACZ,CACE5J,GAAI,EACJ6J,aAAc,6CACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,yCACA,2FAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,gFACA,uFACA,+HACA,+FAIN,CACEhK,GAAI,EACJ6J,aAAc,sCACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,6GACA,qJACA,mIACA,qFACA,wFAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,qGACA,wFACA,uEACA,4EACA,2EACA,sEACA,iEACA,mDAIN,CACEhK,GAAI,EACJ6J,aAAc,mCACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,qGAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,8GACA,8TACA,sIACA,6HAIN,CACEhK,GAAI,EACJ6J,aAAc,6BACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,8HAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,2HACA,kKACA,gEACA,4FACA,8KACA,mDAIN,CACEhK,GAAI,EACJ6J,aAAc,6CACdC,QAAS,0BACTC,KAAM,CACJzG,MAAO,oBACP0G,QAAS,CACP,qOACA,6GACA,sFACA,oEAGJC,gBAAiB,CACf3G,MAAO,gBACP0G,QAAS,CACP,sFACA,mGACA,uJACA,0DACA,8CACA,iFACA,oDAOP1E,QAAS,CAAC,MAAM,mBAAmB,4BAA4B,2BAA2B,eAAe,kBAAkB,kBAC3H4E,MAAO,SACPhI,KAAM,KACNiI,WAAW,CACV,CACE,CAAC,GAAG,OACJ,CACE,CACE,CACEnK,GAAG,UACHsD,MAAO,6EACP8G,OAAQ,mKACRC,KAAM,OACNL,QAAS,k2BACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2DACP8G,OAAQ,uGACRC,KAAM,OACNL,QAAS,kjCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,wEACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,i1CACThO,IAAK,uEAEP,CACEgE,GAAG,UACHsD,MAAO,8DACP8G,OAAQ,sDACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2DACP8G,OAAQ,yBACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,0CAIT,CACE,CACEgE,GAAG,UACHsD,MAAO,wDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0vBACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,qFACP8G,OAAQ,2BACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,mDAEP,CACEgE,GAAG,UACHsD,MAAO,iGACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,yuCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,8FACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,27BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,wGACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,45CACThO,IAAK,+CAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,oDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0iCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,2EACP8G,OAAQ,8CACRC,KAAM,OACNL,QAAS,88CACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,i4BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,k6BACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,6HACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,ghCACThO,IAAK,uEAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,qHACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,0iDACThO,IAAK,yDAEP,CACEgE,GAAG,UACHsD,MAAO,4FACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,qzCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,uFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,+0CACThO,IAAK,8CAEP,CACEgE,GAAG,UACHsD,MAAO,mCACP8G,OAAQ,0FACRC,KAAM,GACNL,QAAS,mdACThO,IAAK,6EAEP,CACEgE,GAAG,UACHsD,MAAO,iEACP8G,OAAQ,kBACRC,KAAM,OACNL,QAAS,g/BACThO,IAAK,kDAGT,CACE,CACEgE,GAAG,UACHsD,MAAO,2HACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,4hCACThO,IAAK,gDAEP,CACEgE,GAAG,UACHsD,MAAO,gFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,iDAKb,CACE,CAAC,EAAE,SACH,CACE,CACE,CACEgE,GAAG,YACHsD,MAAO,8FACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,27BACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,6HACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,ghCACThO,IAAK,sEAEP,CACEgE,GAAG,YACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,k6BACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,qHACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,0iDACThO,IAAK,yDAEP,CACEgE,GAAG,YACHsD,MAAO,oFACP8G,OAAQ,aACRC,KAAM,OACNL,QAAS,i4BACThO,IAAK,4DAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,wGACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,45CACThO,IAAK,8CAEP,CACEgE,GAAG,cACHsD,MAAO,4FACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,qzCACThO,IAAK,gDAEP,CACEgE,GAAG,cACHsD,MAAO,uFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,+0CACThO,IAAK,8CAEP,CACEgE,GAAG,cACHsD,MAAO,2HACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,4hCACThO,IAAK,gDAEP,CACEgE,GAAG,cACHsD,MAAO,gFACP8G,OAAQ,UACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,iDAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,wEACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,i1CACThO,IAAK,0EAKb,CACE,CAAC,EAAE,SACH,CACE,CACE,CACEgE,GAAG,YACHsD,MAAO,2DACP8G,OAAQ,uGACRC,KAAM,OACNL,QAAS,kjCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,6EACP8G,OAAQ,mKACRC,KAAM,OACNL,QAAS,k2BACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,8DACP8G,OAAQ,sDACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,2DACP8G,OAAQ,yBACRC,KAAM,OACNL,QAAS,ywBACThO,IAAK,yCAEP,CACEgE,GAAG,YACHsD,MAAO,wDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0vBACThO,IAAK,iDAGT,CACE,CACEgE,GAAG,YACHsD,MAAO,2EACP8G,OAAQ,8CACRC,KAAM,OACNL,QAAS,88CACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,oDACP8G,OAAQ,mCACRC,KAAM,OACNL,QAAS,0iCACThO,IAAK,gDAEP,CACEgE,GAAG,YACHsD,MAAO,mCACP8G,OAAQ,0FACRC,KAAM,OACNL,QAAS,mdACThO,IAAK,gFAKb,CACE,CAAC,EAAE,YACH,CACE,CACE,CACEgE,GAAG,eACHsD,MAAO,qFACP8G,OAAQ,2BACRC,KAAM,OACNL,QAAS,qvCACThO,IAAK,mDAEP,CACEgE,GAAG,eACHsD,MAAO,iEACP8G,OAAQ,kBACRC,KAAM,OACNL,QAAS,g/BACThO,IAAK,oDAKb,CACE,CAAC,EAAE,WACH,CACE,CACE,CACEgE,GAAG,cACHsD,MAAO,iGACP8G,OAAQ,qBACRC,KAAM,OACNL,QAAS,yuCACThO,IAAK,qDC1kBF,GACXkL,IAAK,CACHoD,KAAM,OACN9G,SAAS,WACTW,QAAQ,kBACRc,SAAS,YACTsF,MAAM,WACNpF,KAAK,QAEPqF,cACAC,kBACAC,iBACAC,kBACAC,gB,yrBCHJ,IAAMC,EAAalK,OAAO3H,UAAUF,KACpC6H,OAAO3H,UAAUF,KAAO,SAAcgS,GACpC,OAAOD,EAAW3R,KAAKsG,KAAMsL,GAAUC,MAAM,SAAA1N,GAAK,OAAGA,KAGvDoD,OAAIC,IAAIsK,QACRvK,OAAIwK,OAAOC,eAAgB,EAE3BzK,OAAID,UAAU,SAAS2K,GAEvBC,EAAOC,WAAW,SAACC,EAAIC,EAAMnE,GAC3BA,IACA,IAAMoE,EAAW,CACf,CAAE3K,KAAM,aAAcb,GAAI,MAAOC,UAAW,uBAC5C,CAAEY,KAAM,WAAYb,GAAI,MAAOC,UAAW,sBAC1C,CAAEY,KAAM,YAAab,GAAI,MAAOC,UAAW,uBAC3C,CAAEY,KAAM,+BAAgCb,GAAI,MAAOC,UAAW,iCAC9D,CAAEY,KAAM,4BAA6Bb,GAAI,MAAOC,UAAW,2BAC3D,CAAEY,KAAM,8BAA+Bb,GAAI,MAAOC,UAAW,mCAC7D,CAAEY,KAAM,cAAeb,GAAI,MAAOC,UAAW,sBAC7C,CAAEY,KAAM,cAAeb,GAAI,MAAOC,UAAW,sBAC7C,CAAEY,KAAM,iBAAkBb,GAAI,MAAOC,UAAW,yBAChD,CAAEY,KAAM,wBAAyBb,GAAI,MAAOC,UAAW,2BACvD,CAAEY,KAAM,4BAA6Bb,GAAI,MAAOC,UAAW,yBAC3D,CAAEY,KAAM,0BAA2Bb,GAAI,MAAOC,UAAW,iCACzD,CAAEY,KAAM,qBAAsBb,GAAI,MAAOC,UAAW,6BACpD,CAAEY,KAAM,sBAAuBb,GAAI,MAAOC,UAAW,yBACrD,CAAEY,KAAM,uBAAwBb,GAAI,MAAOC,UAAW,6BACtD,CAAEY,KAAM,uBAAwBb,GAAI,MAAOC,UAAW,4BAExDuL,EAASC,QAAQ,SAACvJ,GACboJ,EAAGzK,OAASqB,EAAKrB,MAClBd,eAAiBmC,EAAKlC,GAAIkC,EAAKjC,eAKrCQ,OAAIC,IAAIgL,QACR,IAAMC,EAAO,IAAID,OAAQ,CACvBE,OAAQ,KAERC,SAAU,CACHC,KACAC,QAILX,EAAOY,UAAU,WACf7M,OAAO8M,SAAS,EAAE,KAEtB,IAAMC,EAAQ,IAAIlB,OAAKmB,MAAT,KAAmBA,IAEnC,IAAI1L,OAAI,CACN2K,SACAO,KAAKA,EACLO,MAAMA,EACN5M,OAAQ,SAAA8M,GAAC,OAAIA,EAAEC,iBACdC,OAAO,S,2DCzEV,yBAAke,EAAG","file":"js/app.7cb86f6b.js","sourcesContent":[" \t// install a JSONP callback for chunk loading\n \tfunction webpackJsonpCallback(data) {\n \t\tvar chunkIds = data[0];\n \t\tvar moreModules = data[1];\n \t\tvar executeModules = data[2];\n\n \t\t// add \"moreModules\" to the modules object,\n \t\t// then flag all \"chunkIds\" as loaded and fire callback\n \t\tvar moduleId, chunkId, i = 0, resolves = [];\n \t\tfor(;i < chunkIds.length; i++) {\n \t\t\tchunkId = chunkIds[i];\n \t\t\tif(installedChunks[chunkId]) {\n \t\t\t\tresolves.push(installedChunks[chunkId][0]);\n \t\t\t}\n \t\t\tinstalledChunks[chunkId] = 0;\n \t\t}\n \t\tfor(moduleId in moreModules) {\n \t\t\tif(Object.prototype.hasOwnProperty.call(moreModules, moduleId)) {\n \t\t\t\tmodules[moduleId] = moreModules[moduleId];\n \t\t\t}\n \t\t}\n \t\tif(parentJsonpFunction) parentJsonpFunction(data);\n\n \t\twhile(resolves.length) {\n \t\t\tresolves.shift()();\n \t\t}\n\n \t\t// add entry modules from loaded chunk to deferred list\n \t\tdeferredModules.push.apply(deferredModules, executeModules || []);\n\n \t\t// run deferred modules when all chunks ready\n \t\treturn checkDeferredModules();\n \t};\n \tfunction checkDeferredModules() {\n \t\tvar result;\n \t\tfor(var i = 0; i < deferredModules.length; i++) {\n \t\t\tvar deferredModule = deferredModules[i];\n \t\t\tvar fulfilled = true;\n \t\t\tfor(var j = 1; j < deferredModule.length; j++) {\n \t\t\t\tvar depId = deferredModule[j];\n \t\t\t\tif(installedChunks[depId] !== 0) fulfilled = false;\n \t\t\t}\n \t\t\tif(fulfilled) {\n \t\t\t\tdeferredModules.splice(i--, 1);\n \t\t\t\tresult = __webpack_require__(__webpack_require__.s = deferredModule[0]);\n \t\t\t}\n \t\t}\n \t\treturn result;\n \t}\n\n \t// The module cache\n \tvar installedModules = {};\n\n \t// object to store loaded CSS chunks\n \tvar installedCssChunks = {\n \t\t\"app\": 0\n \t}\n\n \t// object to store loaded and loading chunks\n \t// undefined = chunk not loaded, null = chunk preloaded/prefetched\n \t// Promise = chunk loading, 0 = chunk loaded\n \tvar installedChunks = {\n \t\t\"app\": 0\n \t};\n\n \tvar deferredModules = [];\n\n \t// script path function\n \tfunction jsonpScriptSrc(chunkId) {\n \t\treturn __webpack_require__.p + \"js/\" + ({}[chunkId]||chunkId) + \".\" + {\"chunk-150f2a22\":\"7c060552\",\"chunk-48549da8\":\"7eff62c5\",\"chunk-5597a18c\":\"5609a97f\",\"chunk-6b0a8c8b\":\"7dcab209\",\"chunk-7cbeefd3\":\"5bb94f76\",\"chunk-85d5875a\":\"d017c31d\",\"chunk-9a4dc728\":\"d30fc624\",\"chunk-a0684f72\":\"07329ef9\",\"chunk-e8e00e3e\":\"e4354a24\",\"chunk-f5db0f22\":\"6eceb54e\",\"chunk-2e54a88e\":\"a7c81b33\",\"chunk-313ec15e\":\"ffba92c7\",\"chunk-395ac447\":\"5385d2dd\",\"chunk-4e2c65a4\":\"cde0744a\",\"chunk-5c7caa12\":\"a242be8a\",\"chunk-5ec60ba6\":\"8d8cdc28\",\"chunk-638b533c\":\"73943361\",\"chunk-6cb17c0e\":\"b6836812\",\"chunk-70fb8741\":\"18a83dff\",\"chunk-a84b203c\":\"e971ec42\",\"chunk-b9b25720\":\"8296452f\",\"chunk-f884b720\":\"9777babd\"}[chunkId] + \".js\"\n \t}\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n \t// This file contains only the entry chunk.\n \t// The chunk loading function for additional chunks\n \t__webpack_require__.e = function requireEnsure(chunkId) {\n \t\tvar promises = [];\n\n\n \t\t// mini-css-extract-plugin CSS loading\n \t\tvar cssChunks = {\"chunk-150f2a22\":1,\"chunk-48549da8\":1,\"chunk-5597a18c\":1,\"chunk-6b0a8c8b\":1,\"chunk-7cbeefd3\":1,\"chunk-85d5875a\":1,\"chunk-9a4dc728\":1,\"chunk-a0684f72\":1,\"chunk-e8e00e3e\":1,\"chunk-f5db0f22\":1,\"chunk-2e54a88e\":1,\"chunk-313ec15e\":1,\"chunk-395ac447\":1,\"chunk-4e2c65a4\":1,\"chunk-5c7caa12\":1,\"chunk-5ec60ba6\":1,\"chunk-638b533c\":1,\"chunk-6cb17c0e\":1,\"chunk-70fb8741\":1,\"chunk-a84b203c\":1,\"chunk-b9b25720\":1,\"chunk-f884b720\":1};\n \t\tif(installedCssChunks[chunkId]) promises.push(installedCssChunks[chunkId]);\n \t\telse if(installedCssChunks[chunkId] !== 0 && cssChunks[chunkId]) {\n \t\t\tpromises.push(installedCssChunks[chunkId] = new Promise(function(resolve, reject) {\n \t\t\t\tvar href = \"css/\" + ({}[chunkId]||chunkId) + \".\" + {\"chunk-150f2a22\":\"7035f3ad\",\"chunk-48549da8\":\"7510da79\",\"chunk-5597a18c\":\"04c58924\",\"chunk-6b0a8c8b\":\"2dd1ed80\",\"chunk-7cbeefd3\":\"37d37332\",\"chunk-85d5875a\":\"bb49213a\",\"chunk-9a4dc728\":\"ab0a5b59\",\"chunk-a0684f72\":\"b60fe50c\",\"chunk-e8e00e3e\":\"135369ce\",\"chunk-f5db0f22\":\"62cd94a6\",\"chunk-2e54a88e\":\"c6f55108\",\"chunk-313ec15e\":\"219c18b0\",\"chunk-395ac447\":\"9b2ac1f7\",\"chunk-4e2c65a4\":\"226174e8\",\"chunk-5c7caa12\":\"304ec061\",\"chunk-5ec60ba6\":\"02a39819\",\"chunk-638b533c\":\"016888cb\",\"chunk-6cb17c0e\":\"0b9bc176\",\"chunk-70fb8741\":\"05609e40\",\"chunk-a84b203c\":\"071e20c2\",\"chunk-b9b25720\":\"f9ce2cc9\",\"chunk-f884b720\":\"7f6930f1\"}[chunkId] + \".css\";\n \t\t\t\tvar fullhref = __webpack_require__.p + href;\n \t\t\t\tvar existingLinkTags = document.getElementsByTagName(\"link\");\n \t\t\t\tfor(var i = 0; i < existingLinkTags.length; i++) {\n \t\t\t\t\tvar tag = existingLinkTags[i];\n \t\t\t\t\tvar dataHref = tag.getAttribute(\"data-href\") || tag.getAttribute(\"href\");\n \t\t\t\t\tif(tag.rel === \"stylesheet\" && (dataHref === href || dataHref === fullhref)) return resolve();\n \t\t\t\t}\n \t\t\t\tvar existingStyleTags = document.getElementsByTagName(\"style\");\n \t\t\t\tfor(var i = 0; i < existingStyleTags.length; i++) {\n \t\t\t\t\tvar tag = existingStyleTags[i];\n \t\t\t\t\tvar dataHref = tag.getAttribute(\"data-href\");\n \t\t\t\t\tif(dataHref === href || dataHref === fullhref) return resolve();\n \t\t\t\t}\n \t\t\t\tvar linkTag = document.createElement(\"link\");\n \t\t\t\tlinkTag.rel = \"stylesheet\";\n \t\t\t\tlinkTag.type = \"text/css\";\n \t\t\t\tlinkTag.onload = resolve;\n \t\t\t\tlinkTag.onerror = function(event) {\n \t\t\t\t\tvar request = event && event.target && event.target.src || fullhref;\n \t\t\t\t\tvar err = new Error(\"Loading CSS chunk \" + chunkId + \" failed.\\n(\" + request + \")\");\n \t\t\t\t\terr.code = \"CSS_CHUNK_LOAD_FAILED\";\n \t\t\t\t\terr.request = request;\n \t\t\t\t\tdelete installedCssChunks[chunkId]\n \t\t\t\t\tlinkTag.parentNode.removeChild(linkTag)\n \t\t\t\t\treject(err);\n \t\t\t\t};\n \t\t\t\tlinkTag.href = fullhref;\n\n \t\t\t\tvar head = document.getElementsByTagName(\"head\")[0];\n \t\t\t\thead.appendChild(linkTag);\n \t\t\t}).then(function() {\n \t\t\t\tinstalledCssChunks[chunkId] = 0;\n \t\t\t}));\n \t\t}\n\n \t\t// JSONP chunk loading for javascript\n\n \t\tvar installedChunkData = installedChunks[chunkId];\n \t\tif(installedChunkData !== 0) { // 0 means \"already installed\".\n\n \t\t\t// a Promise means \"currently loading\".\n \t\t\tif(installedChunkData) {\n \t\t\t\tpromises.push(installedChunkData[2]);\n \t\t\t} else {\n \t\t\t\t// setup Promise in chunk cache\n \t\t\t\tvar promise = new Promise(function(resolve, reject) {\n \t\t\t\t\tinstalledChunkData = installedChunks[chunkId] = [resolve, reject];\n \t\t\t\t});\n \t\t\t\tpromises.push(installedChunkData[2] = promise);\n\n \t\t\t\t// start chunk loading\n \t\t\t\tvar script = document.createElement('script');\n \t\t\t\tvar onScriptComplete;\n\n \t\t\t\tscript.charset = 'utf-8';\n \t\t\t\tscript.timeout = 120;\n \t\t\t\tif (__webpack_require__.nc) {\n \t\t\t\t\tscript.setAttribute(\"nonce\", __webpack_require__.nc);\n \t\t\t\t}\n \t\t\t\tscript.src = jsonpScriptSrc(chunkId);\n\n \t\t\t\tonScriptComplete = function (event) {\n \t\t\t\t\t// avoid mem leaks in IE.\n \t\t\t\t\tscript.onerror = script.onload = null;\n \t\t\t\t\tclearTimeout(timeout);\n \t\t\t\t\tvar chunk = installedChunks[chunkId];\n \t\t\t\t\tif(chunk !== 0) {\n \t\t\t\t\t\tif(chunk) {\n \t\t\t\t\t\t\tvar errorType = event && (event.type === 'load' ? 'missing' : event.type);\n \t\t\t\t\t\t\tvar realSrc = event && event.target && event.target.src;\n \t\t\t\t\t\t\tvar error = new Error('Loading chunk ' + chunkId + ' failed.\\n(' + errorType + ': ' + realSrc + ')');\n \t\t\t\t\t\t\terror.type = errorType;\n \t\t\t\t\t\t\terror.request = realSrc;\n \t\t\t\t\t\t\tchunk[1](error);\n \t\t\t\t\t\t}\n \t\t\t\t\t\tinstalledChunks[chunkId] = undefined;\n \t\t\t\t\t}\n \t\t\t\t};\n \t\t\t\tvar timeout = setTimeout(function(){\n \t\t\t\t\tonScriptComplete({ type: 'timeout', target: script });\n \t\t\t\t}, 120000);\n \t\t\t\tscript.onerror = script.onload = onScriptComplete;\n \t\t\t\tdocument.head.appendChild(script);\n \t\t\t}\n \t\t}\n \t\treturn Promise.all(promises);\n \t};\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n \t// on error function for async loading\n \t__webpack_require__.oe = function(err) { console.error(err); throw err; };\n\n \tvar jsonpArray = window[\"webpackJsonp\"] = window[\"webpackJsonp\"] || [];\n \tvar oldJsonpFunction = jsonpArray.push.bind(jsonpArray);\n \tjsonpArray.push = webpackJsonpCallback;\n \tjsonpArray = jsonpArray.slice();\n \tfor(var i = 0; i < jsonpArray.length; i++) webpackJsonpCallback(jsonpArray[i]);\n \tvar parentJsonpFunction = oldJsonpFunction;\n\n\n \t// add entry module to deferred list\n \tdeferredModules.push([0,\"chunk-vendors\"]);\n \t// run deferred modules when ready\n \treturn checkDeferredModules();\n","import mod from \"-!../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../node_modules/css-loader/index.js??ref--6-oneOf-1-1!../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=style&index=0&lang=css&\"; export default mod; export * from \"-!../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../node_modules/css-loader/index.js??ref--6-oneOf-1-1!../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=style&index=0&lang=css&\"","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{attrs:{\"id\":\"app\"}},[_c('router-view')],1)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","export * from \"-!cache-loader?{\\\"cacheDirectory\\\":\\\"node_modules/.cache/vue-loader\\\",\\\"cacheIdentifier\\\":\\\"9bd81872-vue-loader-template\\\"}!../node_modules/vue-loader/lib/loaders/templateLoader.js??vue-loader-options!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=template&id=20fc2bd2&\"","const handleClickEvent = (id, eventName, params) => {\n    const clickId = id || '';\n    const options = params || {};\n    BeaconAction.onEvent(clickId, eventName, options);\n    console.log(`id：-${clickId}\\neventName:-${eventName}`);\n    // alert(`id：-${clickId}\\neventName:-${eventName}`) \n  }\n\nexport {\n    handleClickEvent\n}","import mod from \"-!../node_modules/cache-loader/dist/cjs.js??ref--12-0!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../node_modules/cache-loader/dist/cjs.js??ref--12-0!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./App.vue?vue&type=template&id=20fc2bd2&\"\nimport script from \"./App.vue?vue&type=script&lang=js&\"\nexport * from \"./App.vue?vue&type=script&lang=js&\"\nimport style0 from \"./App.vue?vue&type=style&index=0&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","import Vue from 'vue';\nimport Router from 'vue-router';\n\nVue.use(Router);\n\n\nexport default new Router({\n  routes: [\n    {\n      path: '/',\n      redirect: {\n        name: 'index'\n      }\n    },\n    {\n      path: '/index',\n      name: 'index',\n      component: () => import('./views/index/index.vue')\n    },\n    {\n      path: '/research',\n      component: () => import('./views/index/research.vue'),\n      children: [\n        {\n          path: 'international',\n          name: 'international',\n          component: () => import('./views/research/international.vue')\n        },\n        {\n          path: 'newMedia',\n          name: 'newMedia',\n          component: () => import('./views/research/newMedia.vue')\n        },\n        {\n          path: 'videoCode',\n          name: 'videoCode',\n          component: () => import('./views/research/videoCode.vue')\n        },\n        {\n          path: 'VideoUnderstanding',\n          name: 'VideoUnderstanding',\n          component: () => import('./views/research/VideoUnderstanding.vue')\n        },\n        {\n          path: 'videoProcessing',\n          name: 'videoProcessing',\n          component: () => import('./views/research/videoProcessing.vue')\n        },\n        {\n          path: 'perceptual',\n          name: 'perceptual',\n          component: () => import('./views/research/Perceptual.vue')\n        },\n        {\n          path: 'audioProcessing',\n          name: 'audioProcessing',\n          component: () => import('./views/research/AudioProcessing.vue')\n        },\n        {\n          path: 'audioTransmission',\n          name: 'audioTransmission',\n          component: () => import('./views/research/AudioTransmission.vue')\n        },\n        {\n          path: 'assessment',\n          name: 'assessment',\n          component: () => import('./views/research/Assessment.vue')\n        },\n        {\n          path: '',\n          redirect: 'international'\n        }\n      ]\n    },\n    {\n      path: '/project',\n      name: 'project',\n      component: () => import('./views/index/project.vue')\n    },\n    {\n      path:'/projectTpg',\n      name:'projectTpg',\n      component: ()=> import('./views/project/projectTpg.vue')\n    },\n    {\n      path:'/projectTse',\n      name:'projectTse',\n      component: ()=> import('./views/project/projectTse.vue')\n    },\n    {\n      path:'/projectLiYing',\n      name:'projectLiYing',\n      component: ()=> import('./views/project/projectLiYing.vue')\n    },\n    {\n      path:'/projectAudioAndVideo',\n      name:'projectAudioAndVideo',\n      component: ()=> import('./views/project/projectAudioAndVideo.vue')\n    },\n    {\n      path: '/aboutUs',\n      component: () => import('./views/index/aboutUs.vue'),\n      children:[\n        {\n          path: 'laboratory',\n          name: 'laboratory',\n          component: () => import('./views/aboutUs/laboratory.vue')\n        },\n        {\n          path: 'thesis',\n          name: 'thesis',\n          component: () => import('./views/aboutUs/thesis.vue')\n        },\n        {\n          path: 'joinUs',\n          name: 'joinUs',\n          component: () => import('./views/aboutUs/joinUs.vue')\n        },\n        {\n          path: '',\n          redirect: 'laboratory'\n        }\n      ]\n    },\n    {\n      path: '/solution',\n      name: 'solution',\n      component: () => import('./views/index/solution.vue')\n    }\n  ]})\n","export default {\n    SET_NAVITEM_STATUS: 'SET_NAVITEM_STATUS',\n    SET_NAV_MASK: 'SET_NAV_MASK',\n    SET_BANNER_ISANDROID: 'SET_BANNER_ISANDROID',\n    SET_BANNER_ISIOS: 'SET_BANNER_ISIOS',\n    SET_SCROLLINDECX: 'SET_SCROLLINDECX'\n}","import type from './mutation-type';\n\nconst state = {\n    navItemstatus: {\n        items1: false,\n        items2: false,\n        items3: false,\n        items4: false\n    },\n    isMask: false, // mask移动端菜单是否打开，默认关闭\n    isAndroid: false,\n    isiOS: false,\n    scrollIndex: 0 // 移动端研究领域页面tabBar进入位置\n}\n\nconst mutations = {\n    [type.SET_NAVITEM_STATUS](STATE, data) {\n        STATE.navItemstatus[`items${data.index}`] = data.item;\n    },\n    [type.SET_NAV_MASK](STATAE, data) {\n        STATAE.isMask = data\n    },\n    [type.SET_BANNER_ISANDROID](STATE, data) {\n        STATE.isAndroid = data\n    },\n    [type.SET_BANNER_ISIOS](STATE, data) {\n        STATE.isiOS = data\n    },\n    [type.SET_SCROLLINDECX](STATE, data) {\n        STATE.scrollIndex = data\n    }\n}\nexport default {\n    state,\n    mutations\n}","import mutationState from './mutation';\n\nconst { state, mutations } = { ...mutationState };\n\nexport default {\n    mutations,\n    state\n}","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',[_c('div',{ref:\"out\",staticClass:\"out\",style:({background:_vm.outBg,height:_vm.innerHeight+'px'}),attrs:{\"id\":\"out\"}},[_c('div',{ref:\"inner\",style:({width : _vm.iWidth+'px',height:_vm.innerHeight+'px',background:_vm.innerBg}),attrs:{\"id\":\"inner\"}},[_vm._t(\"default\")],2)])])}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <div id=\"out\" class=\"out\" :style=\"{background:outBg,height:innerHeight+'px'}\" ref=\"out\">\n      <div id=\"inner\" :style=\"{width : iWidth+'px',height:innerHeight+'px',background:innerBg}\" ref=\"inner\">\n        <slot></slot>\n      </div>\n    </div>\n  </div>\n</template>\n<script>\n\nexport default {\n  data(){\n     return{\n     }\n  },\n  props:{\n      // 'outHeight','iWidth','innerHeight','outBg','innerBg'\n      outHeight: {\n        type: String,\n        default: ''\n      },\n      iWidth: {\n        type: String,\n        default: ''\n      },\n      innerHeight: {\n        type: String,\n        default: ''\n      },\n      outBg: {\n        type: String,\n        default: ''\n      },\n      innerBg: {\n        type: String,\n        default: ''\n      }\n  },\n  mounted(){\n    var out = document.getElementById(\"out\")\n    var inner = document.getElementById(\"inner\")\n    window.onresize = function () {\n          inner.style.left=(out.clientWidth-inner.clientWidth)/2+\"px\"\n          inner.style.top=(out.clientHeight-inner.clientHeight)/2+\"px\"\n              }\n             \n          inner.style.left=(out.clientWidth-inner.clientWidth)/2+\"px\"\n          inner.style.top=(out.clientHeight-inner.clientHeight)/2+\"px\"\n          }\n     }\n\n</script>\n<style scoped >\n*{\n  margin: 0px;\n  padding: 0px;\n}\nbody{\n   margin: 0px ! important;\n   padding: 0px;\n}\n #out{\n   position: relative; \n }\n #inner{\n   /* position: absolute; */\n   margin: 0 auto;\n   \n   padding-bottom: 100px;\n }\n \n@media only screen and (max-width:768px){\n   #inner{\n     width: 95% !important; \n   }\n }\n @media only screen and (min-width:768px){\n   #inner{\n     width: 720px !important;\n     \n   }\n }\n @media only screen and (min-width: 992px){\n   #inner{\n     width: 960px !important;\n     \n   }\n }\n  @media only screen and (min-width: 1200px){\n   #inner{\n     width: 1180px !important;\n     \n   }\n }\n</style>","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Layout.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Layout.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Layout.vue?vue&type=template&id=5cbf1b3b&scoped=true&\"\nimport script from \"./Layout.vue?vue&type=script&lang=js&\"\nexport * from \"./Layout.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Layout.vue?vue&type=style&index=0&id=5cbf1b3b&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"5cbf1b3b\",\n  null\n  \n)\n\nexport default component.exports","export default {\n  title: \"腾讯多媒体实验室，让世界在你身边\",\n  more: \"了解更多\",\n  //  研究领域\n  research:{\n        title: '研发领域',\n        researchAndMade:{\n          title:\"国际标准研究与制定\",\n          text:\"专注媒体压缩，系统传输等多媒体领域标准研究和制定\"\n         },\n        touch:{  \n          title:\"异构平台\",\n          text:\"具有比传统CPU并行计算更高效率和低延迟的计算性能。\"\n        },\n        newMedia:{  \n         title:\"沉浸式新媒体\",\n         text:\"研究采集、压缩、传输、播放技术链给人带来感官体验\"\n        },\n        videoUnderstanding:{  \n          title:\"智慧媒体\",\n          text:\"利用深度学习，对视频内容进行深入理解\"\n        },\n         VideoCodingAndDecoding:{  \n           title:\"视频编解码\",\n           text:\"更高的视频压缩比、更好的信道适应性\"\n         },\n         videoHandle:{  \n           title:\"视频处理\",\n           text:\"时域信息、结合视频理解和编解码信息的智能处理\"\n         },\n         audioHandle:{  \n           title:\"音频处理\",\n           text:\"致力于解决多人、多场景实时通信系统中的3A问题\"\n         },\n         connectionQuality:{  \n           title:\"音视频通信质量评估\",\n           text:\"符合人眼视觉和人耳听觉的智能质量评估体系\"\n         },\n         transfer:{  \n           title:\"实时音视频传输\",\n           text:\"通过对网络状态的检测制定合适的抗性和传输策略\"\n         },\n  },\n   //  项目\n  project:{\n    title1:\"TPG\",\n    title2:\"TSE\",\n    title3:\"腾讯丽影\",\n    title4:\"音视频质量评测平台\",\n    subTitle1:\"TPG (Tiny Portable Graphic)\",\n    text1:\"基于新一代视频压缩编码标准AVS2的图像压缩格式\",\n    subTitle2:\"TSE (Tencent Screen Encoder)\",\n    text2:\"专门针对屏幕内容进行优化的编码器\",\n    subTitle3:\"腾讯丽影\",\n    text3:\"以人眼视觉技术为基准，结合人工智能、大数据，对视频进行智能增强和压缩\",\n    subTitle4:\"音视频质量评测平台\",\n    text4:\"提供自动化音视频质量测试能力，多方面对音视频质量进行压力测试\",\n    lookAll:\"查看全部\",\n    more: \"了解更多\",\n  },\n   // 解决方案\n  solution:{\n    solutionTitle: \"解决方案\",\n    title1:\"腾讯实时音视频\",\n    text1:\"拥有QQ十几年来在音视频技术上的积累，致力于帮助企业快速搭建低成本、高品质音视频通讯能力的完整解决方案。\",\n    title2:\"互动直播解决方案\",\n    text2:\"全新一站式“多路音视频互动”解决方案，主打直播连麦和多画面特效，通过移动直播 SDK 打造跨平台一对多、多对多的超清酷炫直播场景。\",\n    title3:\"腾讯云游戏多媒体引擎\",\n    text3:\"提供一站式游戏语音解决方案。针对不同游戏场景进行深度优化，功能完备，接入门槛低，一个 SDK 即可满足多样化的游戏语音需求。\",\n    more:\"了解更多\",\n    lookAll:\"查看全部\",\n    join:\"加入腾讯多媒体实验室\",\n    join1:\"加入多媒体实验室\",\n    joining:\"立即加入\"\n  },\n  // mask\n  navList:[\n    {\n      name: '首页'\n    },\n    {\n      name: '研发领域',\n      showList: false,\n      childen: [\n        { eventName: 'click_research_international', id: '86717', name: '国际标准研究与制定', link:'/research/international', index: 0},\n        { eventName: 'click_research_audio_and_video', id: '82091',name: '实时音视频传输', link:'/research/audioTransmission', index: 1 },\n        { eventName: 'click_research_immersive', id: '86718',name: '沉浸式新媒体研究', link:'/research/newMedia', index: 2 },\n        { eventName: 'click_research_codec', id: '86719',name: '视频编解码', link:'/research/videoCode', index: 3 },\n        { eventName: 'click_research_audio', id: '86716',name: '音频处理', link:'/research/audioProcessing', index: 4 },\n        { eventName: 'click_research_process', id: '82088',name: '视频处理', link:'/research/videoProcessing', index: 5 },\n        { eventName: 'click_research_perceived', id: '86720',name: '异构平台', link:'/research/perceptual', index: 6 },\n        { eventName: 'click_research_assess', id: '82092',name: '音视频通信质量评估', link:'/research/assessment', index: 7 },        \n        { eventName: 'click_research_understanding', id: '82087',name: '智慧媒体', link:'/research/VideoUnderstanding', index: 8 },\n      ]\n    },\n    {\n      name: '研究项目',\n      showList: false,\n      childen: [\n        { eventName: 'click_top_project', id: '82082',name: '项目首页', link:'/project' },\n        { eventName: 'click_project_TPG', id: '82096',name: 'TPG', link:'/projectTpg' },\n        { eventName: 'click_project_TSE', id: '82097',name: 'TSE', link:'/projectTse' },\n        { eventName: 'click_project_liying', id: '82098',name: '腾讯丽影', link:'/projectLiYing' },\n        { eventName: 'click_project_platform', id: '82099',name: '音视频质量评测平台', link:'/projectAudioAndVideo' }\n      ]\n    },\n    {\n      name: '解决方案',\n      showList: false,\n      childen: [\n        { eventName: 'click_top_solution', id: '82083',name: '解决方案首页', bgc: false, link:'/solution' },\n        { eventName: 'click_solution_TRTC', id: '82093',name: '腾讯实时音视频', bgc: true, link:'https://cloud.tencent.com/product/trtc '},\n        { eventName: 'click_solution_ILVB', id: '82094',name: '互动直播', bgc: true, link:'https://cloud.tencent.com/solution/ilvb' },\n        { eventName: 'click_solution_GME', id: '82095',name: '腾讯云游戏多媒体引擎', bgc: true, link:' https://cloud.tencent.com/product/gme' }\n      ]\n    },\n    {\n      name: '关于我们',\n      showList: false,\n      childen: [\n        { name: '实验室介绍', link:'/aboutUs/laboratory' },\n        // { name: '相关论文', link:'/aboutUs/thesis' },\n        { name: '加入我们', link:'/aboutUs/joinUs' }\n      ]\n    }\n  ]\n}"," export default {\n    //  研究领域页面总标题\n     title:\"研发领域\",\n\n    //  侧边导航菜单文字使用用home页面里的变量，这里省略\n\n\n\n\n    // 1. 国际标准化研究与制定页面\n    researchAndMade:{\n        title:\"国际标准研究与制定\",\n        title1:\"国际研究与制定\",\n        text:\"我们专注于多媒体领域的标准研究和制定。研究领域包括媒体压缩类，系统与传输类等。同时我们积极参与相关工业论坛，以技术推动产业发展。\",\n        explain:\"我们专注于多媒体领域的标准研究和制定。研究领域包括:\",\n\n        //   tabs切换    --tab媒体压缩\n        \n        tab1:{\n            title:\"媒体压缩\",\n            list:[\n                {\n                    subTitle:\"VVC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"Versatile Video Coding (VVC)是由ISO MPEG与ITU-T VCEG组成的联合专家组JVET制定的下一代视频编解码标准。\",\n                            \"VVC的目标是比其上一代标准HEVC在同等主观质量条件下节省40%-50%的码率。Media Lab深度参与VVC标准制定，并有数十项提案被采纳入VVC标准。\",\n                            \"实验室多人在JVET中担任重要职务。刘杉博士任VVC标准联合主编。李翔博士任VVC标准参考软件联合主席。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG-5 EVC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MPEG-5 Essential Video Coding (EVC)是MPEG旗下另一个视频压缩标准。\",\n                            \"该标准包含一组免费工具集以及可以独立开关的增强工具。当所有增强工具都打开时，MPEG-5 EVC可以提供明显优于HEVC的压缩性能。\",\n                            \"Media Lab提出的针对屏幕内容编码工具已经被采纳入MPEG-5 EVC标准。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"AVS-3\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"音视频编码标准AVS工作组由原中国信息产业部科技司于2002年授权设立。\",\n                            \"AVS工作组已经制定出一系列成功的视频编码标准。其中，AVS-1与H.264/AVC性能相当，AVS-2与HEVC（H.265）性能相当。\",\n                            \"目前，AVS-3第一版本已经制定完成，相比AVS-2可以提供30%左右的码率节省。AVS-3第二版正在制定过程中，目标是比AVS-2节省40%的码率。\",\n                            \"Media Lab积极参与AVS-3的制定工作，牵头制定屏幕内容编码部分，实验室相关技术已经被AVS-3第二版本所采纳。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG-PCC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MPEG正在制定针对三维点云数据（点位置+点属性）的两个标准。\",\n                            \"第一个标准是V-PCC (ISO/IEC 23090-9)，基于视频编码的点云压缩。该标准首先将三维点云转换为二维视频信号，然后用已有视频压缩标准，如AVC，HEVC，VVC，进行压缩。\",\n                            \"第二个标准是G-PCC (ISO/IEC 23090-5），基于几何的点云压缩。这个标准更适合稀疏点云数据，直接对点云的三维几何信息以及相应点属性进行压缩编码。这个两个标准将被广泛用于如AR/VR，自动驾驶，地图，文化遗产，工业应用等多个相关领域。\",\n                            \"Media Lab积极参与这两个标准的研究制定工作，并在主导部分子项研究。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"AVS-PCC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"点云是一系列三维点的集合，可用于表示三维物体及场景。\",\n                            \"点云数据不只包括点的几何位置信息，也包括如颜色，反射率等的点的属性信息。已经被广泛应用于自动驾驶，地理信息系统，文化遗产保护等多个领域。\",\n                            \"AVS点云压缩专题组于2019年6月立项，预计点云压缩证据征集书将于2019年8月AVS会议后正式发布，点云压缩方案征集书将于2020年初发布。\",\n                            \"Media Lab积极参与AVS-PCC相关工作，并担任联合主席等重要位置。\"\n                        ]}\n                    ]\n                }\n            ]\n        },\n\n        // -- tab系统与传输 \n        tab2:{\n            title:\"系统与传输\",\n            list:[\n                {\n                    subTitle:\"OCP\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"开放计算项目（OCP)定义数据中心硬件及底层API。\",\n                            \"Media Lab聚焦于针对视频与点云压缩的硬件加速其及相应接口项目。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"IETF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"IETF致力于制定高质量的技术规范协议从而提高互联网的效率，并影响人们设计，使用以及管理互联网的方式。\",\n                            \"IETF制定的标准包括IP，TCP，UDP，HTTP等。\",\n                            \"Media Lab参与IETF相关工作，聚焦于媒体传输方面，如AVT，QUIC，MOPS。Wenger博士任IETF全球五位受托人之一。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"3GPP SA\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"3GPP是目前主导5G网络标准化的组织。\",\n                            \"Media Lab参与相关3GPP工作，主要参与系统架构组（SA plenary）和编解码器组（SA4)。系统架构组着重于支持各种应用，而编解码器组则负责制定各种视音频编解码器与传输协议栈的接口。同时Media Lab也积极跟进包括SA1，SA2，SA6以及其他核心技术组的相关讨论。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG SYSTEMS\",\n                    subContanier: [\n                        {sencTitle:\"OMAF\",sencText:[\n                            \"MPEG全景媒体格式标准（OMAF）定义了如何传输表示VR以及360度全景媒体内容。通过OMAF，用户以中心视角观察世界。\",\n                            \"OMAF第一版定义了如何将三维视频内容映射为二维视频内容以便于压缩，存储，封装以及基于HASH（Dynamic Adaptive Streaming over HTTP）或MTT（MPEG Media Transport）的传输。也定义了专门编解码器以及相应组合包含了关于媒体表示的一些档次。\",\n                            \"MPEG于2019年1月发布了OMAF第一版。目前正在进行OMAF第二版的工作。OMAF第二版将包含层叠，基于Tile的360度视频流媒体等内容。\",\n                            \"Media Lab深度参与OMAF相关工作。Choi博士任OMAF标准编辑。\"\n                        ]},\n                        {sencTitle:\"MPEG DASH\",sencText:[\n                            \"MPEG DASH （Dynamic Adaptive Streaming over HTTP）标准是一组在互联网上进行流媒体内容交互的规范。其核心部分定义了交互清单以及媒体内容分段的格式。交互清单描述了内容分段信息的位置以及码率信息。相应的，HASH客户端可以根据带宽情况自适应的选择不同码率的内容。分段信息格式则定义了分片的文件格式与媒体内容。\",\n                            \"MPEG DASH是跨平台的，可以支持任何音视频格式以及如HTTP1.1, HTTP2，组播，广播等协议。\",\n                            \"DASH在工业界被广泛使用，如ATSC3.0，DVB，HbbTV, 3GPP, VR-IF and CTA WAVE等。MPEG DASH包含多个部分。MPEG目前将要发布核心规范（Part 1）的第四个版本，同时也在制定扩展以及其他部分。\",\n                            \"Media Lab深度参与DASH相关工作，多项提案获得采纳。Sodagar博士是MPEG DASH主席，标准联合编辑。\"\n                        ]},\n                        {sencTitle:\"CMAF\",sencText:[\n                            \"MPEG CMAF通用媒体格式可以用于多种流媒体协议的编码格式。该标准提供一种适应于多码率的分片文件格式，可与包括DASH与HLS等多种协议结合使用。\",\n                            \"CMAF定义了文件格式限制，通用加密模式，编码限制等。同时也定义了通常使用到的媒体档次（编码器，档次，级别等）。通过CMAF服务提供商可以在单次压缩的基础上，将内容通过不同协议投递到不同平台上，从而降低编码以及CDN的成本。\",\n                            \"目前MPEG将发布第二版CMAF，并正在进行第三版的工作。4.Media Lab深度参与CMAF相关工作。Sodagar博士任CMAF联合主席。\"\n                        ]},\n                        {sencTitle:\"MPEG NBMP\",sencText:[\n                            \"MPEG NBMP （Network Based Media Processing）标准定义了如何在云平台上搭建，部署以及管理多媒体工作流程。通过该标准，服务提供商可以极为方便的制定多媒体工作流程，甚至无需开发代码。\",\n                            \"NBMP是跨平台的，可以用于公有、私有以及混合云。它建立工作流程，管理任务，监控媒体流水线，并回报给服务提供商。\",\n                            \"MPEG目前正在制定NBMP规范，计划于2020年上半年发布。\",\n                            \"Media Lab是NBMP的核心贡献者，多项技术得到采纳。Sodagar博士是NBMP标准的联合编辑。\"\n                        ]}\n                    ]\n                }\n            ]\n        },\n        //  -- tab工业联盟与论坛\n        tab3:{\n            title:\"工业联盟与论坛\",\n            list:[\n                {\n                    subTitle:\"MC-IF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"媒体编解码工业论坛MC-IF致力于推广MPEG标准。\",\n                            \"该论坛以VVC为起点，兼顾消费者与工业界，推广建立被广泛接受与使用的标准。\",\n                            \"Media Lab代表腾讯参与建立了该论坛。Wenger博士任董事，并主管专利生态工作组。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"DASH-IF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"DASH工业论坛致力于促进推广MPEG DASH标准。\",\n                            \"该论坛目前已有80家会员单位。自2012年，该论坛已经发布了多个指导说明，涵盖DASH实现、部署、针对服务器端加密服务的内容保护交换格式、测试集、点播和实时内容合规测试工具，DASH直播模拟器，封装工具，测试平台，dash.js开源客户端，以及多个白皮书。\",\n                            \"DASH工业论坛与MPEG DASH工作组，以及其他工业联盟如3GPP，DVB, W3C, ATSC, CTA WAVE and HbbTV等密切合作，推进DASH相关技术的使用。\",\n                            \"Media Lab在该论坛中主导贡献部分技术工作。Sodagar博士任该论坛主席，董事长。\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"8K Association\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"8K联盟致力于推动基于5G网络的8K视频生态系统。其目标是推广8K电视、8K内容在消费与专业市场的应用，制定相关技术需求与规范，并促进8K与5G生态系统内的交流以利于相应商业化。\",\n                            \"Media Lab被邀请成为主任会员以及候选董事会成员。Wenger博士和Choi博士目前负责协调同三星、英特尔、TCL的相关合作，共同领导8K生态系统。\"\n                        ]}\n                    ]\n                },\n            ]\n        },\n       },\n        \n\n    //  2.沉浸式新媒体页面\n    newMedia:{\n         title:\"沉浸式新媒体\",\n         text:\"着眼于前沿多媒体信息技术的发展，探索未来信息传递的多种可能性，研究采集、压缩、传输、播放等端到端完整技术链；深入理论而落地产业，将OMAF，DASH等标准研究成果融入产品，延展人类获取信息的方式与维度，从而带给人们沉浸式感官体验与全新认知世界方式。\",\n         explain:\"功能点\",\n         list:[\n             {\n                subTitle:\"采集\",\n                subContanier:\"对XR数据的采集技术进行深度细致的研究，针对不同的采集方式进行专业性拼接、投影等相关模块算法进行研究探讨，提升数据的质量并进行相应性能的优化。\"\n             },\n             {\n                subTitle:\"传输\",\n                subContanier:\"新媒体信息包含空间数据相较传统多媒体数据量有显著增加，使传输的质量与稳定性受到挑战。我们通过优化，实现降低整体传输带宽，同质量下相对于传统传输方式节省50%带宽。\"\n             },\n             {\n                subTitle:\"转码\",\n                subContanier:\"可兼顾不同终端的不同使用方式，在不同场景下，有选择性的对不同数据进行不同格式的压缩、处理、转码，以及多码率适配VR服务器方案。完成转码操作。\"\n            },\n            {\n                subTitle:\"播放\",\n                subContanier:\"支持HLS，DASH等传输标准，集成ABR，ROI，投影等不同算法的成果，并且可以适配PC端、移动端、小程序等多平台，同时可针对低延时的要求对视频进行不断优化。\"\n            },\n            {\n                subTitle:\"互动\",\n                subContanier:\"主要是通过与三方数据结合的模式，针对不同场景下与不同用户使用的不同特性提供客制化的3D互动功能，用户沉浸式感提升，使用户达到身临其境的效果。\"\n            },\n         ],\n\n         business:\"合作业务\",\n         businessList:[\n             {\n                subTitle:\"文旅-智慧小镇\",\n                subContanier:\"深度结合地方特色，客制化产业服务方式，与当地5G实验点相合作，提供全新导游导览体验。\"\n             },\n             {\n                subTitle:\"教育-VR教育\",\n                subContanier:\"TOB设计结构合作专业机构内容，通过VR展现方式，拓展学习场景，增强用户学习兴趣。\"\n            }\n         ]\n\n    },\n       \n\n    //   3.视频编解码页面\n    VideoCodingAndDecoding:{\n         title:\"视频编解码\",\n         text:\"视频内容占据着通信信息流量的绝大部分,原始视频数据量巨大，通常需要进行压缩处理之后才进行传输和存储。视频编解码技术致力于在保证质量的同时，将视频压缩得尽可能小。视频编码标准以及相关技术在不断地发展，并且在相关领域得到了广泛应用。\",\n         explain :\"功能点\",\n         list:[\n             {\n                subTitle:\"视频编解码技术\",\n                subContanier:\"我们拥有高效的H.264、H.265、AVS2实时编解码器，支持全平台，支持高清实时编码，压缩效率以及编解码性能业界领先，广泛应用于各种实时视频通信场景。\"\n             },\n             {\n                subTitle:\"硬件编解码技术\",\n                subContanier:\"我们支持PC及移动端各平台硬件编解码，设备兼容性好，支持软硬件编码智能切换。智能编码器切换算法充分考虑了压缩效率，编码速度等因素，极大地提升了用户体验。\"\n            },\n            {\n                subTitle:\"网络自适应编码技术\",\n                subContanier:\"视频编码与网络深度结合，根据当前网络状态动态调整帧率、分辨率和码率，同时主动调整编码参数，大幅提升编码的抗丢包能力。\"\n            },\n            // {\n            //     subTitle:\"动态编码策略\",\n            //     subContanier:\"根据终端设备的性能动态调整编码的复杂度，同时根据网络状况的波动对编码分辨率，帧率，码率等参数进行针对性的调整。\"\n            // },\n         ],\n\n         project:\"项目\",\n         projectName1:\"TPG\",\n         projectName2:\"TSE\"\n    },\n\n    \n\n    //  4.视频理解页面\n    videoUnderstanding:{\n          title:\"智慧媒体\",\n          text:\"搭建基于跨模态（图像、音频、文本、语音）时序算法的多媒体内容分析和理解框架，实现媒体标签，场景分类，事件检测，视频摘要，多媒体内容描述，多媒体内容检索等功能。\",\n          explain :\"功能点\",\n          list:[\n              {\n                subTitle:\"精彩视频\",\n                subContanier:\"分析视频中的视觉、音频、文字等多模态信息，并利用基于深度学习的时序建模技术，检测出精彩动作和事件，自动输出集锦视频。目前支持游戏、体育、综艺等垂类视频。\"\n              },\n              {\n                subTitle:\"智能封面\",\n                subContanier:\"利用精彩视频和图像美学评估技术，选出最优的关键帧或关键片段作为视频封面，提升视频点击转化及用户体验。目前支持游戏、体育、MV、短视频等垂类视频。\"\n            },\n            {\n                subTitle:\"场景识别\",\n                subContanier:\"利用基于深度学习的场景识别技术，对视频内容进行智能分析，捕捉视频中的人物、场景、语音、文字等信息，自动生成视频标签、类别等。\"\n            },{\n                subTitle:\"显著性检测\",\n                subContanier:\"利用基于深度学习的语义分割和目标检测技术，对视频的感兴趣区域进行检测和分割，精准定位视频中的人眼关注区域，为视频理解以及编码资源分配提供有利信息。\"\n            }\n          ]\n    },\n\n    \n\n    //   5.视频处理页面\n    videoHandle:{\n        title:\"视频处理\",\n        text:\"多年视频处理技术积累，从移动端到后台，从实时到离线，从视频到图片，从传统算法到人工智能，覆盖几乎所有视频处理的领域，同时对每种算法进行深度的性能和效果优化，输出真正能落地于产品的有价值的技术。\",\n\n\n        explain1:\"视频修复\",\n        list1:[\n            {\n                subTitle:\"智能降噪\",\n                subContanier:\"实时智能降噪：利用前帧的预测信息，根据噪声强度对当前帧进行智能降噪；离线智能降噪：结合深度学习及前后帧的预测信息，寻找最优匹配块进行联合降噪。\",\n            },\n            {\n                subTitle:\"去压缩失真\",\n                subContanier:\"基于深度学习的技术，在确保可保留主边缘并且恢复弱边缘细节的同时，去掉因编码压缩引入的块效应、边缘毛刺等artifact，以此可大幅度提升视频的主观质量。\"\n            },\n            {\n                subTitle:\"色彩还原\",\n                subContanier:\"利用深度学习的技术，对视频中受损的画面进行高强度修复，可以将视频受损的部分恢复到色彩最佳的状态，从而修复整个视频的色彩度，达到提升视频主观质量的目的。\"\n\n            },\n            {\n                subTitle:\"视频智能去抖动\",\n                subContanier:\"实时去抖动：移动端将防抖算法和硬件设备位置信息结合，对视频画面实时去抖动，使画面稳定；离线去抖动：智能检测抖动状况，结合前后帧信息对当前帧运动进行预估，改善抖动状况。\"\n            },\n            {\n                subTitle:\"去雾\",\n                subContanier:\"通过暗通道理论做基础，在效果上解决过曝和平坦区域的适应问题，并且适应每个视频的所有场景，使视频在性能上通过深度优化使算法几乎不会消耗资源。\"\n            }\n        ],\n        explain2:\"视频增强\",\n        list2:[\n            {\n                subTitle:\"智能细节增强\",\n                subContanier:\"主要是根据人眼的视觉特效，对人眼关注的地方和感兴趣的画面进行智能细节增强，可提升人眼关注区的画面质量，达到关注区细节更清晰的效果。更有利于视频编码。\"\n            },\n            {\n                subTitle:\"色彩增强\",\n                subContanier:\"通过播放器对不同视频原本色彩的展现，对视频中不同的的色彩进行自适应调整，提高视频中不同色彩的饱和度，使视频内容显得更丰富，提升人眼的视觉体验。\"\n            },\n            {\n                subTitle:\"超分辨率\",\n                subContanier:\"利用深度学习的技术，针对不同视频不同的场景，对视频场景进行深入优化，在提高视频分辨率的前提下，大量提升视频中画面的细节信息。达到超高分辨率。\"\n            },\n            {\n                subTitle:\"帧率上采样\",\n                subContanier:\"在视频播放期间，对视频的帧率进行多倍率的提升，从而使视频的画面细节更平滑，视频画面更细腻。提升视频画面的饱和度和细腻感，提高人眼观看视频的感官感受。\"\n            },\n            {\n                subTitle:\"暗场景增强\",\n                subContanier:\"利用最前沿的科研技术结合优秀的自研能力，通过对各种暗场景的效果优化，以及对实际应用场景的适配，达到对视频的实时处理能力，几乎不会消耗资源。\"\n            },\n            {\n                subTitle:\"美颜\",\n                subContanier:\"对人脸进行准确美化处理时，提升人面部中人眼、眉毛、嘴唇等五官的立体感、同时在保证背景依然清晰情况下，抹掉人脸上的瑕疵点，提升人眼肤色，使人脸肤质提升。\"\n            },\n            {\n                subTitle:\"滤镜\",\n                subContanier:\"满足各种不同场景下以及不同风格的滤镜，确保视频呈现效果较好的情况下，呈现不同的风格，让视频质量更贴近人眼视觉。提升视频的色彩饱和度或者丰富度。\"\n            },\n            {\n                subTitle:\"特效\",\n                subContanier:\"不同场景和不同风格的动漫特效让视频秒变漫画风，通过细腻的素描画风格让视频更有趣生动。提升视频内容的饱和度，可提升视频的趣味性，增加视频的点击率和播放量。\"\n            }\n        ]\n\n    },\n\n\n    //  6.感知交互页面\n    touch:{\n        title:\"异构平台\",\n        text:\"视频编解码和处理对计算能力提出了越来越多的需求，传统的单一架构已经无法满足我们对不同应用的性能需求，异构计算被认为是现阶段挑起视频处理计算大梁的关键技术，它最大的优点是具有比传统CPU并行计算更高效率和低延迟的计算性能。\",\n        explain :\"功能点\",\n        list:[\n            {\n                subTitle:\"视频转码\",\n                subContanier:\"将视频流从一种格式转化为一路或多路其他格式，支持不同格式，分辨率， 码率，帧率和其他设置等，能够支持实时直播和非实时点播等不同应用场景。\"\n            },\n            {\n                subTitle:\"面向视频编解码的视频处理技术\",\n                subContanier:\"通过紧密结合相对应的编解码信息，对需要处理的视频进行针对性的辅助处理，其中包括利用编解码信息对视频进行智能增强，以及利于编码的智能抹除对视频进行处理等技术。\"\n            },\n            {\n                subTitle:\"硬件平台支持\",\n                subContanier:\"能够支持不同的硬件平台， 包括CPU，GPU, FPGA, ASIC等，\"\n            }\n        ],\n        business:\"合作业务\",\n         businessList:[\n             {\n                subTitle:\"云游戏\",\n                subContanier:\"云游戏是以云计算为基础的游戏方式，在云游戏的运行模式下，所有游戏都在服务器端运行，并将渲染完毕后的游戏画面压缩后通过网络传送给用户。在客户端，用户的游戏设备不需要任何高端处理器和显卡，只需要基本的视频解压能力就可以了.\"\n             }\n         ]\n     },\n\n    //   7.音频处理页面\n    audioHandle:{\n          title:\"音频处理\",\n          text:\"针对多人，多场景实时通信系统中3A问题（降噪、回声抑制、增益控制），提供低复杂度、高鲁棒性解决方案，有效提升各种办公和移动端的声音体验。\",\n          explain:\"功能点\",\n          list:[\n              {\n                subTitle:\"音频引擎\",\n                subContanier:\"具有业界领先的语音音频编码系统，提供VoIP等不同场景下高效率、高质量语音频编解码方案。高度结合信道优化，提升网络抗性以及端到端的主观质量体验。\"\n              },\n              {\n                subTitle:\"机器学习\",\n                subContanier:\"将最新机器学习技术融合到音频业务中，消除各种杂音和非平稳噪声，增加语音清晰度和可懂度，应用覆盖了To B端到端的通话质量改善，和To C声音美化、语音转换等。\"\n            },\n            {\n                subTitle:\"音频工程\",\n                subContanier:\"构建高效率算法移值和工程化能力。满足真实场景复杂的应用问题，建立端到端的实时语音质量监控系统，交付覆盖服务器及移动客户端，提供优质音频解决方案。\"\n            },\n            {\n                subTitle:\"语音引擎\",\n                subContanier:\"设计提供多级码率-质量控制的信源编码器。提升流控下的信源编码器的灵活性，结合信道优化，提升网络抗性和端到端的主观质量体验。\"\n            },\n            {\n                subTitle:\"语音前处理技术——3A技术\",\n                subContanier:\"(1)声学回声消除(AEC),消除本地麦克风采集的播放信号，以免对端听到回声；(2)自适应降噪(ANS),消除麦克风采集到的环境中的噪声；(3)自动增益(AGC)，自动控制麦克风采集到的信号大小；\"\n            },\n            {\n                subTitle:\"AI智能降噪/分离\",\n                subContanier:\"主要通过深度学习技术，检测和去除混合在传播的语音信号中的噪声干扰信号，提高语音的质量、和语音的可懂度，提升用户听到声音的清晰度，改善用户听感。\"\n            },\n            {\n                subTitle:\"AI语言美化\",\n                subContanier:\"可通过对质量效果不好的歌声进行针对性的调整。主要是检测存在的错字，漏字等问题，改善歌声的音调，音准等，改善歌声传递过程中的清晰度，提高歌声的美感。\"\n            },\n            {\n                subTitle:\"AI声音转换\",\n                subContanier:\"通过深度学习的技术，可以将说话人的声音转换成某个特定人的声音。模仿其被转换入的声音音色、说话语调、说话方式等。可以使听者达到身临其境的声感。\"\n            },\n            {\n                subTitle:\"3D音效\",\n                subContanier:\"给原本单声道的声音赋予方位信息，加入环境处理来更真实体现的体现声音实际效果。比如将声音放置在左前，右后等不同位置，可以听到声音从不同方位传出，增强空间感。\"\n            },\n            {\n                subTitle:\"语言变声\",\n                subContanier:\"把说话人的声音转变为萝莉音，机器人音，卡通音等等，增加趣味性；可根据兴趣把说话人的声音进行转换，转变形式多变，种类多样。可转化为为萝莉音、机器人音、卡通音等不同音色的声音，以此达到更改说话人的声音的目的。增加声音阅读的趣味性。\"\n            },\n          ]\n    },\n\n   \n\n    // 8.实时音视频传输页面\n    transfer:{\n          title:\"实时音视频传输\",\n          text:\"超高清音视频的传输需要稳定的网络和充足的带宽，任何的网络波动都会对音视频质量造成影响。在实时通讯的场景下，如何高速地检测网络状态并根据网络状态制定合适的抗性和传输策略，一直是学术界和业界的难题。我们的团队具有丰富的学术界和业界经验，为此提供了能适应各种复杂场景的高品质高可靠性的解决方案。\",\n          explain:\"功能点\",\n          list:[\n              {\n                subTitle:\"拥塞控制\",\n                subContanier:\"我们调研了各种学术界和业界最新的拥塞控制算法，结合我们丰富的网络状态数据库，提出了新的实时拥塞控制算法，在不同网络场景下都能迅速给出可靠的带宽预测。\"\n              },\n              {\n                subTitle:\"损伤抗性\",\n                subContanier:\"网络故障种类繁多、千差万别，我们的智能抗性算法可以在极低的延时下抵御住各种突发性的网络波动及网络损伤，以此保持媒体播放的流畅性和清晰度，确保优质的播放体验。\"\n            },\n            {\n                subTitle:\"QoS/QoE最优化\",\n                subContanier:\"在网络带宽受限的情况下，清晰度、流畅性和延时不可兼得。我们根据应用的上下文和网络状态，实时自动做出最优的取舍，让最终用户能获得最好的体验。\"\n            },\n            {\n                subTitle:\"多人通讯流控\",\n                subContanier:\"多人通讯环境，既要保证重要通话的清晰度，又要兼顾他人的体验。我们使用了两套策略：在决策空间有限的情况下使用人工智能进行流控；在更复杂的场景下提供基于主观体验调整策略。\"\n            },\n            {\n                subTitle:\"网络度量\",\n                subContanier:\"网络策略的制定离不开现网大量数据支持。我们有丰富的去隐私数据集用来提取和学习网络的状态，可以在离线场景下复现差网状态，与在线场景下迅速判断网络状态。\"\n            },\n          ]\n    },\n\n\n    // 9.音视频通信质量评估页面\n    connectionQuality:{\n        title:\"音视频通信质量评估\",\n        text:\"我们研究基于深度学习的客观音视频质量评估算法，进行实现端到端的音视频内容质量评估。我们的研究内容涵盖多种多媒体格式，即衡量不同的编解码器在业务场景中的使用效果，又评估自研音视频前/后处理增强算法所带来的质量提升。我们以人的主观质量感受为目标，通过质量评估算法来指导音视频产品线的各个环节，最终提升用户体验。\",\n        explain:\"功能点\",\n        list:[\n            {\n                subTitle:\"音频无参考评估\",\n                subContanier:\"开发适用于实时音视频通话的音频质量评估算法。在网络受限情况下，衡量卡顿、延时和可懂度。在理想网络情况下，评估降噪、回声抑制、增益控制等语音增强技术的效果。\"\n            },\n            {\n                subTitle:\"图像/视频无参考评估\",\n                subContanier:\"针对实际业务中的高清源不可用场景，开发基于深度学习的无参考视频质量评估算法。我们开发的算法即适用于消费级相机所拍摄的视频，又能衡量视频质量增强算法所带来的质量提升。我们提出的算法已经落地在部分视频业务中。\"\n            },\n            {\n                subTitle:\"视频全参考质量评估\",\n                subContanier:\"针对视频高清源可用场景，使用深度学习技术来精确衡量视频质量，进而更好的平衡视频码流大小与视频观看体验。我们提出的算法在标准codec压缩失真数据集上取得了领先业界和学术界的性能。\"\n            },\n        ],\n        project:\"项目\",\n        projectName:\"音视频质量评测平台\",\n        projectText:\"提供自动化音视频质量测试能力，多方面对音视频质量进行压力测试\",\n        more: \"了解更多\"\n    },\n\n\n}","export default {\n    // 项目聚合页面\n    title:\"研究项目\",\n    textTPG:\"传统 CDN 集成 TPG 图片转码能力，保证视觉效果并降低图片流量/带宽，实现海量图片业务场景的节流。\",\n    textTSE:\"专门针对屏幕内容进行优化的编码器\",\n    textLiYing:\"新世代的视频服务平台，它以人眼视觉技术为基准，结合人工智能、大数据，对视频进行智能增强和压缩\",\n    pingtai:\"提供高效的自动化音视频质量测试能力，多方面对音视频质量进行全自动的压力测试\",\n    more:\"了解更多\",\n\n    nav:{\n        back:\"上一个项目 ：\",\n        next:\"下一个项目 : \"\n    },\n\n   \n    // TPG页面\n    tpg:{\n        textTPG:\"TPG是基于AVS2推出的一种图片压缩技术。压缩效率比JPG高47%，比PNG高60%，比GIF高85%，比WebP高25%。TPG功能全面，支持透明通道和动画，支持Windows，Linux，Mac，iOS，Android等平台。目前，TPG已经作为AVS-P7部分，申报成为国家标准。\",\n        partner:\"合作伙伴\",\n        itemTitle1:\"TPG压缩效率对比\",\n        itemText1:\"相同图片质量下，TPG图片压缩效率比JPG高 47%， 比PNG高60%，比Gif高85%，比WebP高25%\",\n        itemTitle2:\"TPG编码效率对比\",\n        itemText2:\"在PSNR质量对齐下，TPG的编码效率比 webP 高23.5%，比 libjpeg 高46.8%\",\n        itemTitle3:\"TPG解码性能对比\",\n        itemText3Item1:\"1.iOS平台下 TPG 的解码耗时是webP的2倍，是libjpeg的2.3倍\",\n        itemText3Item2:\"2.Android平台下 TPG 的解码耗时是 webP 的1.2倍，是 libjpeg 的1.6倍\",\n        itemTitle4:\"TPG解码内存对比\",\n        itemText4Item1:\"1.iOS平台下 TPG 的解码内存是 webP 的 2 倍，是libjpeg的 2.5 倍\",\n        itemText4Item2:\"2.Android平台下 TPG 的解码内存是 webP 的 2 倍，是 libjpeg 的 4 倍\",\n        itemTitle5:\"TPG应用场景\",\n        itemText5:{\n            text1:{\n                one:\"社交类应用\",\n                two:\"1.空间相册\",\n                // three:\"2.朋友圈\",\n                four:\"2.群图片\",\n                // five:\"4....\"\n            },\n            text2:{\n                one:\"新闻类应用\",\n                two:\"1.H5页面\",\n                three:\"2.新闻客户端\",\n                four:\"3.浏览器\",\n            },\n            text3:{\n                one:\"游戏类应用\",\n                two:\"1.安装包\",\n                three:\"2.游戏UI资源\",\n            }\n        },\n        partners:\"TPG合作伙伴\" \n    },\n\n    // TES页面\n    tse:{\n       tesText:\"TSE是专门针对屏幕内容推出的编码器，TSE在普通编码器的基础上增加了对帧内块复制（Intra Block Copy）和调色板编码（Palette Mode）的支持，压缩效率相比x265-medium模式提升55%以上，同时编码速度比x265-medium快近10倍，能够满足实时场景应用需求。\",\n       itemTitle1:\"TSE与X265压缩效率对比\",\n       itemText1one:\"摄像头采集序列：TSE的编码效率相比于x265-ultrafast模式有20%左右提升\",\n       itemText1two:\"屏幕内容序列：TSE相比于x265-ultrafast模式的提升在70%以上；对于x265-medium模式有55%左右的提升\",\n       itemTitle2:\"TSE与X265编码速度对比\",\n       itemText2one:\"摄像头采集序列：TSE的平均编码耗时是x265-ultrafast的88%左右\",\n       itemText2two:\"屏幕内容序列：不打开IBC和PLT，TSE的平均编码耗时仅为x265-ultrafast的33%左右； 打开IBC和PLT，TSE的平均编码耗时是x265-ultrafast的50%左右\", \n       partner:\"TSE合作伙伴\",\n    },\n\n    // 腾讯丽影\n    liYing:{\n       title:\"腾讯丽影\",\n       textLiYing:\"腾讯丽影是新世代的视频服务平台，它以人眼视觉技术为基准，结合人工智能、大数据，对视频进行智能增强和压缩。\",\n       vidio:\"丽影宣传视频\",\n       itemTitle1:\"人眼视觉技术 - 关注区检测\",\n       itemTitle2:\"人眼视觉技术 - 去压缩失真 \",\n       itemTitle3:\"人眼视觉技术 - 超分辨率\",\n       itemTitle4:\"人眼视觉技术 - 图片修复增强\",\n       partner:\"腾讯丽影合作伙伴\", \n       before:\"原图\",\n       after:\"处理后\"\n    },\n\n\n    // 音视频质量测评平台\n    vidioAndaudioTest:{\n        title:\"音视频质量评测平台\",\n        // 缺\n        pingtaiText:\"TMEC是一站式的音视频质量评估平台，可有效提升音视频产品专项测试工作效率。该平台集成了多种商用设备及自研工具，借助其可以高效完成实时音视频产品在不同网络场景下的评测工作。\",\n        itemText1:\"音视频相关产品的专项质量测试往往需要借助专业的实验室环境来完成，测试过程中需要使用网络模拟、测试仪表等专用工具，同时需要测试人员有一定的背景知识及操作经验，测试门槛和测试成本都相对较高。TMEC将实验室相关资源进行整合，实现任务远程提交、自动化执行等能力，使非专业团队也能快速，准确的完成相关评估需求\",\n        itemText2:\"TMEC的设计理念是提供通用的、准确的、公平的测试能力与测试方法论，在各个指标的测量方法上尽可能选择最可靠的技术方案，追求可以服务不同产品（竞品）的最大普适性。除端到端的实时音视频通话测试能力外，平台还提供了视频有参评估、无参清晰度评估、语音有参评估、语音回声评估等云端评估工具供用户使用，便于非实时场景下的质量评估需求。\",\n    }\n\n}\n","export default {\n    // 解决方案聚合页面\n    title:\"解决方案\",\n    list:[\n        {\n            title:\"腾讯实时音视频(TRTC)\",\n            text:\"拥有QQ十几年来在音视频技术上的积累，致力于帮助企业快速搭建低成本、高品质音视频通讯能力的完整解决方案\",\n        },\n        {\n            title:\"互动直播解决方案(ILVB)\",\n            text:\"一站式“多路音视频互动”解决方案，主打直播连麦和多画面特效，通过移动直播 SDK 打造跨平台的超清酷炫直播场景\",\n        },\n        {\n            title:\"腾讯云游戏多媒体引擎(GME)\",\n            text:\"提供一站式游戏语音解决方案。针对不同游戏场景进行深度优化，功能完备，接入门槛低，一个 SDK 即可满足多样化的游戏语音需求\",\n        },\n    ]\n}","export default {\n    bannerTitle:\"关于我们\",\n    title:\"腾讯多媒体实验室\",\n    text:\"腾讯旗下顶尖的音视频通信和处理研发团队，专注于实时音视频通信、音视频编解码前沿算法研究、音视频国际标准、计算机视觉图像处理、端到端音视频质量评测。在实时音视频通信和处理技术、音视频国际标准等领域积累了完整的解决方案和领先的技术水平。\",\n    members:\"\",\n    sideNavBarTitle:[\"实验室介绍\",\"相关论文\",\"加入我们\"],\n        \n    //关于我们-实验室介绍\n    team: \"专家成员\",\n    expertList:[\n        {\n          id: 1,\n          imgClass: \"icon-about_image_expert_0\",\n          name:\"刘杉 博士\",\n          text:\"本科毕业于清华大学电子工程系，硕士和博士毕业于美国南加州大学电机工程系，现任腾讯集团杰出科学家和多媒体实验室负责人。加入腾讯之前，刘杉博士曾任职多家全球500强和国际知名企业并担任高级技术和管理职务。自2017年加入腾讯，组建标准团队并代表腾讯在多媒体系统，数据压缩和网络传输协议等行业标准领域取得突破性成果。同时，带领团队研发和交付多项多媒体核心技术，全面上线腾讯会议，投屏，微视，企鹅电竞等产品并持续服务QQ空间和腾讯视频等业务，进而积极投入开源云平台和生态建设。刘杉博士深耕多媒体数据压缩、传输协议和系统、无线网络、IoT等行业标准领域，曾多次担任国际标准专家小组主席和联席主席，并多次在国际顶级学术会议担任领域主席或做邀请报告。是超过60篇学术期刊和会议论文的作者和超过200个美国和全球专利申请的发明人。是已定稿国际标准H.265/HEVC v4和正在研发的下一代国际视频编解码标准VVC的联合主编。于2014-2015年担任IEEE Signal Processing Society工业关系委员会委员；2016-2017担任Asia-Pacific Signal and Information Processing Association (APSIPA) 副总裁主管工业关系和发展。于2019年获得APSIPA工业领袖称号。\" \n        },\n        {\n          id: 2,\n          imgClass: \"icon-about_image_expert_1\",\n          name:\"李翔 博士\",\n          text:\" 本科硕士毕业于清华大学电子工程系，博士毕业于德国纽伦堡-爱尔兰根大学电气电子工程与通信系，现任腾讯多媒体实验室视频标准负责人，资深专家研究员。加入腾讯之前，他曾供职于美国高通公司，德国西门子公司。李翔博士深耕视频压缩领域多年，是国际视频压缩标准的积极贡献者，并担任包括JEM参考软件联席主席、VVC参考软件联席主席、MPEG-5 EVC标准联合编辑等多个专家小组主席/联席主席职位。李翔博士是IEEE高级会员，在相关领域顶级期刊和学术会议发表论文40多篇，提交标准提案300余个，持有美国授权专利及公开专利申请120余件。\" \n        },\n        {\n          id: 3,\n          imgClass: \"icon-about_image_expert_2\",\n          name: \"赵欣 博士\",\n          text: \"2006年本科毕业于清华大学电子工程系，之后进入中科院计算所继续深造，并于2012年获得工科博士学位。2017年他加入腾讯，任职专家研究员，专注于下一代VVC视频编码标准的研究和制定工作，并带领团队研发新视频标准。在此之前，于2012年至2017年间，他就职位于美国加利福尼亚州圣地亚哥市的高通公司，任职主任工程师。自2009年以来，他积极参与国际视频标准及其扩展标准的制定工作，包括HEVC，3D-AVC，3D-HEVC以及由联合视频专家组（JVET）发起的下一代视频压缩标准VVC。赵欣在视频编码和处理技术领域深耕超过12年，在该领域有着丰富的从业经验。在新一代国际视频编码标准组织JVET中，他担任了多项重要角色，其中包括主持核心技术实验及担任多项BoG和AhG的主席。他有超过260项专利申请，其中包括20多项已授权专利，已发表30多篇著名国际期刊及会议论文。他目前的研究兴趣包括图像及视频压缩，视频处理。\"\n        },\n        {\n          id: 4,\n          imgClass: \"icon-about_image_expert_3\",\n          name:\"Stephan Wenger 博士\",\n          text:\"Stephan Wenger博士于2018年加入腾讯美国，现任知识产权与标准高级总监。加入腾讯之前，他曾任统一协作解决方案提供商Vidyo公司知识产权与标准副总裁。他曾就职于诺基亚知识产权部和研究中心。他曾服务于媒体压缩领域的初创公司，担任UB Video和eBrisk Video公司董事直至两公司于2006和2016年被收购。他同时活跃于多媒体技术标准化的多个组织，如IETF、ITU-T和MPEG。Wenger博士持有40多个已授权专利以及更多的专利申请。他分别于1989年和1995年在柏林工业大学取得计算机科学硕士和博士学位。\" \n        },\n        {\n          id: 5,\n          imgClass: \"icon-about_image_expert_4\",\n          name:\"Iraj Sodagar 博士\",\n          text:\"Iraj Sodagar 于2018年11月加入腾讯多媒体实验室，现任专家研究员。加入腾讯之前，他在微软担任多媒体系统主任架构师。在微软期间，他进行了多个多媒体技术的研究和产品项目，负责Windows多媒体交付策略，标准和产品的对接，以及协调微软内部流媒体技术标准化工作。在过去的25年中，Sodagar博士参与、领导和管理了多个研发项目、先进的架构设计以及产品开发，包括图片与视频编码，媒体索引、分析、存储、传输、交付已及云端转码。他建立并管理技术团队进行研究，软件开发与产品化，以及成熟技术的标准化。他在多个标准组织中担任领导职位，如in ITU-T, JPEG, MPEG, AMQP, W3C, DVB, 3GPP, W3C, CTA WAVE, DLNA 以及AOMedia。他在21世纪初开创了第一个关于移动视频交付的工业论坛——无线媒体论坛。自MPEG  DASH开始以来，他一直担任主席职务。他建立了DASH工业论坛，并任主席和董事长。此外，他担任MPEG CMAF联席主席至CMAF第一版本发布。Sodagar博士于1994年在亚特兰大的佐治亚理工学院电气工程系取得博士学位。\" \n        },\n        {\n          id: 6,\n          imgClass: \"icon-about_image_expert_5\",\n          name:\"许晓中 博士\",\n          text:\"许晓中博士于2017年加入腾讯多媒体实验室。他本科和博士均毕业于清华大学电子工程系。从2004年起，开始从事视频编码领域的研究及标准化的工作。他参与进行的标准制定工作包括H.264/AVC, H.265/HEVC, H.266/VVC, MPEG-5, ITU-T IPTV, AVS等一系列国际国内视频标准。在这些标准化组织中，他提交了超过100个技术提案，并有多于20项技术提案被各个标准化组织所采纳。许博士积极推动行业标准的发展，目前担任HEVC SCC, VVC 及AVS等标准组织的屏幕内容编码专题小组的联合主席，MPEG-5标准文本联合编辑等职务。\" \n        },\n        {\n          id: 7,\n          imgClass: \"icon-about_image_expert_6\",\n          name:\"朱斌 博士\",\n          text:\"毕业于爱荷华州立大学取得电子和计算机工程博士学位。毕业后先后就职于硅谷的几家知名半导体初创公司和Intel，Apple带领技术团队负责视频核心技术开发，期间也有2年独立创业经历。2018年12月加入公司以来，带领团队负责下一代视频编解码国际标准VVC/H.266编解码器 的实现，优化和开源协同。\" \n      }\n    ],\n\n\n    //关于我们-加入我们\n    contactTitle: \"若对以下任意职位有意向，请联系\",\n    contactEmail: \"medialab@tencent.com\",\n    positionList: [\n      {\n        id: 1,\n        positionName: \"视频内核开发高级工程师\",\n        address: \"工作地点：深圳\",\n        duty: {\n          title: \"工作职责:\",\n          content: [\n            \"- 负责H264、H265编码算法深度调优；\",\n            \"- 跟进业界前沿编码标准制定进展，如H266/AV1等。\"\n          ]\n        },\n        jobRequirements: {\n          title: \"岗位要求:\",\n          content: [\n            \"- 熟悉视频编解码底层算法，如H264/HEVC等；\",\n            \"- 有汇编优化，GPU加速开发经验者优先；\",\n            \"- 精通ffmpeg，x264，x265等开源框架，有移动硬件加速开发能力；\",\n            \"- 扎实的数学、数据结构基础，优秀的英文阅读能力，沟通交流能力强，工作积极主动，良好的团队协作能力\"\n          ]\n        }\n      },\n      {\n        id: 2,\n        positionName: \"视频处理高级工程师\",\n        address: \"工作地点：深圳\",\n        duty: {\n          title: \"工作职责:\",\n          content: [\n            \"- 视频处理算法研究开发, 包括：降噪, 压缩失真还原, 色彩及边缘增强, 视频倍帧等；\",\n            \"- 熟悉视频处理时域建模技术, 包含深度学习以及时域信号处理框架；\",\n            \"- 针对不同x86及ARM平台优化各类视频处理演算法；\",\n            \"- 了解各类视频增强技术的量化评估方法及标准；\"\n          ]\n        },\n        jobRequirements: {\n          title: \"岗位要求:\",\n          content: [\n            \"- 精通视频处理，图像处理，数字信号处理，以及视频增强评估方法；\",\n            \"- 有视频编解码算法相关经验者优先；\",\n            \"- 在CNN深度学习领域有丰富经验；\",\n            \"- 有较强的学术比赛经验或者在重要数据集的Leaderboard上排名靠前，比如ImageNet等学术数据集或者Kaggle等一些国内外商业比赛者优先，或来自国内外计算机视觉/计算机图形学/机器学习等领域内知名实验室；\",\n            \"- 精通 C++，Python 程序开发及软件开发流程，熟悉计算机演算法及资料结构；\",\n            \"- 有手机平台（iOS、Android）开发及性能优化经验者优先；\",\n            \"- 沟通交流能力强，良好的团队合作精神，工作积极主动；\"\n          ]\n        }\n      },\n      {\n        id: 3,\n        positionName: \"视频网络高级工程师\",\n        address: \"工作地点：深圳\",\n        duty: {\n          title: \"工作职责:\",\n          content: [\n            \"- 负责音视频全球实时通话网络部分算法研究与开发。\",\n          ]\n        },\n        jobRequirements: {\n          title: \"岗位要求:\",\n          content: [\n            \"- C/C++开发能力，熟悉TCP/IP协议栈，有高负载网络应用APP的开发经验；\",\n            \"- 熟悉音视频编解码技术，了解音视频开源项目如FFmpeg，VLC、WebRTC、X264等，熟悉网络音视频通话的核心技术，如JITTER BUFFER，FEC，带宽预测和码率自适应技术，有网络音视频通话项目经验者优先；\",\n            \"- 熟悉网络部署、运营，有海外加速、CDN运维、边缘计算者优先;\",\n            \"- 熟悉网络度量体系，对网络测速的原理、协议和系统实现有深入研究者优先。\"\n          ]\n        }\n      },\n      {\n        id: 4,\n        positionName: \"音频算法研究员\",\n        address: \"工作地点：深圳\",\n        duty: {\n          title: \"工作职责:\",\n          content: [\n            \"- 负责音频领域算法的研究和开发，涉及的问题包括但不限于：语音增强领域的回声抵消&噪声抑制、说话人转换、麦克风阵列技术等。\",\n          ]\n        },\n        jobRequirements: {\n          title: \"岗位要求:\",\n          content: [\n            \"- 具有计算机工程、感知科学、计算语言学、物理学、数学或者相关领域的硕士或者博士学位；\",\n            \"- 对机器学习、深度学习有较深理解和应用，能独立开展研发工作；\",\n            \"- 熟练的C/C++编程技巧，掌握一门脚本语言，如Python。具备将机器学习算法在不同的移动平台上进行移植适配，性能优化的能力；\",\n            \"- 对音频信号处理和内容分析有较好的研究和开发经验；\",\n            \"- 在模式识别、机器学习、语言学或者信号处理领域有丰富经验；\",\n            \"- 有较强的学术比赛经验或者在重要数据集的Leaderboard上排名靠前，比如ImageNet等学术数据集或者Kaggle等一些国内外商业比赛者优先，或来自国内外音频信号处理实验室/机器学习/数据挖掘等领域内知名实验室；\",\n            \"- 优秀的书面和口头沟通能力，具有团队精神。\"\n          ]\n        }\n      },\n      {\n        id: 5,\n        positionName: \"音频内核高级开发工程师\",\n        address: \"工作地点：深圳\",\n        duty: {\n          title: \"工作职责:\",\n          content: [\n            \"- QQ与腾讯云音频通话核心编解码算法、前后处理算法研究，跨平台音频引擎研发，语音识别相关的前处理算法研究开发，包括：手机多平台声音采集播放；前后处理如回声抵消、噪声抑制、静音检测、自动增益等；网络传输适配包括FEC策略、缓冲控制、错误掩盖等；编解码器核心算法针对不同平台优化。\",\n          ]\n        },\n        jobRequirements: {\n          title: \"岗位要求:\",\n          content: [\n            \"- 计算机、通信等相关专业硕士以上学历，2年及以上工作经验；\",\n            \"- 精通数字信号处理，熟悉相关音频编解码标准以及音频编解码核心算法；\",\n            \"- 有回声抵消、噪声抑制、自动增益控制、频响均衡等其他语音前后处理及增强算法相关经验；\",\n            \"- 有基于机器学习的语音增强相关经验者优先；\",\n            \"- 了解音频网络传输及Qos控制；\",\n            \"- 精通c/c++程序开发及软件开发流程；\",\n            \"- 有手机平台（iOS、Android）开发及性能优化经验者优先；\",\n            \"- 沟通交流能力强，良好的团队合作精神，工作积极主动。\"\n          ]\n        }\n      }\n    ],\n\n\n    //关于我们-相关论文\n    navList: [\"全部\",\"语音音频\",\"网络传输优化\",\"图像质量评估\",\"视频编解码\",\"计算机视觉\",\"3D点云\"],\n    Total: '共 ',\n    item: ' 条',\n    thesisList:[\n      [\n        [22,\"all\"],\n        [\n          [\n            {\n              id:'all-3-5',\n              title: \"Multiple Reference Line Coding for Most Probable Modes in Intra Prediction\",\n              author: \"Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand\",\n              time: \"2019\",\n              content: \"This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.\",\n              src: \"https://ieeexplore.ieee.org/document/8712640\"\n            },\n            {\n              id:'all-3-4',\n              title: \"Wide Angular Intra Prediction for Versatile Video Coding\",\n              author: \"Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape\",\n              time: \"2019\",\n              content: \"This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).\",\n              src: \"https://ieeexplore.ieee.org/document/8712681\"\n            },\n            {\n              id:'all-3-1',\n              title: \"Blind image quality assessment based on joint log-contrast statistics\",\n              author: \"Yabin Zhang et at.\",\n              time: \"2019\",\n              content: \"During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.\",\n              src: \"https://www.sciencedirect.com/science/article/pii/S0925231218313432\"\n            },\n            {\n              id:'all-4-1',\n              title: \"Fast Adaptive Multiple Transform for Versatile Video Coding\",\n              author: \"Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu\",\n              time: \"2019\",\n              content: \"The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.\",\n              src: \"https://ieeexplore.ieee.org/document/8712650\"\n            },\n            {\n              id:'all-4-2',\n              title: \"Recent advances in video coding beyond the HEVC standard\",\n              author: \"Xiaozhong Xu, Shan Liu\",\n              time: \"2019\",\n              content: \"The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.\",\n              src: \"https://doi.org/10.1017/ATSIP.2019.11\"\n            },\n            \n          ],\n          [\n            {\n              id:'all-4-3',\n              title: \"Current Picture Referencing in Versatile Video Coding\",\n              author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n              time: \"2019\",\n              content: \"Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.\",\n              src: \"https://ieeexplore.ieee.org/document/8695359\"\n            },\n            {\n              id:'all-5-1',\n              title: \"High Performance Gesture Recognition via Effective and Efficient Temporal Modeling\",\n              author: \"Yang Yi, Feng Li, et al.\",\n              time: \"2019\",\n              content: \"State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization\",\n              src: \"https://www.ijcai.org/proceedings/2019/0141.pdf\"\n            },\n            {\n              id:'all-5-2',\n              title: \"A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.\",\n              author: \"Yabin Zhang, et al\",\n              time: \"2019\",\n              content: \"3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.\",\n              src: \"https://ieeexplore.ieee.org/document/8704253\"\n            },\n            {\n              id:'all-1-1',\n              title: \"A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network\",\n              author: \"sarahqwang\",\n              time: \"2018\",\n              content: \"The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8706582\"\n            },\n            {\n              id:'all-2-1',\n              title: \"Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy\",\n              author: \"zeqilai\",\n              time: \"2018\",\n              content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in today’s network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps’ performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.\",\n              src: \"https://dl.acm.org/citation.cfm?id=3318101\"\n            }\n          ],\n          [\n            {\n              id:'all-3-2',\n              title: \"Intra Block Copy for Next Generation Video Coding\",\n              author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n              time: \"2018\",\n              content: \"Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.\",\n              src: \"https://ieeexplore.ieee.org/document/8551528\"\n            },\n            {\n              id:'all-3-3',\n              title: \"Coupled Primary and Secondary Transform for Next Generation Video Coding\",\n              author: \"Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu\",\n              time: \"2018\",\n              content: \"The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.\",\n              src: \"https://ieeexplore.ieee.org/document/8698635\"\n            },\n            {\n              id:'all-1-5',\n              title: \"Gaussian density guided deep neural network for single-channel speech enhancement\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8168116\"\n            },\n            {\n              id:'all-1-3',\n              title: \"A maximum likelihood approach to deep neural network based speech dereverberation\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8282019\"\n            },\n            {\n              id:'all-1-2',\n              title: \"A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.\",\n              src: \"https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF\"\n            }\n          ],\n          [\n            {\n              id:'all-1-4',\n              title: \"A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemale–female, male–male, and female–male mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/7917323\"\n            },\n            {\n              id:'all-2-2',\n              title: \"Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.\",\n              src: \"https://ieeexplore.ieee.org/document/8117568\"\n            },\n            {\n              id:'all-2-3',\n              title: \"Furion: Engineering high-quality immersive virtual reality on today's mobile devices\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).\",\n              src: \"https://dl.acm.org/citation.cfm?id=3117815\"\n            },\n            {\n              id:'all-4-4',\n              title: \"TPG Image Compression Technology\",\n              author: \"ShitaoWang、PiaoDing、XiaozhengHuang、HanjunLiu、BinjiLuo、XinxingChen、YoubaoWu、RonggangWang\",\n              time: \"\",\n              content: \"TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression\",\n              src: \"http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238\"\n            },\n            {\n              id:'all-4-5',\n              title: \"Saliency detection with two-level fully convolutional networks\",\n              author: \"Yang Yi, et al.\",\n              time: \"2017\",\n              content: \"This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.\",\n              src: \"https://ieeexplore.ieee.org/document/8019309/\"\n            },\n          ],\n          [\n            {\n              id:'all-2-4',\n              title: \"Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.\",\n              src: \"https://ieeexplore.ieee.org/document/7898362\"\n            },\n            {\n              id:'all-2-5',\n              title: \"Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of user’s input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.\",\n              src: \"https://dl.acm.org/citation.cfm?id=3234201\"\n            }\n          ]\n        ]\n      ],\n      [\n        [5,\"audio\"],\n        [\n          [\n            {\n              id:'audio-1-1',\n              title: \"A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network\",\n              author: \"sarahqwang\",\n              time: \"2018\",\n              content: \"The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8706582\"\n            },\n            {\n              id:'audio-1-2',\n              title: \"A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.\",\n              src: \"https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF\"\n            },\n            {\n              id:'audio-1-3',\n              title: \"A maximum likelihood approach to deep neural network based speech dereverberation\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8282019\"\n            },\n            {\n              id:'audio-1-4',\n              title: \"A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemale–female, male–male, and female–male mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/7917323\"\n            },\n            {\n              id:'audio-1-5',\n              title: \"Gaussian density guided deep neural network for single-channel speech enhancement\",\n              author: \"yannanwang\",\n              time: \"2017\",\n              content: \"Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.\",\n              src: \"https://ieeexplore.ieee.org/abstract/document/8168116\"\n            }\n          ]\n        ]\n      ],\n      [\n        [5,\"network\"],\n        [\n          [\n            {\n              id:'network-1-1',\n              title: \"Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy\",\n              author: \"zeqilai\",\n              time: \"2018\",\n              content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in today’s network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps’ performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.\",\n              src: \"https://dl.acm.org/citation.cfm?id=3318101\"\n            },\n            {\n              id:'network-1-2',\n              title: \"Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.\",\n              src: \"https://ieeexplore.ieee.org/document/8117568\"\n            },\n            {\n              id:'network-1-3',\n              title: \"Furion: Engineering high-quality immersive virtual reality on today's mobile devices\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).\",\n              src: \"https://dl.acm.org/citation.cfm?id=3117815\"\n            },\n            {\n              id:'network-1-4',\n              title: \"Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.\",\n              src: \"https://ieeexplore.ieee.org/document/7898362\"\n            },\n            {\n              id:'network-1-5',\n              title: \"Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing\",\n              author: \"zeqilai\",\n              time: \"2017\",\n              content: \"We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of user’s input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.\",\n              src: \"https://dl.acm.org/citation.cfm?id=3234201\"\n            },\n          ]\n        ]\n      ],\n      [\n        [1,\"quality\"],\n        [\n          [\n            {\n              id:'quality-1-1',\n              title: \"Blind image quality assessment based on joint log-contrast statistics\",\n              author: \"Yabin Zhang et at.\",\n              time: \"2019\",\n              content: \"During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.\",\n              src: \"https://www.sciencedirect.com/science/article/pii/S0925231218313432\"\n            }\n          ]\n        ]\n      ],\n      [\n        [8,\"video\"],\n        [\n          [\n            {\n              id:'video-1-3',\n              title: \"Wide Angular Intra Prediction for Versatile Video Coding\",\n              author: \"Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape\",\n              time: \"2019\",\n              content: \"This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).\",\n              src: \"https://ieeexplore.ieee.org/document/8712681\"\n            },\n            {\n              id:'video-1-4',\n              title: \"Multiple Reference Line Coding for Most Probable Modes in Intra Prediction\",\n              author: \"Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand\",\n              time: \"2019\",\n              content: \"This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.\",\n              src: \"https://ieeexplore.ieee.org/document/8712640\"\n            },\n            {\n              id:'video-1-5',\n              title: \"Fast Adaptive Multiple Transform for Versatile Video Coding\",\n              author: \"Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu\",\n              time: \"2019\",\n              content: \"The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.\",\n              src: \"https://ieeexplore.ieee.org/document/8712650\"\n            },\n            {\n              id:'video-2-1',\n              title: \"Recent advances in video coding beyond the HEVC standard\",\n              author: \"Xiaozhong Xu, Shan Liu\",\n              time: \"2019\",\n              content: \"The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.\",\n              src: \"https://doi.org/10.1017/ATSIP.2019.11\"\n            },\n            {\n              id:'video-2-2',\n              title: \"Current Picture Referencing in Versatile Video Coding\",\n              author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n              time: \"2019\",\n              content: \"Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.\",\n              src: \"https://ieeexplore.ieee.org/document/8695359\"\n            }\n          ],\n          [\n            {\n              id:'video-1-2',\n              title: \"Coupled Primary and Secondary Transform for Next Generation Video Coding\",\n              author: \"Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu\",\n              time: \"2018\",\n              content: \"The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.\",\n              src: \"https://ieeexplore.ieee.org/document/8698635\"\n            },\n            {\n              id:'video-1-1',\n              title: \"Intra Block Copy for Next Generation Video Coding\",\n              author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n              time: \"2018\",\n              content: \"Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.\",\n              src: \"https://ieeexplore.ieee.org/document/8551528\"\n            },\n            {\n              id:'video-2-3',\n              title: \"TPG Image Compression Technology\",\n              author: \"ShitaoWang、PiaoDing、XiaozhengHuang、HanjunLiu、BinjiLuo、XinxingChen、YoubaoWu、RonggangWang\",\n              time: \"2017\",\n              content: \"TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression\",\n              src: \"http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238\"\n            }\n          ]\n        ]\n      ],\n      [\n        [2,\"computer\"],\n        [\n          [\n            {\n              id:'computer-1-2',\n              title: \"High Performance Gesture Recognition via Effective and Efficient Temporal Modeling\",\n              author: \"Yang Yi, Feng Li, et al.\",\n              time: \"2019\",\n              content: \"State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization\",\n              src: \"https://www.ijcai.org/proceedings/2019/0141.pdf\"\n            },\n            {\n              id:'computer-1-1',\n              title: \"Saliency detection with two-level fully convolutional networks\",\n              author: \"Yang Yi, et al.\",\n              time: \"2017\",\n              content: \"This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.\",\n              src: \"https://ieeexplore.ieee.org/document/8019309/\"\n            }\n          ]\n        ]\n      ],\n      [\n        [1,\"dianYun\"],\n        [\n          [\n            {\n              id:'dianYun-1-1',\n              title: \"A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.\",\n              author: \"Yabin Zhang, et al\",\n              time: \"2019\",\n              content: \"3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.\",\n              src: \"https://ieeexplore.ieee.org/document/8704253\"\n            } \n          ]\n        ]\n      ]\n    ]\n  }\n\n\n","import homeLanguge from \"./pages/zh/homePageLanguge\"\nimport researchLanguge from \"./pages/zh/researchPageLanguge\"\nimport projectLanguge from \"./pages/zh/projectPageLanguge\"\nimport solutionLanguge from \"./pages/zh/solutionPageLanguge\"\nimport aboutLanguge from \"./pages/zh/aboutPageLanguge\"\n\n\n// 分别引入 首页，研究，项目，解决方案，关于我们页面的语言包\n\n\n\n\n\nexport default {\n  // 导航条\n    nav: {\n      home: \"首页\",\n      research:\"研发领域\",\n      project:\"研究项目\",\n      solution:\"解决方案\",\n      about:\"关于我们\",\n      title:\"腾讯音视频实验室\"\n    },\n\n    // 五个页面的语言包\n    homeLanguge,\n    researchLanguge,\n    projectLanguge,\n    solutionLanguge,\n    aboutLanguge\n\n\n\n\n\n\n    // 首页\n    // home:{\n      //  研究领域\n    //   \tresearch:{\n    //        researchAndMade:{\n    //          title:\"国际标准化研究与制定\",\n    //          text:\"专注媒体压缩，系统传输等多媒体领域标准研究和制定\"\n    //         },\n    //        touch:{  \n    //          title:\"交互与感知\",\n    //          text:\"基于算法、人眼视觉系统技术，对视频进行修复、增强\"\n    //        },\n    //        newMedia:{  \n    //         title:\"沉浸式新媒体\",\n    //         text:\"研究采集、压缩、传输、播放技术链给人带来感官体验\"\n    //       },\n    //        videoUnderstanding:{  \n    //         title:\"视频理解\",\n    //         text:\"利用深度学习，对视频内容进行深入理解\"\n    //       },\n    //       VideoCodingAndDecoding:{  \n    //         title:\"视频编解码\",\n    //         text:\"更高的视频压缩比、更好的信道适应性\"\n    //       },\n    //       videoHandle:{  \n    //         title:\"视频处理\",\n    //         text:\"时域信息、结合视频理解和编解码信息的智能处理\"\n    //       },\n    //       audioHandle:{  \n    //         title:\"音频处理\",\n    //         text:\"致力于解决多人、多场景实时通信系统中的3A问题\"\n    //       },\n    //       connectionQuality:{  \n    //         title:\"音视频通信质量评估\",\n    //         text:\"评估音视频内容的质量的优质平台\"\n    //       },\n    //       transfer:{  \n    //         title:\"实时音视频传输\",\n    //         text:\"通过对网络状态的检测制定合适的抗性和传输策略\"\n    //       },\n    // },\n\n        //  项目\n        // project:{\n        //    title1:\"TPG\",\n        //    title2:\"TSE\",\n        //    title3:\"腾讯丽影\",\n        //    title4:\"音视频质量评测平台\",\n        //    text1:\"TPG是基于AVS2推出的一种图片压缩技术 ：压缩效率\",\n        //    text2:\"比JPG高47%，比PNG高60%，比Gif高85%，比\",\n        //    text3:\"比Webp高25%，功能齐全\",\n        //    more:\"了解更多\"\n        // },\n        // 解决方案\n  //       solution:{\n  //          title1:\"腾讯实时音视频（Tencent Real-Time Communication，TRTC)\",\n  //          text1:\"拥有QQ十几年来在音视频技术上的积累，致力于帮助企业快速搭建低成本、高品质音视频通讯能力的完整解决方案\",\n  //          title2:\"互动直播解决方案\",\n  //          text2:\"全新一站式“多路音视频互动”解决方案，主打直播连麦和多画面特效，通过移动直播 SDK 打造跨平台一对多、多对多的超清酷炫直播场景。\",\n  //          title3:\"腾讯云游戏多媒体引擎（Gaming Multimedia Engine，GME)\",\n  //          text3:\"提供一站式游戏语音解决方案。针对不同游戏场景进行深度优化，功能完备，接入门槛低，一个 SDK 即可满足多样化的游戏语音需求。\",\n  //          more:\"了解更多\",\n  //          total:\"查看全部\",\n  //          join:\"加入腾讯音视频实验室\",\n  //          joining:\"立即加入\"\n  //       }\n  // }\n}\n  ","export default {\n    title: \"Tencent Media Lab, may the world be around you.\",\n    more: \"Learn More\",\n    //  研究领域\n    research:{\n          title: 'Research',\n          researchAndMade:{\n            title:\"International Standardization\",\n            text:\"International standardization research covering multimedia compression, system, and communication\"\n           },\n          touch:{  \n            title:\"Heterogeneous Platform\",\n            text:\"Provide higher efficiency and low latency compute power than traditional parallel CPU architecture. \"\n          },\n          newMedia:{  \n           title:\"Immersive Media\",\n           text:\"Immersive experience through complete pipeline of media capture, compression, communication, and playback\"\n          },\n          videoUnderstanding:{  \n            title:\"Intelligent Media\",\n            text:\"High level analysis and understanding of video semantics through AI and deep learning\"\n          },\n           VideoCodingAndDecoding:{  \n             title:\"Video Coding\",\n             text:\"Higher compression ratio and better channel adaptation\"\n           },\n           videoHandle:{  \n             title:\"Video Processing\",\n             text:\"Video spatial and temporal processing that works seamlessly with video understanding and compression\"\n           },\n           audioHandle:{  \n             title:\"Audio Processing\",\n             text:\"Multi-Party 3A solution for real-time communications among various scenes\"\n           },\n           connectionQuality:{  \n             title:\"Multimedia QA\",\n             text:\"Quality assessment platform for audio and video contents\"\n           },\n           transfer:{  \n             title:\"Real-Time Multimedia\",\n             text:\"Network robustness against volatile bandwidth and package loss, through network condition detection and feedback\"\n           }\n    },\n     //  项目\n    project:{\n      title1:\"TPG\",\n      title2:\"TSE\",\n      title3:\"KANASKY\",\n      title4:\"TMEC \",\n      subTitle1:\"TPG (Tiny Portable Graphic)\",\n      text1:\"A new image compression standard based on AVS2\",\n      subTitle2:\"TSE (Tencent Screen Encoder)\",\n      text2:\"Optimized video encoder for screen content\",\n      subTitle3:\"KANASKY\",\n      text3:\"Human perceptual video service based on AI and big data, which produces prettier yet smaller video.\",\n      subTitle4:\"TMEC（Tencent Multimedia Evaluation Cloud）\",\n      text4:\"Provide ability for efficient automatical AV(audio/video) quality assesment, conducting full automatical presssure assesment at audio and video quality in multi aspect.\",\n      lookAll:\"See All\",\n      more: \"Find out more\",\n    },\n     // 解决方案\n    solution:{\n      solutionTitle: \"Solutions\",\n      title1:\"TRTC（Tencent Real-Time Communication）\",\n      text1:\"With 10+ years serving QQ Real-Time video communications, TRTC dedicates to provide high quality and low cost solutions for multimedia business.\",\n      title2:\"Solution for Interactive Webcasting\",\n      text2:\"A brand new all-in-one solution for multi-way audio video interaction. Audio interaction with the host, split view display, cross-platform one-to-many, multi-way high definition webcasting via mobile webcasting SDK\",\n      title3:\"GME（Gaming Multimedia Engine）\",\n      text3:\"All-In-One voice communication solution for gaming. Highly optimized for various gaming scenes, full functionalities, easy integration. One single SDK fits all.\",\n      more:\"Find out more\",\n      lookAll:\"See All\",\n      join:\"Tencent Media Lab Job Opportunities\",\n      join1:\"Tencent Media Lab Job Opportunities\",\n      joining:\"Join Us Now!\"\n    },\n    // mask\n  navList:[\n    {\n      name: 'Home'\n    },\n    {\n      name: 'Research',\n      showList: false,\n      childen: [\n        { eventName: 'click_research_international', id: '86717',name: 'International Standardization', link:'/research/international', index: 0},\n        { eventName: 'click_research_audio_and_video', id: '82091',name: 'Real-Time Multimedia', link:'/research/audioTransmission', index: 1 },\n        { eventName: 'click_research_immersive', id: '86718',name: 'Immersive Media', link:'/research/newMedia', index: 2 },\n        { eventName: 'click_research_codec', id: '86719',name: 'Video Coding', link:'/research/videoCode', index: 3 },\n        { eventName: 'click_research_audio', id: '86716',name: 'Audio Processing', link:'/research/audioProcessing', index: 4 },\n        { eventName: 'click_research_process', id: '82088',name: 'Video Processing', link:'/research/videoProcessing', index: 5 },\n        { eventName: 'click_research_perceived', id: '86720',name: 'Heterogeneous Platform', link:'/research/perceptual', index: 6 },\n        { eventName: 'click_research_assess', id: '82092',name: 'Multimedia QA', link:'/research/assessment', index: 7 },        \n        { eventName: 'click_research_understanding', id: '82087',name: 'Intelligent Media', link:'/research/VideoUnderstanding', index: 8 },\n      ]\n    },\n    {\n      name: 'Research Topics',\n      showList: false,\n      childen: [\n        { eventName: 'click_top_project', id: '82082',name: 'Research Topics', link:'/project' },\n        { eventName: 'click_project_TPG', id: '82096',name: 'TPG', link:'/projectTpg' },\n        { eventName: 'click_project_TSE', id: '82097',name: 'TSE', link:'/projectTse' },\n        { eventName: 'click_project_liying', id: '82098',name: 'KANASKY', link:'/projectLiYing' },\n        { eventName: 'click_project_platform', id: '82099',name: 'TMEC', link:'/projectAudioAndVideo' }\n      ]\n    },\n    {\n      name: 'Solutions',\n      showList: false,\n      childen: [\n        { eventName: 'click_top_solution', id: '82083',name: 'Solutions', bgc: false, link:'/solution' },\n        { eventName: 'click_solution_TRTC', id: '82093',name: 'TRTC', bgc: true, link:'https://cloud.tencent.com/product/trtc '},\n        { eventName: 'click_solution_ILVB', id: '82094',name: 'ILVB', bgc: true, link:'https://cloud.tencent.com/solution/ilvb' },\n        { eventName: 'click_solution_GME', id: '82095',name: 'GME', bgc: true, link:' https://cloud.tencent.com/product/gme' }\n      ]\n    },\n    {\n      name: 'About Us',\n      showList: false,\n      childen: [\n        { name: 'Tencent Media Lab', link:'/aboutUs/laboratory' },\n        // { name: 'Paper', link:'/aboutUs/thesis' },\n        { name: 'Join us', link:'/aboutUs/joinUs' }\n      ]\n    }\n  ]\n  }","export default {\n    //  研究领域页面总标题\n     title:\"Research\",\n\n    //  侧边导航菜单文字使用用home页面里的变量，这里省略\n\n\n\n\n    // 1. 国际标准化研究与制定页面\n    researchAndMade:{\n        title:\"International Standardization\",\n        text:\"We focus on research and development of multimedia standards, including media compression, transmission and systems. Meanwhile, we actively participate in industry consortia， promoting the industry development with technologies.\",\n        explain:\"我们专注于多媒体领域的标准研究和制定。研究领域包括:\",\n\n        //   tabs切换    --tab媒体压缩\n        \n        tab1:{\n            title:\"Multimedia Compression\",\n            list:[\n                {\n                    subTitle:\"VVC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"Versatile Video Coding (VVC) is a video compression standard being developed for finalization around 2020 by the Joint Video Experts Team (JVET) which is a united video expert team of the MPEG  and the ITU-T VCEG. \",\n                            \"VVC will be the successor to High Efficiency Video Coding (HEVC) and targets at 40%-50% bit-rate reduction over HEVC. Media Lab actively contributes to VVC and many proposals have been adopted in VVC. \",\n                            \"Media Lab holds several key positions in JVET. Dr. Shan Liu is one of the VVC standard editors. Dr. Xiang Li is one of the VVC reference software co-chairs.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG-5 EVC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MPEG-5 Essential Video Coding (EVC) is another video coding standard that is expected to be completed in 2020.\",\n                            \"The standard is to consist of a royalty-free subset and individually switchable enhancements. With the enhancements, the coding efficiency of MPEG-5 EVC will be noticeably higher than HEVC. \",\n                            \"Media Lab’s screen content coding technology has been adopted in MPEG-5 EVC.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"AVS-3\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"Audio Video coding Standard (AVS) workgroup of China was authorized to be established by the Science and Technology Department under the former Ministry of Industry and Information Technology of People’s Republic of China in June 2002. \",\n                            \"A series of successful video coding standards have been developed by this workgroup, among which AVS-1 is comparable to H.264/AVC standard in coding efficiency; AVS-2 is comparable to H.265/HEVC standard; AVS-3 v1 has been finalized and can reportedly achieve ~30% bit-rate reduction when compared with AVS-2. \",\n                            \"AVS-3 v2 development is ongoing and targets at 40% bit-rate reduction when compared with AVS-2. Media Lab is actively participating in AVS-3 development and takes leadership as one of the chairs for screen content coding activities. \",\n                            \"Media Lab’s screen content coding technology has been adopted in AVS-3 v2.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG-PCC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MPEG (the Moving Picture Experts Group) is developing two video coding standards to address the needs for efficient dissemination of 3D point cloud data which consists of the position and the attribute information of each point in 3D objects or scenes. \",\n                            \"The first standard is V-PCC (ISO/IEC 23090-9 Video-based Point Cloud Compression), which performs a conversion of the dense 3D point cloud into a 2D video that can be compressed by leveraging existing video codecs such as AVC, HEVC, and VVC.  \",\n                            \"The second one is G-PCC (ISO/IEC 23090-5 Geometry-based Point Cloud Compression), which is more appropriate for sparse point clouds and directly codes the 3D geometry and associated attributes representing large point clouds. These two compression standards find many promising areas of relevance such as AR/VR, autonomous driving, map services, cultural heritage, and industrial applications. Media Lab is actively participating in both standards leading several sub-activities. \"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"AVS-PCC\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"A point cloud is a set of 3D points used to represent 3D objects or scenes.\",\n                            \"It includes not only geometry information but also attribute information such as color or reflectance, etc. Point clouds have been widely used in many areas such as autonomous driving, geographic information systems (GIS), and culture heritage preservations, etc.  AVS point cloud coding sub-group (AVS-PCC) was established in June 2019 in AVS workgroup to investigate use cases, technique requirements and technique solutions of point cloud compression.\",\n                            \"It is expected that a Call for Evidence (CfE) will be issued after August meeting in 2019 and a Call for Proposal (CfP) will be issued in early 2020. \",\n                            \"Media Lab is actively participating in AVS-PCC and also takes leadership role as one of three joint chairs of the sub-group and chairs of several activities.\"\n                         ]}\n                    ]\n                }\n            ]\n        },\n        // -- tab系统与传输 \n        tab2:{\n            title:\"Transmission and System\",\n            list:[\n                {\n                    subTitle:\"OCP\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"The Open Compute Project (OCP, http:// https://www.opencompute.org) standardizes datacenter hardware and certain low-level APIs. \",\n                            \"Media Lab contributes primarily towards accelerator hardware and API projects, in particular for video and point cloud compression.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"IETF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"The mission of the IETF (http://www.ietf.org) is to make the Internet work better by producing high quality, relevant technical documents that influence the way people design, use, and manage the Internet. \",\n                            \"Standards produced by the IETF include IP, TCP, UDP, and HTTP.  \",\n                            \"Media Lab participates in subgroups with a focus on media transport, including AVT, QUIC, and MOPS.  Dr. Wenger also holds the position of an IETF Trustee.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"3GPP SA\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"3GPP is, at present, the dominant standardization organization for the 5th the fifth generation cellular network technology (5G). \",\n                            \"Media Lab participates primarily in the System Architecture groups, namely in SA plenary and SA4 (codec).  The former has certain oversight functions, the latter standardizes audio/video codecs and their interface to the transport protocol stack.  Media Lab also follows other working groups including SA1, SA2, SA6, and some of the Core Technology (CT) working groups.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"MPEG SYSTEMS\",\n                    subContanier: [\n                        {sencTitle:\"OMAF\",sencText:[\n                            \"MPEG Omnidirectional Media Format (OMAF, ISO/IEC ISO/IEC 23090-2) provides a specification for delivery of VR/360 video and multimedia content.  In an OMAF experience, the viewer’s perspective is from the center of the sphere looking outward towards the inside surface of the sphere. \",\n                            \"OMAF 1st edition specification defines projection and packing schemes for mapping 3D content to 2D content for encoding, storage of the encoded content in ISO Based File Format, and finally encapsulation, signaling and streaming of the encoded content with Dynamic Adaptive Streaming over HTTP (DASH) as well as MPEG Media Transport (MMT). It also includes a few media and presentation profiles by defining specific codecs and combinations for each profile. \",\n                            \"MPEG published the 1st edition of OMAF in January 2019 and currently working on the 2nd edition. The 2nd edition will include overlay support as well as late binding of tile-streaming of 360 Video among other features. \",\n                            \"Media Lab is participating in the 2nd edition development of OMAF. Dr. Choi is one of the OMAF standard editors.\"\n                        ]},\n                         {sencTitle:\"MPEG DASH\",sencText:[\n                            \"MPEG Dynamic Adaptive Streaming over HTTP (DASH) standard (ISO/IEC 23009) is a set of specifications primarily for interoperable streaming of multimedia content over the internet. The core specification defines a manifest and a segment format. The manifest describes the location of segmented content resources and their variation with different bitrates so that the DASH client can adapt to the variable network bandwidth by switching between various bitrates. The segment format defines the fragmented file format for media content.\",\n                            \"MPEG DASH is codec and protocol agnostic, i.e. it can support any video and audio codec and media types, as well as various delivery protocols such as HTTP1.1, HTTP2, multicast, and broadcast.\",\n                            \"DASH has been widely adopted by industry and consortia, including ATSC 3.0, DVB, HbbTV, 3GPP, VR-IF and CTA WAVE. MPEG DASH specification has several parts. MPEG is about to publish the 4th edition of the core spec (Part 1) and is currently working on new extensions and additional parts. Dr. Iraj Sodagar has been the MPEG DASH subgroup chair in MPEG since the start of DASH standardization in 2009. He is also co-editors of DASH specifications.\",\n                            \"Media Lab actively participates in the DASH standard developments, and several of its submissions were adopted into the 4th edition.\"\n                        ]},\n                        {sencTitle:\"CMAF\",sencText:[\n                            \"MPEG Common Media Format (ISO/IEC 23000-19) is an encoding format to be used by the various streaming protocol. It provides a fragmented file format along with encoding constraints for creating multi-rate encoded content, which can be used with MPEG-DASH as well as other streaming formats such as HTTP Live Streaming (HLS).\",\n                            \"CMAF defines the file format constraints, the common encryption modes and encoding constraints for its media profiles. It also defines a set of commonly used media profiles (with defined codec, profile, level, and other characteristics). With CMAF, a service provider can encode the content once and deliver it using different streaming protocols or delivery platforms. Therefore CMAF reduces the content encoding cost as well as the CDN efficiency and cost during delivery.\",\n                            \"MPEG is about to publish the 2nd edition of CMAF and is currently working on the 3rd edition.\",\n                            \"Dr. Sodagar was the first co-chair of CMAF subgroup in MPEG and Media Lab actively participates in CMAF spec developments.\"\n                        ]},\n                        {sencTitle:\"MPEG NBMP\",sencText:[\n                            \"MPEG Network-Based Media Processing (NBMP, ISO/IEC 23090-8) provides a standard for building, deploying and managing media workflows over Cloud platforms. MPEG NBMP enables a service provider to define a media workflow and request the NBMP Workflow Manager to build it using a repository of pre-built media functions without writing a single line of software. \",\n                            \"NBMP is platform-agnostic i.e. it can deploy services on any private or public cloud platform or a hybrid of various Cloud platforms. It establishes the workflow, manages the tasks, monitors the media pipeline and provides reports back to the service provider. \",\n                            \"MPEG is currently developing the NBMP specification with a target publication date of H1/2020. 4.Media Lab is one of the key contributors to NBMP specification, with dozen adopted submissions into the specification and Dr. Iraj Sodagar is the co-editor of NBMP specification.\"\n                        ]},\n                    ]\n                }\n            ]\n        },\n        //  -- tab工业联盟与论坛\n        tab3:{\n            title:\"Industry Consortia\",\n            list:[\n                { \n                    subTitle:\"MC-IF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"MC-IF furthers the adoption of MPEG Standards.\",\n                            \"It initially focuses on VVC, by establishing them as well-accepted and widely used standards for the benefit of consumers and industry.  \",\n                            \"Media Lab, on Tencent’s behalf was a founder of this organization, and Dr. Wenger sits on its Board of Directors and chairs the IP Ecosystem working group.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"DASH-IF\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"The DASH Industry Forum (DASH-IF) is an industry forum to foster the deployment of MPEG-DASH standard. \",\n                            \"DASH-IF currently has 80 member companies and since 2012 has developed several implementation guidelines and specifications on DASH deployment, content protection exchange format for server-side encryption services, extensive test vector database, and conformance tools for on-demand and live content, DASH live streaming simulator, packaging tools, and test platforms, dash.js open-source client, and several white papers. \",\n                            \"DASH-IF collaborates closely with MPEG on the DASH spec developments as well as with other consortia such as 3GPP, DVB, W3C, ATSC, CTA WAVE and HbbTV on DASH consortia-related specifications and deployments. Dr. Iraj Sodagar is DASH-IF President/Chairman of Board.\",\n                            \"Media Lab is leading and contributing to some of the technical activities in DASH-IF.\"\n                        ]}\n                    ]\n                },\n                {\n                    subTitle:\"8K Association\",\n                    subContanier:[\n                        {sencTitle:\"\",sencText:[\n                            \"8K Association is an organization to address the concerns of the 8K ecosystem over 5G network. The missions of 8K Association are promoting 8K TVs and 8K Content to consumers and professionals, developing technical requirements and specification for 8K visual quality over 5G, and facilitating communication within 8K and 5G ecosystem for commercialization. \",\n                            \"Tencent Media Lab. was invited as a Principal member & Board of Director (BoD) candidate of 8K Association. Now, Dr. Stephan Wenger and Dr. Byeongdoo Choi are coordinating the collaboration with global companies including Samsung, Intel, TCL, to lead 8K-ecosystem.\"\n                        ]}\n                    ]\n                }\n            ]\n        }\n       },\n        \n\n    //  2.沉浸式新媒体页面\n    newMedia:{\n         title:\"Immersive Media\",\n         text:\"Our team seeks to expand the possibilities of how people interact with information and media through fully immersive virtual reality.  We are infusing cutting edge technologies in capture, compression, playback, etc. into our products to create end to end solutions, allowing users to experience the world through brand new perspectives.\",\n         explain:\"Function Points\",\n         list:[\n             {\n                subTitle:\"Capture\",\n                subContanier:\"Research data capture methods with a focus on improving video quality through algorithms used for stitching, projection, etc.\"\n             },\n             {\n                subTitle:\"Transmission\",\n                subContanier:\"Transfer of multi-dimensional data for immersive virtual reality places significantly greater demands on bandwidth than traditional media.  As a result, stability and quality are both important hurdles.By focusing on ROI transmitting algorithms, our team was able to bandwidth requirements by 50% while maintaining the quality of the data stream.\"\n             },\n             {\n                subTitle:\"Transcoding\",\n                subContanier:\"Our VR transcoding server is compatible with a range of end-client needs, supporting adaptive bitrate, multiple formats, and differing processing methods.\"\n            },\n            {\n                subTitle:\"Playback\",\n                subContanier:\"Multi-Platform playback in HLS/DASH protocols is integrated with ABR, ROI, and projection algorithms.  Mobile, PC, and tablet products are supported.\"\n            },\n            {\n                subTitle:\"Interactive\",\n                subContanier:\"Interactive 3D models worked with customized data.\"\n            },\n         ],\n\n         business:\"Partnerships\",\n         businessList:[\n             {\n                subTitle:\"Cultural Tourism – Smart Town\t\t\t\t\t\t\",\n                subContanier:\"We created a completely new travel experience by highlighting cultural and local specialties while personalizing services with our immersive guide.\"\n             },\n             {\n                subTitle:\"Education—Collaborating with our partner,\",\n                subContanier:\"We transformed the student’s experiences by bringing study subjects to life with VR technology.  The new teaching methods generated greater student enthusiasm and improved learning results.\"\n            }\n         ]\n\n    },\n       \n\n    //   3.视频编解码页面\n    VideoCodingAndDecoding:{\n         title:\"Video Coding\",\n         text:\"Video contents occupy a majority of communication information nowadays. The storage and delivery of raw video are under great pressure due to the massive size of uncompressed data. To alleviate such a pressure, it is critical to achieve higher compression ratio and better channel adaptivity. Video coding standard and technology are constantly evolving, empowering a lot of related areas.\",\n         explain :\"Function Points\",\n         list:[\n             {\n                subTitle:\"Software Video Coding Technology\",\n                subContanier:\"We have high efficiency real-time codecs for H.264, H.265 and AVS2 that run on any platform. High Definition (HD) Real-Time encoding funtionality is well supported. Our codecs have been widely used in all kinds of real-time video communication scenarios, offering great compression efficiency and industry-leading performance.\"\n             },\n             {\n                subTitle:\"Hardware Video Coding Technology\",\n                subContanier:\"We have various hardware codecs targeting different platforms, including but not limited to PC, mobile. We have a set of intelligent algorithms taking coding efficiency and hardware complexity into consideration for switching between software and hardware codecs. As a result, the user experience can be improved significantly.\"\n            },\n            {\n                subTitle:\"Network Adaptive Coding Technology\",\n                subContanier:\"Combining the network with video coding, the encoder parameters can be adjusted automatically according to the real-time feedback of the network. For example, when the network is in bad condition, the encoders reduce bit rate or frame rate to ensure better transmission quality such that the network resilience can be improved considerably.\"\n            },\n            // {\n            //     subTitle:\"Dynamic Coding Strategy\",\n            //     subContanier:\" We dynamically adjust encoder parameters (including but not limited to resolution, frame rate, bit rate) to adapt to the network fluctuation and the unstable performance of terminal devices.\"\n            // },\n         ],\n\n         project:\"Invention project\",\n         projectName1:\"TPG\",\n         projectName2:\"TSE\"\n    },\n\n    \n\n    //  4.视频理解页面\n    videoUnderstanding:{\n          title:\"Intelligent Media\",\n          text:\"The multimedia analysis and understanding framework based on the cross-modal (image, audio, text, voice) temporal algorithm is built to realize media label, scene classification, event detection, video summary, multimedia description, multimedia retrieval and other functions.\",\n          explain :\"Function Points\",\n          list:[\n              {\n                subTitle:\"Video Highlight\",\n                subContanier:\"Analyze multimodal information such as visual, audio, and text in video, and use the Deep Learning based time series modeling technology to detect exciting actions and events, and automatically output highlighted video. Currently supports games, sports, variety shows and others.\"\n              },\n              {\n                subTitle:\"Smart Cover\",\n                subContanier:\"Use the highlight video and image aesthetic evaluation techniques to select the best keyframes or key clips as video covers to enhance video click conversion and user experience. Currently supports game, sports, MV, short video and others.\"\n            },\n            {\n                subTitle:\"Video Scene\",\n                subContanier:\" Using deep learning based scene recognition technology to intelligently analyze video content, capture information such as characters, scenes,  and texts in the video, and automatically generate video tags and categories.\"\n            },{\n                subTitle:\"Saliency Detection\",\n                subContanier:\"Using Deep Learning based semantic segmentation and object detection technology to detect and segment the region of interest of the video, accurately locate the human eye's attention area in the video, and provide favorable information\"\n            }\n          ]\n    },\n\n    \n\n    //   5.视频处理页面\n    videoHandle:{\n        title:\"Video Processing\",\n        text:\"We have been working on video processing for many years and accumulated a large amount of technology. Both mobile and backstage, both real-time and offline, both video and image, both traditional algorithms and artificial intelligence, our researches cover almost all fields of video processing. Meanwhile, each algorithm is deeply optimized for performance and effects,  and finally make it a valuable technology which can be truly applied on the product.\",\n\n\n        explain1:\"Video Restoration\",\n        list1:[\n            {\n                subTitle:\"Intelligent Denoising\",\n                subContanier:\"Real-time intelligent denoising:  We use the predictied information from the previous frame, to intelligently denoise on the current frame according to the noise intensity;Offline intelligent denoising: We combine deep learning with the predicted information from previous and subsequent frames, to search the optimal matching block for joint denoising,  and finally obtain the best denoised effect.\",\n            },\n            {\n                subTitle:\"Artifacts Removal\",\n                subContanier:\"Using deep learning,  we can not only remove the artifacts such as block effect, edge burr, which produced by the coding compression, but also ensure that the main edge can be preserved and the weak edge details can be restored,  so that the algorithm can greatly improve the subjective quality of the video.\"\n            },\n            {\n                subTitle:\"Color Restoration\",\n                subContanier:\"Using deep learning to perform high-intensity repairs on damaged frames in the video, we can restore the color of the damaged part to optimal status, thereby the color of the entire video would be restored and the subjective quality of the video can be improved.\"\n\n            },\n            {\n                subTitle:\"Intelligent Video Stabilization\",\n                subContanier:\"Real-time video stabilization: We combine the anti-shake algorithm with the location information of the hardware device on the mobile terminal to anti-shake, and make the video stable in real time;Offline video stabilization: Intelligently detecting the shaking level, we utilize the information of the previous and subsequent frames to estimate motion on the current frame, in order to anti-shake.\"\n            },\n            {\n                subTitle:\"Dehazing\",\n                subContanier:\"Based on the dark channel prior, our technique solves the problem of overexposure and the adaptation on flat area, and adapts to all scenes of each video. And after deep optimization of video on performance, our algorithm consume almost no resources.\"\n            }\n        ],\n        explain2:\"Video Enhancement\",\n        list2:[\n            {\n                subTitle:\"Intelligent Detail Enhancement\",\n                subContanier:\"According to the visual effects of the human eyes, we enhance the details on the regions that human eyes interest intelligently. It can enhance the image quality of ROI, achieve a clearer effect of the details，and can be more beneficial for video coding.\"\n            },\n            {\n                subTitle:\"Color Enhancement\",\n                subContanier:\"Through the player's display of the original color of different videos, we adaptively adjust the different colors in the video to improve the saturation, in order to make the video content more rich and enhance the visual effects.\"\n            },\n            {\n                subTitle:\"Super-Resolution\",\n                subContanier:\"Using deep learning technology, we deeply optimize the video for different scenes of different videos. Under the premise of increasing the video resolution, our SR algorithm also greatly enhance the details in the video.\"\n            },\n            {\n                subTitle:\"Frame Rate Upsampling\",\n                subContanier:\"During the video playback, we have increased the frame rate of the video, resulting in smoother video and more detailed image. Enhanced the saturation and exquisiteness of the video,  the visual enjoyment of watching the video would be improved.\"\n            },\n            {\n                subTitle:\"Dark Scene Enhancement\",\n                subContanier:\"using cutting-edge technology combined with excellent self-research ability, we optimize the effects of various dark scene, and adapt to actual scenes. Our algorithm can achieve real-time processing of video, with consuming almost no resources .\"\n            },\n            {\n                subTitle:\"Beauty\",\n                subContanier:\"Our algorithm beautify face accurately, ensuring that the background is still clear, we can not only enhance the three-dimensional sense of the human face, eyebrows, lips and other facial features, but also erase the defects on faces, brighten eyes and whiten skin.\"\n            },\n            {\n                subTitle:\"Filter\",\n                subContanier:\"We design a variety of filters in different scenes and styles. In the case of ensuring good visual effects, our filters can present different styles, make the video quality closer to the human eyes and improve the color saturation and exquisiteness.\"\n            },\n            {\n                subTitle:\"Special Effects\",\n                subContanier:\"We can transform different scenes and different styles into the comic style, sketch style and so on, which make the video much more instereting and vivid,  and then increase the click rate.\"\n            }\n        ]\n\n    },\n\n\n    //  6.感知交互页面\n    touch:{\n        title:\"Heterogeneous Platform\",\n        text:\"Video encode/decode and processing needs more and more compute power,  and there is no single solution can fit various application and performance requirement.  Heterogeneous computing is the key technology and it can provide higher efficiency and low latency compute power than traditional parallel CPU architecture. \",\n        explain :\"Function Points\",\n        list:[\n            {\n                subTitle:\"Video Transcoding\",\n                subContanier:\"Transcode the video stream from one format to one or multiple video streams with different formats, can support differnt format, resolution, bitrate, framerate and configuration etc, and can also support different usage scenarios like real-time live broadcasting or non real-time VOD.\"\n            },\n            {\n                subTitle:\"Video Processing for Video Codec\",\n                subContanier:\"We perform video assisted processing by closely combining the corresponding codec information, including intelligent video enhancement used codec information,  smart erasing on video that facilitates coding and so on.\"\n            },\n            {\n                subTitle:\"Hardware platform support\",\n                subContanier:\"Can support different hardware platform, including CPU, GPU, FPGA, ASIC etc\"\n            }\n        ],\n        business:\"合作业务\",\n         businessList:[\n             {\n                subTitle:\"Cloud  Gaming \",\n                subContanier:\"Cloud gaming is a type of online gaming that aims to provide smooth and direct playability to end users of games across various devices. This could include a host gaming server capable of executing a gaming engine and streaming the gaming data to the client device.\"\n             }\n         ]\n     },\n\n    //   7.音频处理页面\n    audioHandle:{\n          title:\"Audio Processing\",\n          text:\"We are focusing on designing low-complexity and high-robust solutions for elimating the quality problems in voice communications for diversed usecases, to enrich the audio experiences in office and mobile devices.\",\n          explain:\"Function Points\",\n          list:[\n              {\n                subTitle:\"Audio Engine\",\n                subContanier:\"It focus on inventing world-leading audio compressions system in VoIP with high-efficient and high-quality. The solution is highly jointly correlated to  the optimizations in channel aspect to  improve the end-to-end experiences with error resillience.\"\n              },\n              {\n                subTitle:\"Machine Learning\",\n                subContanier:\"It  is targeted into design novel algorithms in elimiating complex glitches and non-stationary noise and improving the quality and intelligibility by combining state-of-art machine learning technologies. The applications include but not limit, quality improvement for end-to-end voice communication, voice beautification, and voice conversion, etc.\"\n            },\n            {\n                subTitle:\"Audio Engineering\",\n                subContanier:\"It focus on building high-efficiency coding and engineering capabability to fullfil different business and technical requirements. The outcomes include high-quality audio solutions covering server and clients, including the real-time quality monitoring.\"\n            },\n            {\n                subTitle:\"Voice Engine\",\n                subContanier:\"It is target on next-generation source coding with multi-rate QoE-control and jointly-source-channel coding, to help further improve the end-to-end subjective experience with robust QoE-control based on channel feedback.\"\n            },\n            {\n                subTitle:\"Pre-Processing (3A)\",\n                subContanier:\"It includes a set of 3A processing (i.e., Automatic Echo Cancellations, Automatic Noise Cancellations, and Automatic Gain Control) to elimate diversed artifacts in capturing aspect.\"\n            },\n            {\n                subTitle:\"AI Denoising/Separations\",\n                subContanier:\"It combines deep learning methos to detect and remove the atternuations in transmitted signal, to improve the subjective quality and intelligibility.\"\n            },\n            {\n                subTitle:\"AI Sound Beautification\",\n                subContanier:\"It targets to improve the sining sound quality, including but not limit detect word-error and word-missing, improve the tune, to improve the definitions of the signing sound with better aesthetic feelings.\"\n            },\n            {\n                subTitle:\"AI Voice Conversion\",\n                subContanier:\"It combines deep learning methods to convert the original sound to another form with another speaker's features. It changes the tones and timbre with natural approach.\"\n            },\n            {\n                subTitle:\"3D Sound Effect\",\n                subContanier:\"It append the artificial spatial features into original monophonic sound to enable the spatiness feeling, e.g., the effects of directional sound is perceived.\"\n            },\n            {\n                subTitle:\"Voice Morphing\",\n                subContanier:\"It provides the capababilities to morphing the sound and outcome interesting sound effect (e.g., lolita, robot, cartoons, etc). The configurations is able to customized to generate different morphing effect to improve the interesting in human-machine interactions.\"\n            },\n          ]\n    },\n\n   \n\n    // 8.实时音视频传输页面\n    transfer:{\n          title:\"Real-Time Multimedia\",\n          text:\"Streaming High-Quality multimedia contents requires stable network conditions with significant available bandwidth, and the instability of underlying network affects the user-perceived experience. How to efficiently and accurately measure the network condition and make appropriate congestion control decision is still an unsolved problem for both academia and industry. The networking group of Tencent Media Lab focuses on designing and implementing systematical, adaptive and reliable solutions for the network-level issues for Real-Time multimedia streaming.\",\n          explain:\"Function Points\",\n          list:[\n              {\n                subTitle:\"Congestion Control\",\n                subContanier:\"We studied the State-of-the-Art congestion control algorithms from both academia and industry.  After measuring and evaluating the network traces collected from Tencent products, we design novel congestion control to make proper decision under various network conditions.\"\n              },\n              {\n                subTitle:\"Network Resilience\",\n                subContanier:\"A user may suffer various network instabilities in the wild, such as packet loss, high delay or even intermittent connections. We have proposed the intelligent network resilience algorithms to accommodate network instability and guarantee smooth and high-quality multimedia streaming.\"\n            },\n            {\n                subTitle:\"QoS/QoE Optimiaztion\",\n                subContanier:\"We perform the optimization based on the media context and network conditions to achieve the best trade-off among clarity, smoothness, and latency under constrained network environments.\"\n            },\n            {\n                subTitle:\"Flow control for Multi-User Communication\",\n                subContanier:\"We adopt two strategies to optimize the quality under multi-party conferencing: AI for simple scenarios, and subjective-test based fine adjustment for complex scenarios.\"\n            },\n            {\n                subTitle:\"Network Measurement\",\n                subContanier:\"We develop algorithms to learn network behaviors from the traces without personal information. This helps us emulate Bad-Cases offline and detect network condition online.\"\n            },\n          ]\n    },\n\n\n    // 9.音视频通信质量评估页面\n    connectionQuality:{\n        title:\"Multimedia QA\",\n        text:\"We strive to develop end-to-end Audio/Video Quality Assessment algorithms with an emphasis on deep learning based techniques. Our research covers various multimedia contents in real-world applications. We evaluate the performance of Standard/In-House codecs on different tasks. Particularly, we are endavoring to study Pre/Post-Processing techniques for the purpose of quality enchancement. The aim is to apply quality monitoring to our product pipeline and increase the user experience of our multimedia applications.\",\n        explain:\"Function Points\",\n        list:[\n            {\n                subTitle:\"No-Reference Audio Quality Assessment\",\n                subContanier:\"Machine learning based techniques to assess the quality of real-time audio. On one hand, we evalute the degree of jerking, lagging, legibility when the network is not ideal. On the other hand, we measure the the effects of audio enhancement techniques.\"\n            },\n            {\n                subTitle:\"No-Reference Video Quality Assessment\",\n                subContanier:\"Deep learning based techniques to assess the quality of videos. The system is designed for applications either capturing with low-end consumer cameras, or embedded with quality enhancement filters, or both. Our method has launched in several video applications.\"\n            },\n            {\n                subTitle:\"Full-Reference Video Quality Assessment\",\n                subContanier:\"Deep learning based techniques to precisely measure the quality of compressed videos. The system is suitable for applications where the pristine reference is available and provides a trade-off between bit rates and viewing experience. Our method achieves STOA performance on datasets compressed standard codecs.\"\n            },\n        ],\n        project:\"Invention project\",\n        projectName:\"TMEC\",\n        projectText:\"Provide ability for efficient automatical AV(audio/video) quality assesment, conducting full automatical presssure assesment at audio and video quality in multi aspect.\",\n        more: \"learn more\"\n    },\n\n\n}","\n    export default {\n        // 项目聚合页面\n        title:\"Research Topics\",\n        textTPG:\"The ability to transcode from traditonal CDN to TPG images, which esures visual effect and reduces data/bandwith cost, saving data for massive image related scenes.\",\n        textTSE:\"The encoder specific for content optimization on screen.\",\n        textLiYing:\"Innovative video service platform, which intelligently enhances or compresses video using AI and big data based on human visual technology.\",\n        pingtai:\"Provide ability for efficient automatical AV(audio/video) quality assesment, conducting full automatical presssure assesment at audio and video quality in multi aspect.\",\n        more:\"More\",\n    \n        nav:{\n            back:\"BACK : \",\n            next:\"NEXT : \"\n        },\n    \n       \n        // TPG页面\n        tpg:{\n            textTPG:\"TPG is an image compression technology, leveraging AVS2 encoding kernel underneath. It provides high compression efficiency, which is 47% higher than JPG, 60% higher than GIF, and 25% higher than WebP. TPG has all kinds of image compression functionalities, including alpha channel, animation encoding and decoding. It can be used across various platforms, such as Windows , Linux , Mac , iOS and Android. TPG has been declared as a national standard as a part of AVS2-P7.\",\n            partner:\"合作伙伴\",\n            itemTitle1:\"Compression Performance of TPG\",\n            itemText1:\"Under the same quality of the compressed images, the compression ratio of TPG is 47% higher than JPG,60% higher than PNG, 85% higher than GIF and 25% higher than WebP\",\n            itemTitle2:\"Coding Efficiency of TPG\",\n            itemText2:\"Under the same PSNR value of the compressed images, the coding efficiency of TPG is 23.5% higher than WebP, and 46.8% higher than libjpeg\",\n            itemTitle3:\"Decoding Performance of TPG\",\n            itemText3Item1:\"1.The decoding speed of TPG is 2 times faster than webP, 2.3 times faster than libjpeg on iOS platform\",\n            itemText3Item2:\" 2. The decoding speed of TPG is 1.2 times faster than webP, 1.6 times faster than libjpeg on Android platform\",\n            itemTitle4:\"Decoding Used Memory of TPG\",\n            itemText4Item1:\"1.iOS platform: the decoding memory occupation of TPG is 2 times larger than webP, and 2.5 times larger than libjpeg\",\n            itemText4Item2:\"2. Android platform: the decoding memory occupation of TPG is 2 times larger than webP, and 1.6 times larger than libjpeg\",\n            itemTitle5:\"TPG Applications\",\n            itemText5:{\n                text1:{\n                    one:\"Social apps\",\n                    two:\"1.QZone photo album\",\n                    // three:\"2.Moments\",\n                    four:\"2.Group pictures\",\n                    // five:\"4....\"\n                },\n                text2:{\n                    one:\"News apps\",\n                    two:\"1.H5 page\",\n                    three:\"2.News Client\",\n                    four:\"3.Browser\",\n                },\n                text3:{\n                    one:\"Game apps\",\n                    two:\"1.Installation Package\",\n                    three:\"2.Game UI Resources\",\n                }\n            },\n            partners:\"Partners of TPG\"  \n        },\n    \n        // TES页面\n        tse:{\n           tesText:\"Tencent Screen encoder(TSE) is designed for screen content encoding. TSE adds intra block copy and palette mode support based on conventional encoding algorithms. The coding efficiency of TSE is 55% higher compared to x265-medium mode, and the encoding speed is 10 times faster than x265-media mode. TSE can be used in real-time conditions.\",\n           itemTitle1:\"Coding Efficiency Comparision of TSE and X265\",\n           itemText1one:\"Camera Sequences: The coding efficiency of TSE is 20% higher than x265-ultrafast mode\t\",\n           itemText1two:\"Screen Sequences:The coding efficiency of TSE is 70% higher than x265-ultrafast mode, and 55% higher than x265-medium mode\",\n          \n           itemTitle2:\"Encoding Speed Comparision of TSE and X265\",\n           itemText2one:\"Camera Sequences:The encoding time of TSE is around 88% of x265-ultrafast mode on average\",\n           itemText2two:\"Screen Sequances:1. Turn off IBC and PLT: The encoding time of TSE is about 33% of that of x265-ultrafast mode on average; 2. Turn on IBC and PLT: The encoding time of TSE is about 50% of that of x265-ultrafast mode on average \",\n           partner:\"Partners of TSE\",\n        },\n    \n        // 腾讯丽影\n        liYing:{\n           title:\"KANASKY\",\n           textLiYing:\"Kanasky is a next generation video service platform, which intelligently produces smaller yet prettier video using human perception, artificial intelligence and big data technologies.\",\n           vidio:\"丽影宣传视频\",\n           itemTitle1:\"Human Perceptual Technology - Saliency Detection\",\n           itemTitle2:\"Human Perceptual Technology - Artifact Reduction\",\n           itemTitle3:\"Human Perceptual Technology - Super Resolution\",\n           itemTitle4:\"Human Perceptual Technology - Old Picture Restoration\",\n           partner:\"Partners of KANASKY\", \n           before:\"Original Image\",\n           after:\"Processed\"\n        },\n    \n    \n        // 音视频质量测评平台\n        vidioAndaudioTest:{\n            title:\"TMEC\",\n            pingtaiText:\"TMEC(Tencent Multimedia Evaluation Cloud) is an One-Stop service platform for evaluating the QoE of multimedia, it can make the evaluation more efficient, various commercial equipment and self-developed tools are integrated in this platform.The Real-Time call app can be evaluated under different virtual net environment with TMEC.\",\n            itemText1:\"The professional equipments and laboratory are usually very necessary for evaluation of multimedia, such as network emulation, relevant media protocal meter, etc. On the other hand, the competence requirement and learning curve of test enare relatively higher. TMEC is a platform founded on above mentioned equipments and techniques. It supports remote task submission, automated execution and report generation which can improve the evaluation procedure efficently\",\n            itemText2:\"The design concept of TMEC is providing a general, fair, accurate and reliable real-time call evaluation methodology and technique support.Besides of real-time call e2e evaluation solution, some general tools are also provided as cloud tools for other evaluation purposes, such as vdieo reference assessment, video Non-Reference assessment, audio echo assessment and so on.\",\n        }\n    }\n    ","export default {\n    // 解决方案聚合页面\n    title:\"Solutions\",\n    list:[\n        {\n            title:\"TRTC\",\n            title1:\"Tencent Real-Time Communication(TRTC)\",\n            text:\"With 10+ years serving QQ Real-Time video communications, TRTC dedicates to provide high quality and low cost solutions for multimedia business.\",\n        },\n        {\n            title:\"ILVB\",\n            title1:\"Interactive Live Video Broadcasting(ILVB)\",\n            text:\"A brand new All-In-One solution for Multi-Way audio video interaction. Audio interaction with the host, split view display, cross-platform one-to-many, Multi-Way high definition webcasting via mobile webcasting SDK.\",\n        },\n        {\n            title:\"GME\",\n            title1:\"Tencent Cloud Game Multimedia Engine (GME)\",\n            text:\"All-In-One voice communication solution for gaming. Highly optimized for various gaming scenes, full functionalities, easy integration. One single SDK fits all.\",\n        },\n    ]\n}","export default {\n  bannerTitle:\"About Us\",\n  title:\"Tencent Media Lab\",\n  text:\"Tencent Media Lab (TML) dedicates to Cutting-Edge research on audio and video technologies, including real-time video communication, advaced audio and video codec algorithms and standardization, computer vision, image processing, and multimedia quality evaluation. TML is recognized as an elite pioneer in the multimedia industry, providing total solutions with leading technologies across various multimedia applications.\",\n  members:\"\",\n  sideNavBarTitle:[\"Tencent Media Lab Introduction\",\"Paper\",\"Join us\"],\n\n  //关于我们-实验室介绍\n  team: \"Team\",\n  expertList:[\n    {\n      id: 1,\n      imgClass: \"icon-about_image_expert_0\",\n      name:\"Dr. Shan Liu\",\n      text:\"Shan Liu is a Distinguished Scientist and General Manager at Tencent where she heads the Tencent Media Lab. Prior to joining Tencent she was the Chief Scientist and Head of America Media Lab at Futurewei Technologies. She was formerly Director of Multimedia Technology Division at MediaTek USA. She was also formerly with MERL, Sony and IBM. Dr. Liu is the inventor of more than 200 US and global patent applications and the author of more than 60 journal and conference articles. She actively contributes to international standards such as ITU-T H.265 | ISO/IEC HEVC, MPEG-DASH, MPEG-I, etc. and served as co-Editor of H.265/HEVC v4 and MPEG-I VVC. She was in technical and organizing committees, or an invited speaker, at various international conferences including IEEE ICIP, VCIP, ICNC, ICME, MIPR and ACM Multimedia. She served in Industrial Relationship Committee of IEEE Signal Processing Society 2014-2015. She was the VP of Industrial Relations and Development of Asia-Pacific Signal and Information Processing Association (APSIPA) 2016-2017 and was named the Industrial Distinguished Leader of APSIPA in 2018. Dr. Liu obtained her B.Eng. degree in Electronics Engineering from Tsinghua University, Beijing, China and M.S. and Ph.D. degrees in Electrical Engineering from University of Southern California, Los Angeles, USA.\" \n    },\n    {\n      id: 2,\n      imgClass: \"icon-about_image_expert_1\",\n      name:\"Dr. Xiang Li\",\n      text:\"Xiang Li received the B.Sc. and M.Sc. degrees in electronic engineering from Tsinghua University, Beijing, China, and the Dr.-Ing degree in Electrical, Electronic and Communication Engineering from University of Erlangen-Nuremberg, Germany. He is currently a senior principal researcher and the head of video coding standards in Tencent’s Media Lab. Before joining Tencent, he was with Qualcomm and Siemens. Dr. Li has been working in the field of video compression for years and is an active contributor to international video coding standards. He served as chair and co-chair in a number of Ad Hoc groups, core experiments, including the co-chair of JEM reference software, VVC reference software, and co-editor of MPEG-5 EVC. Dr. Li is a senior member of IEEE. He has published over 40 journal and conference papers, 300+ standard contributions, and hold 120+ US granted and pending patents.\" \n    },\n    {\n      id: 3,\n      imgClass: \"icon-about_image_expert_2\",\n      name: \"Dr. Xin Zhao\",\n      text: \"Dr. Xin Zhao received the B.S. degree in electronic engineering from Tsinghua University, Beijing, China, in 2006, and the Ph.D. degree in computer applications from Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China.In 2017, he joined Tencent, Palo Alto, CA, USA, where he is currently a Principal Researcher, focusing on the research and development of VVC standard, and leading a team developing new video coding standard. From 2012 to 2017, he was a Staff Engineer with Qualcomm, San Diego, CA, USA. Since 2009, he has been actively contributing to the development of international video coding standards and their extensions, such as HEVC, 3D extensions to H.264/AVC and HEVC standards, and VVC with Joint Video Exploration Team (JVET).Dr. Zhao has over 12 years’ experience on video coding and processing algorithms. He has contributed over 200 proposals to multiple video coding standards, including around 30 adopted proposals. Dr. Zhao has been serving as the chairs of core experiments and multiple Break-out-Groups and Ad-hoc Group established in JVET. He is the inventor of over 260 filed patent applications with around 20 granted patents, and author of over 30 papers published in top-tier academic journals and conferences. His current research interests include image and video coding, video processing.\"\n    },\n    {\n      id: 4,\n      imgClass: \"icon-about_image_expert_3\",\n      name:\"Dr. Stephan Wenger\",\n      text:\"Dr. Stephan Wenger is Sr. Director of Intellectual Property and Standards in Tencent America LLC.  Until early 2018, he was VP IP & Standards at Vidyo Inc., a leading supplier of unified collaboration solutions, and before that, he was active in Nokia's Intellectual Property Rights department and research center.  He has also helped start companies in the field of multimedia coding and served on the Board of Directors of UB Video Inc. and eBrisk Video until their successful acquisitions in 2006 and 2016, respectively.  He is also very active in the standardization for new Multimedia technologies, especially in the IETF, ITU-T, and MPEG.   Dr. Wenger has been inventor of some 40 patents with many more pending. He received the diploma and Dr.-Ing degrees in Computer Science from Technische Universität Berlin, Germany, in 1989 and 1995 respectively.\" \n    },\n    {\n      id: 5,\n      imgClass: \"icon-about_image_expert_4\",\n      name:\"Dr. Iraj Sodagar\",\n      text:\"Iraj Sodagar joined Tencent America’s Media Lab in November 2018 as a Principal Researcher. Before Tencent, he was with Microsoft for 10 years as a Principal Multimedia System Architect. At Microsoft, he worked with various research and product groups in the development and standardization of multimedia technologies. He was responsible for Windows’ multimedia delivery strategy and alignment of products and standardization, as well as coordination of all streaming standardization cross Microsoft. Throughout the last 25 years, Dr. Sodagar has participated, led and managed various R&D projects, advanced architecture design and product development including image and video coding, media indexing and analysis, media storage, transport and delivery, and media transcoding on cloud.  He built and managed technical teams for R&D, software development and productization, and standardization of developed technology. He had various leadership positions and engagements in ITU-T, JPEG, MPEG, AMQP, W3C, DVB, 3GPP, W3C, CTA WAVE, DLNA and AOMedia organizations. He also started Wireless Media Forum, the first consortia on mobile video delivery in early 2000s. More recently, he has been the chair of MPEG’s DASH subgroup from its very start. He is also the founder, and the President and Chairman of Board of DASH-Industry Forum (DASH-IF). He was also the co-chair of MPEG’s CMAF subgroup from the start until publication of its 1st edition.He received the Ph.D. degree in electrical engineering from Georgia Institute of Technology, Atlanta, in 1994.\" \n    },\n    {\n      id: 6,\n      imgClass: \"icon-about_image_expert_5\",\n      name:\"Dr. Xiaozhong Xu\",\n      text:\"Dr.Xiaozhong Xu joined Tencent Media Lab in 2017.He received his B.S. and Ph.D. degrees both from Department of Electronic Egnineering, Tsinghua University. He started working on video coding research and standardization since 2004. He has participated in multiple national and international video coding standard activities, including H.264/AVC, H.265/HEVC, H.266/VVC, MPEG-5, ITU-T IPTV and AVS. For those standardization activities, he have submitted more than 100 technical contributions and had more than 20 of them been adopted by the related standard committees. Dr. Xu has been actively promoting the standardization work by serving as a co-chair for screen content coding sub-groups in H.265/HEVC, H.266/VVC and AVS standards, and as a co-editor for MPEG-5 specification.\" \n    },\n    {\n      id: 7,\n      imgClass: \"icon-about_image_expert_6\",\n      name:\"Dr. Bin Zhu\",\n      text:\"Bin graduated from Iowa State University with Ph.D in Electrical & Computer Engineering. After graduation, Bin worked in several semiconductor start ups and later joined Intel and Apple to lead the team to develop core video products and technologies.  Bin joined Tencent in Dec, 2018 and lead the team to develop next generation(VVC/H266) video codec products.\" \n  }\n],\n\n\n\n\n  //关于我们-加入我们\n  contactTitle: \"Please contact  Tencent Media Lab\",\n  contactEmail: \"medialab@tencent.com\",\n  positionList: [\n    {\n      id: 1,\n      positionName: \"Video Kernel Developing Principal Engineer\",\n      address: \"Working Place ：Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"- Optimize H264、H265 coding algorithm.\",\n          \"- Follow up frontier coding standard establishment in industry, such as H266, AV1,etc.\"\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- Expert knowledge in video coding basic algorithms, such as H264, HEVC, etc.\",\n          \"- Experience in assembly optimization and GPU acceleration development is preferred.\",\n          \"- Expert in open-source framework such as ffmpeg，x264 and x265 with the ability of mobile hardware acceleration development.\",\n          \"- Solid knowledge in math and data structure, good written and oral communication skills.\"\n        ]\n      }\n    },\n    {\n      id: 2,\n      positionName: \"Video Processing Principal Engineer\",\n      address: \"Working Place ：Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"- Fundamental video processing algorithms, video enhancement and assessment, cross platform solution, etc.\",\n          \"- Video processing algorithm development, including video denoiser, artifact reduction, color and edge enhancement, frame rate up conversion, etc.\",\n          \"- Familiar with temporal technologies and solutions for video processing, in both fields of deep learning and signal processing.\",\n          \"- Video processing algorithm optimization across various platforms, e.g. x86, ARM.\",\n          \"- Good knowledge on quality assessment standards and methods for video enhancement.\"\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- Expert knowledge in video,  image, and signal processing principles, algorithms, and techniques.\",\n          \"- Expert knowledge in quality assessment standards and methods for video enhancement.\",\n          \"- Expert knowledge in Convolutional Neural Network (CNN) techniques.\",\n          \"- Familiarity with video compression standards and codec would be a plus.\",\n          \"- Familiarity with iOS and Android software development would be a plus.\",\n          \"- Excellent software design, problem solving, and debugging skills.\",\n          \"- Solid programming skills and C/C++, Python coding abilities.\",\n          \"- Good written and oral communication skills.\"\n        ]\n      }\n    },\n    {\n      id: 3,\n      positionName: \"Video Network Principal Engineer\",\n      address: \"Working Place ：Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"- Network algorithm research and developement on auido/video for global real-time communication.\",\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- C/C++ coding abilities, familiar with TCP/IP protocol stacks, experience in high loading APP development.\",\n          \"- Expert in audio/video coding and open-source framework such as FFmpeg, VLC, WebRTC, X264, etc. Expert in online audio/video core technology, such as JITTER BUFFER and FEC, along with bandwidth prediction and code rate self-adaption technology. Experience in online audio/video communication projects is preferred.\",\n          \"- Expert in network deployment and operation. Experience in oversea acceleration, CDN maintainance and edge computing is preferred.\",\n          \"- Expert in network metrics. Experience in research on FCC Speed Test principles, protocol and reliazaion is preferred.\"\n        ]\n      }\n    },\n    {\n      id: 4,\n      positionName: \"Audio Algorithm Researcher\",\n      address: \"Working Place ：Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"- Fondamental audio algorithm, echo cancellation, noise suppression，voice conversion and microphone array technology,etc.\",\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- Master or doctor degree in computier engineering, sensory science, computer science, physics, maths or related fields.\",\n          \"- Solid programming skills and C/C++, Python coding abilities. Capable of adaptating and optmizing machine learning algorithm among different mobile platforms.\",\n          \"- Experience in audio signal processing and content analysis.\",\n          \"- Experience in pattern recognition, machine learning, linguistics and signal processing.\",\n          \"- Experience in academic competitions like Kaggle or frontier labs in audio/video signal process/machine learning/data mining. Top on significant dataset such as ImageNet.\",\n          \"- Good written and oral communication skills.\"\n        ]\n      }\n    },\n    {\n      id: 5,\n      positionName: \"Audio Kernel Developing Principal Engineer\",\n      address: \"Working Place ：Shenzhen\",\n      duty: {\n        title: \"Responsibilities:\",\n        content: [\n          \"-Audio commucation core coding for QQ and Tencent Cloud, before-and-after algorithm, multi-platform voice engine, before algorithm realted to voice recognition, such as mobile corss-platform voice acquisition and broadcasting.\",\n          \"-Before-and-after algorithm for echo cancellation, noise suppression, silence detection, auto gain, etch. \",\n          \"-Network transmission adaption, FEC strategy, buffering control, error concealment.\",\n          \"-Core coding optimization in adaptation to different platforms.\"\n        ]\n      },\n      jobRequirements: {\n        title: \"Requirements:\",\n        content: [\n          \"- Master degree in computer and comminication with working experience over 2 years.\",\n          \"- Expert knowledge in digital signal processing, audio coding standard and audio core algorithm.\",\n          \"- Experience in before-and-after algorithm for echo cancellation, noise suppression, silence detection, auto gain and  frequency response equalizer.\",\n          \"- Expert in audio network transmission and Qos control.\",\n          \"- Expert in C/C++ and software development.\",\n          \"- Experience in mobile(iOS/Andorid) development and optimization is preferred.\",\n          \"- Good written and oral communication skills.\"\n        ]\n      }\n    }\n  ],\n\n   //关于我们-相关论文\n   navList: [\"All\",\"Speech and Audio\",\"Transmission Optimization\",\"Image quality assessment\",\"Video Coding\",\"Computer Vision\",\"3D point cloud\"],\n   Total: 'Total ',\n   item: '  ',\n   thesisList:[\n    [\n      [22,\"all\"],\n      [\n        [\n          {\n            id:'all-3-5',\n            title: \"Multiple Reference Line Coding for Most Probable Modes in Intra Prediction\",\n            author: \"Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand\",\n            time: \"2019\",\n            content: \"This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.\",\n            src: \"https://ieeexplore.ieee.org/document/8712640\"\n          },\n          {\n            id:'all-3-4',\n            title: \"Wide Angular Intra Prediction for Versatile Video Coding\",\n            author: \"Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape\",\n            time: \"2019\",\n            content: \"This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).\",\n            src: \"https://ieeexplore.ieee.org/document/8712681\"\n          },\n          {\n            id:'all-3-1',\n            title: \"Blind image quality assessment based on joint log-contrast statistics\",\n            author: \"Yabin Zhang et at.\",\n            time: \"2019\",\n            content: \"During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.\",\n            src: \"https://www.sciencedirect.com/science/article/pii/S0925231218313432\"\n          },\n          {\n            id:'all-4-1',\n            title: \"Fast Adaptive Multiple Transform for Versatile Video Coding\",\n            author: \"Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu\",\n            time: \"2019\",\n            content: \"The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.\",\n            src: \"https://ieeexplore.ieee.org/document/8712650\"\n          },\n          {\n            id:'all-4-2',\n            title: \"Recent advances in video coding beyond the HEVC standard\",\n            author: \"Xiaozhong Xu, Shan Liu\",\n            time: \"2019\",\n            content: \"The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.\",\n            src: \"https://doi.org/10.1017/ATSIP.2019.11\"\n          },\n          \n        ],\n        [\n          {\n            id:'all-4-3',\n            title: \"Current Picture Referencing in Versatile Video Coding\",\n            author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n            time: \"2019\",\n            content: \"Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.\",\n            src: \"https://ieeexplore.ieee.org/document/8695359\"\n          },\n          {\n            id:'all-5-1',\n            title: \"High Performance Gesture Recognition via Effective and Efficient Temporal Modeling\",\n            author: \"Yang Yi, Feng Li, et al.\",\n            time: \"2019\",\n            content: \"State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization\",\n            src: \"https://www.ijcai.org/proceedings/2019/0141.pdf\"\n          },\n          {\n            id:'all-5-2',\n            title: \"A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.\",\n            author: \"Yabin Zhang, et al\",\n            time: \"2019\",\n            content: \"3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.\",\n            src: \"https://ieeexplore.ieee.org/document/8704253\"\n          },\n          {\n            id:'all-1-1',\n            title: \"A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network\",\n            author: \"sarahqwang\",\n            time: \"2018\",\n            content: \"The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8706582\"\n          },\n          {\n            id:'all-2-1',\n            title: \"Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy\",\n            author: \"zeqilai\",\n            time: \"2018\",\n            content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in today’s network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps’ performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.\",\n            src: \"https://dl.acm.org/citation.cfm?id=3318101\"\n          }\n        ],\n        [\n          {\n            id:'all-3-2',\n            title: \"Intra Block Copy for Next Generation Video Coding\",\n            author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n            time: \"2018\",\n            content: \"Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.\",\n            src: \"https://ieeexplore.ieee.org/document/8551528\"\n          },\n          {\n            id:'all-3-3',\n            title: \"Coupled Primary and Secondary Transform for Next Generation Video Coding\",\n            author: \"Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu\",\n            time: \"2018\",\n            content: \"The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.\",\n            src: \"https://ieeexplore.ieee.org/document/8698635\"\n          },\n          {\n            id:'all-1-5',\n            title: \"Gaussian density guided deep neural network for single-channel speech enhancement\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8168116\"\n          },\n          {\n            id:'all-1-3',\n            title: \"A maximum likelihood approach to deep neural network based speech dereverberation\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8282019\"\n          },\n          {\n            id:'all-1-2',\n            title: \"A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.\",\n            src: \"https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF\"\n          }\n        ],\n        [\n          {\n            id:'all-1-4',\n            title: \"A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemale–female, male–male, and female–male mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/7917323\"\n          },\n          {\n            id:'all-2-2',\n            title: \"Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.\",\n            src: \"https://ieeexplore.ieee.org/document/8117568\"\n          },\n          {\n            id:'all-2-3',\n            title: \"Furion: Engineering high-quality immersive virtual reality on today's mobile devices\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).\",\n            src: \"https://dl.acm.org/citation.cfm?id=3117815\"\n          },\n          {\n            id:'all-4-4',\n            title: \"TPG Image Compression Technology\",\n            author: \"ShitaoWang、PiaoDing、XiaozhengHuang、HanjunLiu、BinjiLuo、XinxingChen、YoubaoWu、RonggangWang\",\n            time: \"\",\n            content: \"TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression\",\n            src: \"http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238\"\n          },\n          {\n            id:'all-4-5',\n            title: \"Saliency detection with two-level fully convolutional networks\",\n            author: \"Yang Yi, et al.\",\n            time: \"2017\",\n            content: \"This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.\",\n            src: \"https://ieeexplore.ieee.org/document/8019309/\"\n          },\n        ],\n        [\n          {\n            id:'all-2-4',\n            title: \"Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.\",\n            src: \"https://ieeexplore.ieee.org/document/7898362\"\n          },\n          {\n            id:'all-2-5',\n            title: \"Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of user’s input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.\",\n            src: \"https://dl.acm.org/citation.cfm?id=3234201\"\n          }\n        ]\n      ]\n    ],\n    [\n      [5,\"audio\"],\n      [\n        [\n          {\n            id:'audio-1-1',\n            title: \"A Maximum Likelihood Approach to Masking-based Speech Enhancement Using Deep Neural Network\",\n            author: \"sarahqwang\",\n            time: \"2018\",\n            content: \"The minimum mean squared error (MMSE) is usually adopted as the training criterion for speech enhancement based on deep neural network (DNN). In this study, we propose a probabilistic learning framework to optimize the DNN parameter for masking-based speech enhancement. Ideal ratio mask (IRM) is used as the learning target and its prediction error vector at the DNN output is modeled to follow statistically independent generalized Gaussian distribution (GGD). Accordingly, we present a maximum likelihood (ML) approach to DNN parameter optimization. We analyze and discuss the effect of shape parameter of GGD on noise reduction and speech preservation. Experimental results on the TIMIT corpus show the proposed ML-based learning approach can achieve consistent improvements over MMSE-based DNN learning on all evaluation metrics. Less speech distortion is observed in ML-based approach especially for high frequency units than MMSE-based approach.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8706582\"\n          },\n          {\n            id:'audio-1-2',\n            title: \"A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for singlechannel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.\",\n            src: \"https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF\"\n          },\n          {\n            id:'audio-1-3',\n            title: \"A maximum likelihood approach to deep neural network based speech dereverberation\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"Recently, deep neural network (DNN) based speech dereverberation becomes popular with a standard minimum mean squared error (MMSE) criterion for learning the parameters. In this study, a probabilistic learning framework to estimate the DNN parameters for single-channel speech dereverberation is proposed. First, the statistical analysis shows that the prediction error vector at the DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a maximum likelihood (ML) approach to DNN parameter learning by charactering the prediction error vector as a multivariate Gaussian density with a zero mean vector and an unknown co- variance matrix. Our experiments demonstrate that the proposed ML-based DNN learning can achieve a better generalization capability than MMSE-based DNN learning. And all the object measures of speech quality and intelligibility are consistently improved.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8282019\"\n          },\n          {\n            id:'audio-1-4',\n            title: \"A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"We propose an unsupervised speech separationframework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks (DNNs). We rely ona key assumption that two speakers could be well segregated if theyare not too similar to each other. A dissimilarity measure betweentwo speakers is first proposed to characterize the separationability between competing speakers. We then show that speakerswith the same or different genders can often be separated if twospeaker clusters, with large enough distances between them, foreach gender group could be established, resulting in four speakerclusters. Next, a DNN-based gender mixture detection algorithm isproposed to determine whether the two speakers in the mixture arefemales, males, or from different genders. This detector is based ona newly proposed DNN architecture with four outputs, two of themrepresenting the female speaker clusters and the other two char-acterizing the male groups. Finally, we propose to construct threeindependent speech separation DNN systems, one for each of thefemale–female, male–male, and female–male mixture situations.Each DNN gives dual outputs, one representing the target speakergroup and the other characterizing the interfering speaker cluster.Trained and tested on the speech separation challenge corpus,our experimental results indicate that the proposed DNN-basedapproach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledgeabout the mixed target and interfering speakers being segregated.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/7917323\"\n          },\n          {\n            id:'audio-1-5',\n            title: \"Gaussian density guided deep neural network for single-channel speech enhancement\",\n            author: \"yannanwang\",\n            time: \"2017\",\n            content: \"Recently, the minimum mean squared error (MMSE) has beena benchmark of optimization criterion for deep neural net-work (DNN) based speech enhancement. In this study, a prob-abilistic learning framework to estimate the DNN parameter-s for single-channel speech enhancement is proposed. First,the statistical analysis shows that the prediction error vector atthe DNN output well follows a unimodal density for each log-power spectral component. Accordingly, we present a max-imum likelihood (ML) approach to DNN parameter learningby charactering the prediction error vector as a multivariateGaussian density with a zero mean vector and an unknown co-variance matrix. It is demonstrated that the proposed learningapproach can achieve a better generalization capability thanMMSE-based DNN learning for unseen noise types, whichcan significantly reduce the speech distortions in low SNRenvironments.\",\n            src: \"https://ieeexplore.ieee.org/abstract/document/8168116\"\n          }\n        ]\n      ]\n    ],\n    [\n      [5,\"network\"],\n      [\n        [\n          {\n            id:'network-1-1',\n            title: \"Wireless Network Instabilities in the  Wild: Prevalence, Applications (non) Resilience, and OS Remedy\",\n            author: \"zeqilai\",\n            time: \"2018\",\n            content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decades are plenty evident, the extent of signal strength fluctuation and network disruptions unexpected switching or disconnections experienced by mobile users in today’s network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and significant signal strength fluctuations together denoted as network instabilities experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remains prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present Janus, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and satisfy apps’ performance requirement. We have implemented a prototype of Janus and our evaluation using a set of popular apps shows that Janus can: 1 transparently and efficiently handle network disruptions; 2 reduce video stalls by 2.9 times and increase 31% of the time of good voice quality; 3 reduce traffic size by 26.4% and energy consumption by 16.3% compared to naive solutions.\",\n            src: \"https://dl.acm.org/citation.cfm?id=3318101\"\n          },\n          {\n            id:'network-1-2',\n            title: \"Wireless network instabilities in the wild:Prevalence, app (non)resilience, and OS remedy\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"While the bandwidth and latency improvement of both WiFi and cellular data networks in the past decade are plenty evident, the extent of signal strength fluctuation and network disruptions (unexpected switching or disconnections) experienced by mobile users in today's network deployment remains less clear. This paper makes three contributions. First, we conduct the first extensive measurement of network disruptions and signal strength fluctuations (together denoted as instabilities) experienced by 2000 smartphones in the wild. Our results show that network disruptions and signal strength fluctuations remain prevalent as we moved into the 4G era. Second, we study how well popular mobile apps today handle such network instabilities. Our results show that even some of the most popular mobile apps do not implement any disruption-tolerant mechanisms. Third, we present JANUS, an intelligent interface management framework that exploits the multiple interfaces on a handset to transparently handle network disruptions and improve apps' QoE. We have implemented JANUS on Android and our evaluation using a set of popular apps shows that Janus can (1) transparently and efficiently handle network disruptions, (2) reduce video stalls by 2.9 times and increase 31% of the time of good voice quality compared to naive solutions.\",\n            src: \"https://ieeexplore.ieee.org/document/8117568\"\n          },\n          {\n            id:'network-1-3',\n            title: \"Furion: Engineering high-quality immersive virtual reality on today's mobile devices\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"In this paper, we perform a systematic design study of the 'elephant in the room' facing the VR industry -- is it feasible to enable high-quality VR apps on untethered mobile devices such as smartphones? Our quantitative, performance-driven design study makes two contributions. First, we show that the QoE achievable for high-quality VR applications on today's mobile hardware and wireless networks via local rendering or offloading is about 10X away from the acceptable QoE, yet waiting for future mobile hardware or next-generation wireless networks (e.g. 5G) is unlikely to help, because of power limitation and the higher CPU utilization needed for processing packets under higher data rate. Second, we present Furion, a VR framework that enables high-quality, immersive mobile VR on today's mobile devices and wireless networks. Furion exploits a key insight about the VR workload that foreground interactions and background environment have contrasting predictability and rendering workload, and employs a split renderer architecture running on both the phone and the server. Supplemented with video compression, use of panoramic frames, and parallel decoding on multiple cores on the phone, we demonstrate Furion can support high-quality VR apps on today's smartphones over WiFi, with under 14ms latency and 60 FPS (the phone display refresh rate).\",\n            src: \"https://dl.acm.org/citation.cfm?id=3117815\"\n          },\n          {\n            id:'network-1-4',\n            title: \"Quicksync: Improving synchronization efficiency for mobile cloud storage services. IEEE Transactions on Mobile Computing\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.\",\n            src: \"https://ieeexplore.ieee.org/document/7898362\"\n          },\n          {\n            id:'network-1-5',\n            title: \"Immersion on the Edge: A Cooperative Framework for Mobile Immersive Computing\",\n            author: \"zeqilai\",\n            time: \"2017\",\n            content: \"We present CoIC, a cooperative framework for mobile immersive computing applications. To speed up computationintensive IC tasks, CoIC leverages the insight that similar or redundant IC tasks among different applications/users can be cached and shared to improve the user-perceived quality of experience (QoE), especially the end-to-end latency. Initially, the client pre-processes the request to generate and send a feature descriptor of user’s input to the edge. On the edge, CoIC attempts to make a lookup with the feature descriptor(as the key) by matching the key to any results cached on the edge. If there is a hit, the cached result is returned to the client immediately. Otherwise, the edge forwards the request to the cloud and inserts the result to the edge cache.\",\n            src: \"https://dl.acm.org/citation.cfm?id=3234201\"\n          },\n        ]\n      ]\n    ],\n    [\n      [1,\"quality\"],\n      [\n        [\n          {\n            id:'quality-1-1',\n            title: \"Blind image quality assessment based on joint log-contrast statistics\",\n            author: \"Yabin Zhang et at.\",\n            time: \"2019\",\n            content: \"During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model, instead of using model parameters directly. Finally, we develop an effective joint-NSS model based BIQA metric called BJLC (BIQA based on joint log-contrast statistics). Extensive experiments on four public large-scale image databases have validated that objective quality scores predicted by the proposed BIQA method are in higher accordance with subjective ratings generated by human observers compared with existing methods.\",\n            src: \"https://www.sciencedirect.com/science/article/pii/S0925231218313432\"\n          }\n        ]\n      ]\n    ],\n    [\n      [8,\"video\"],\n      [\n        [\n          {\n            id:'video-1-3',\n            title: \"Wide Angular Intra Prediction for Versatile Video Coding\",\n            author: \"Liang Zhao, Xin Zhao, Shan Liu, Xiang Li, Jani Lainema, Gagan Rath, Fabrice Urban, and Fabian Racape\",\n            time: \"2019\",\n            content: \"This paper presents a technical overview of Wide Angular Intra Prediction (WAIP) that was adopted into the test model of Versatile Video Coding (VVC) standard. Due to the adoption of flexible block partitioning using binary and ternary splits, a Coding Unit (CU) can have either a square or a rectangular block shape. However, the conventional angular intra prediction directions, ranging from 45 degrees to -135 degrees in clockwise direction, were designed for square CUs. To better optimize the intra prediction for rectangular blocks, WAIP modes were proposed to enable intra prediction directions beyond the range of conventional intra prediction directions. For different aspect ratios of rectangular block shapes, different number of conventional angular intra prediction modes were replaced by WAIP modes. The replaced intra prediction modes are signaled using the original signaling method. Simulation results reportedly show that, with almost no impact on the run-time, on average 0.31% BD-rate reduction is achieved for intra coding using VVC test model (VTM).\",\n            src: \"https://ieeexplore.ieee.org/document/8712681\"\n          },\n          {\n            id:'video-1-4',\n            title: \"Multiple Reference Line Coding for Most Probable Modes in Intra Prediction\",\n            author: \"Yao-Jen Chang, Hong-Jheng, Jhu, Hui-Yu Jiang, Liang Zhao, Xin Zhao, Xiang Li, Shan Liu, Benjamin Bross, Paul Keydel, Heiko Schwarz, Detlev Marpe, Thomas Wiegand\",\n            time: \"2019\",\n            content: \"This paper provides a technical overview of most probable modes (MPM)-based multiple reference line (M-MRL) intra prediction that was adopted into the Versatile Video Coding standard at the 12th JVET meeting. The M-MRL applies not only the nearest reference line but also farther reference lines to MPMs for intra prediction. The techniques of the adopted M-MRL highlighted in this paper includes the multiple reference line signaling scheme, the discontinuous reference lines scheme, the reconstruction process for non-zero reference lines, and the encoding algorithm for joint reference line and intra mode decisions. Experimental results are provided to evaluate the performance of the M-MRL on top of the test model VTM-2.0.1, and new analyses are also discussed to investigate the capability of M-MRL under different numbers of discontinuous reference lines.\",\n            src: \"https://ieeexplore.ieee.org/document/8712640\"\n          },\n          {\n            id:'video-1-5',\n            title: \"Fast Adaptive Multiple Transform for Versatile Video Coding\",\n            author: \"Zhaobin Zhang, Xin Zhao, Xiang Li, Zhu Li, Shan Liu\",\n            time: \"2019\",\n            content: \"The Joint Video Exploration Team (JVET) recently launched the standardization of nextgeneration video coding named Versatile Video Coding (VVC) in which the Adaptive Multiple Transforms (AMT) is adopted as the primary residual coding transform solution. AMT introduces multiple transforms selected from the DST/DCT families and achieves noticeable coding gains. However, the set of transforms are calculated using direct matrix multiplication which induces higher run-time complexity and limits the application for practical video codec. In this paper, a fast DST-VII/DCT-VIII algorithm based on partial butterfly with dual implementation support is proposed, which aims at achieving reduced operation counts and run-time cost meanwhile yield almost the same coding performance. The proposed method has been implemented on top of the VTM-1.1 and experiments have been conducted using Common Test Conditions (CTC) to validate the efficacy. The experimental results show that the proposed methods, in the state-of-the-art codec, can provide an average of 7%, 5% and 8% overall decoding time savings under All Intra (AI), Random Access (RA) and Low Delay B (LDB) configuration, respectively yet still outputs almost the same coding results.maintains coding performance.\",\n            src: \"https://ieeexplore.ieee.org/document/8712650\"\n          },\n          {\n            id:'video-2-1',\n            title: \"Recent advances in video coding beyond the HEVC standard\",\n            author: \"Xiaozhong Xu, Shan Liu\",\n            time: \"2019\",\n            content: \"The standardization process for Versatile Video Coding (VVC), the next generation video coding standard, was launched in 2018, after several recent advances in video coding technologies had been investigated under the Joint Video Experts Team (JVET) of ITU-T VCEG and ISO/IEC MPEG experts. The recent standard development status (up to VVC working draft 2) shows that the VTM software, the test model for this VVC standard, can achieve over 23% average coding gain under random access configuration when compared to the HM software, the test model of HEVC standard. This paper gives a review of recently developed video coding technologies that have been either adopted into the VVC working draft as part of the standard or under further evaluation for potential inclusions.\",\n            src: \"https://doi.org/10.1017/ATSIP.2019.11\"\n          },\n          {\n            id:'video-2-2',\n            title: \"Current Picture Referencing in Versatile Video Coding\",\n            author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n            time: \"2019\",\n            content: \"Screen content coding (SCC) is a recognized feature in versatile video coding (VVC), the most recent international video coding standard development. Among all the coding tools that have been evaluated for SCC in the past, current picture referencing (CPR), or by another name intra block copy (IBC), has shown its effectiveness in coding of computer-generated contents such as texts and graphics. Therefore, it has been adopted into the HEVC standard version 4, extensions for SCC. In this paper, several variations of CPR implementations are proposed and discussed. Among those methods, the one with the best trade-off between coding efficiency and implementation cost has been adopted into the new VVC standard working draft and the reference software VTM.\",\n            src: \"https://ieeexplore.ieee.org/document/8695359\"\n          }\n        ],\n        [\n          {\n            id:'video-1-2',\n            title: \"Coupled Primary and Secondary Transform for Next Generation Video Coding\",\n            author: \"Xin Zhao, Li Li, Zhu Li, Xiang Li, Shan Liu\",\n            time: \"2018\",\n            content: \"The discrete cosine transform type II can efficiently approximate the Karhunen-Loeve transform under the first-order stationary Markov condition. However, the highly dynamic characteristics of natural images will not always follow the first-order stationary Markov condition. It is well known that multi-core transforms and non-separable transforms capture diversified and directional texture patterns more efficiently. And a combination of enhanced multiple transform (EMT) and nonseparable secondary transform (NSST) are provided in the reference software of the next generation video coding standard to solve this problem. However, the current method of combining the EMT and NSST may lead to quite significant encoder complexity increase, which makes the video codec rather impractical for real applications. Therefore, in this paper, we investigate the interactions between EMT and NSST, and propose a coupled primary and secondary transform to simplify the combination to obtain a better trade-off between the performance and the encoder complexity. With the proposed method, the transform for the Luma and Chroma components is also unified for a consistent design as an additional benefit. We implement the proposed transform on top of the Next software, which has been proposed for the next generation video coding standard. The experimental results demonstrate that the proposed algorithm can provide significant time reduction while keeping the majority of the performance.\",\n            src: \"https://ieeexplore.ieee.org/document/8698635\"\n          },\n          {\n            id:'video-1-1',\n            title: \"Intra Block Copy for Next Generation Video Coding\",\n            author: \"Xiaozhong Xu, Xiang Li, Shan Liu\",\n            time: \"2018\",\n            content: \"Screen content is partly driven by the rapid development of screen applications such as wireless display, screen sharing, cloud computing and gaming, etc. Different from camera-captured content, screen content has relatively bad continuity in spatiotemporal domain and severe movements or sudden changes may occur in continuous pictures. Owing to the special characteristics of screen content, conventional coding tools including High Efficiency Video Coding (HEVC) standard are unsuitable for screen content coding (SCC). A SCC extension to HEVC was brought out and developed to meet the demand of screen content coding. This paper provides a brief survey on the main coding tools in the HEVC-SCC extension. Screen content sequences also pose challenges on transmission due to its discontinuity. The alternate presentation of moving and stationary pictures makes the exploitation of bandwidth a technical difficulty. This paper introduces the improvements of SCC rate control in HEVC with better transmission performance and more efficient bandwidth utilization.\",\n            src: \"https://ieeexplore.ieee.org/document/8551528\"\n          },\n          {\n            id:'video-2-3',\n            title: \"TPG Image Compression Technology\",\n            author: \"ShitaoWang、PiaoDing、XiaozhengHuang、HanjunLiu、BinjiLuo、XinxingChen、YoubaoWu、RonggangWang\",\n            time: \"2017\",\n            content: \"TPG(tiny portable graphic) is a new image compression technology based on the video part of AVS2 standard,whose compression efficiency is notably higher than traditional image formats like JPG,PNG and GIF.Theory and feature of TPG image compression technology were introduced.Then,the compression efficiency of TPG and traditional image formats was compared.Results show that TPG has overwhelming advantage.Key words: tiny portable graphic, AVS2, image compression\",\n            src: \"http://www.infocomm-journal.com/dxkx/CN/10.11959/j.issn.1000-0801.2017238\"\n          }\n        ]\n      ]\n    ],\n    [\n      [2,\"computer\"],\n      [\n        [\n          {\n            id:'computer-1-2',\n            title: \"High Performance Gesture Recognition via Effective and Efficient Temporal Modeling\",\n            author: \"Yang Yi, Feng Li, et al.\",\n            time: \"2019\",\n            content: \"State-of-the-art hand gesture recognition methods have investigated the spatiotemporal features based on 3D convolutional neural networks (3DCNNs) or convolutional long short-term memory (ConvLSTM). However, they often suffer from the inefficiency due to the high computational complexity of their network structures. In this paper, we focus instead on the 1D convolutional neural networks and propose a simple and efficient architectural unit, Multi-Kernel Temporal Block (MKTB), that models the multi-scale temporal responses by explicitly applying different temporal kernels. Then, we present a Global Refinement Block (GRB), which is an attention module for shaping the global temporal features based on the cross-channel similarity. By incorporating the MKTB and GRB, our architecture can effectively explore the spatiotemporal features within tolerable computational cost. Extensive experiments conducted on public datasets demonstrate that our proposed model achieves the state-of-the-art with higher efficiency. Moreover, the proposed MKTB and GRB are plug-and-play modules and the experiments on other tasks, like video understanding and video-based person reidentification, also display their good performance in efficiency and capability of generalization\",\n            src: \"https://www.ijcai.org/proceedings/2019/0141.pdf\"\n          },\n          {\n            id:'computer-1-1',\n            title: \"Saliency detection with two-level fully convolutional networks\",\n            author: \"Yang Yi, et al.\",\n            time: \"2017\",\n            content: \"This paper proposes a deep architecture for saliency detection by fusing pixel-level and superpixel-level predictions. Different from the previous methods that either make dense pixellevel prediction with complex networks or region-level prediction for each region with fully-connected layers, this paper investigates an elegant route to make two-level predictions based on a same simple fully convolutional network via seamless transformation. In the transformation module, we integrate the low level features to model the similarities between pixels and superpixels as well as superpixels and superpixels. The pixel-level saliency map detects and highlights the salient object well and the superpixel-level saliency map preserves sharp boundary in a complementary way. A shallow fusion net is applied to learn to fuse the two saliency maps, followed by a CRF post-refinement module. Experiments on four benchmark data sets demonstrate that our method performs favorably against the state-of-art methods.\",\n            src: \"https://ieeexplore.ieee.org/document/8019309/\"\n          }\n        ]\n      ]\n    ],\n    [\n      [1,\"dianYun\"],\n      [\n        [\n          {\n            id:'dianYun-1-1',\n            title: \"A Two-stage Outlier Filtering Framework for City-Scale Localization using 3D SfM Point Clouds.\",\n            author: \"Yabin Zhang, et al\",\n            time: \"2019\",\n            content: \"3D Structure-based localization aims to estimate the 6-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore a 3D Structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of SfM point clouds. Firstly, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Secondly, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier filtering framework only relies on intrinsic information of a SfM point cloud. It is thus widely applicable to be embedded into existing localization approaches. The experimental results on two real-world datasets demonstrate the effectiveness of the proposed two-stage outlier filtering framework for city-scale localization.\",\n            src: \"https://ieeexplore.ieee.org/document/8704253\"\n          } \n        ]\n      ]\n    ]\n  ]\n}\n\n\n\n\n\n\n\n\n\n\n","import homeLanguge from \"./pages/en/homePageLanguge\"\nimport researchLanguge from \"./pages/en/researchPageLanguge\"\nimport projectLanguge from \"./pages/en/projectPageLanguge\"\nimport solutionLanguge from \"./pages/en/solutionPageLanguge\"\nimport aboutLanguge from \"./pages/en/aboutPageLanguge\"\n\nexport default {\n    nav: {\n      home: 'Home', //首页\n      research:\"Research\",\n      project:\"Research Topics\",\n      solution:\"Solutions\",\n      about:\"About Us\",\n      join:\"join\"\n    },\n    homeLanguge,\n    researchLanguge,\n    projectLanguge,\n    solutionLanguge,\n    aboutLanguge\n  }\n  ","import '@babel/polyfill'\nimport Vue from 'vue';\nimport Vuex from 'vuex';\nimport App from './App.vue';\nimport router from './router';\nimport Store from './store';\nimport Layout from './components/Layout.vue';\n\nimport 'bootstrap/dist/css/bootstrap.min.css';\n// 引入中英文语言包zh ,en\nimport zh from \"../public/languge/zh.js\";\nimport en from \"../public/languge/en.js\";\nimport VueI18n from 'vue-i18n';\nimport { handleClickEvent } from './util/util.js';\n\nimport Router from 'vue-router'\nconst routerPush = Router.prototype.push\nRouter.prototype.push = function push(location) {\n  return routerPush.call(this, location).catch(error=> error)\n}\n\nVue.use(Vuex);\nVue.config.productionTip = false;\n\nVue.component('Layout',Layout);\n\nrouter.beforeEach((to, from, next) => {\n  next()\n  const toRouter = [\n    { path: '/researchl', id: 87517, eventName: 'access_top_research' },\n    { path: '/project', id: 87516, eventName: 'access_top_project' },\n    { path: '/solution', id: 87515, eventName: 'access_top_solution' },\n    { path: '/research/VideoUnderstanding', id: 87514, eventName: 'access_research_understanding' },\n    { path: '/research/videoProcessing', id: 87513, eventName: 'access_research_process' },\n    { path: '/research/audioTransmission', id: 87511, eventName: 'access_research_audio_and_video' },\n    { path: '/projectTpg', id: 87506, eventName: 'access_project_TPG' },\n    { path: '/projectTse', id: 87505, eventName: 'access_project_TSE' },\n    { path: '/projectLiYing', id: 87504, eventName: 'access_project_liying' },\n    { path: '/projectAudioAndVideo', id: 87503, eventName: 'access_project_platform' },\n    { path: '/research/audioProcessing', id: 87502, eventName: 'access_research_audio' },\n    { path: '/research/international', id: 87501, eventName: 'access_research_international' },\n    { path: '/research/newMedia', id: 87500, eventName: 'access_research_immersive' },\n    { path: '/research/videoCode', id: 87499, eventName: 'access_research_codec' },\n    { path: '/research/perceptual', id: 87498, eventName: 'access_research_perceived' },\n    { path: '/research/assessment', id: 87497, eventName: 'access_research_assess1' },\n  ]\n  toRouter.forEach((item) => {\n    if(to.path === item.path) {\n      handleClickEvent(item.id, item.eventName)\n    }\n  })\n})\n\nVue.use(VueI18n); // 通过插件的形式挂载\nconst i18n = new VueI18n({\n  locale: 'zh',    // 语言标识\n  //this.$i18n.locale // 通过切换locale的值来实现语言切换\n  messages: {\n         zh,\n         en\n       }\n    });\n\n    router.afterEach(() => {\n      window.scrollTo(0,0)\n  });\n  const store = new Vuex.Store({...Store});\n\nnew Vue({\n  router,\n  i18n:i18n,\n  store:store,\n  render: h => h(App)\n}).$mount('#app')\n","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/index.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Layout.vue?vue&type=style&index=0&id=5cbf1b3b&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/index.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Layout.vue?vue&type=style&index=0&id=5cbf1b3b&scoped=true&lang=css&\""],"sourceRoot":""}